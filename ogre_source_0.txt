<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Introduction</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_introduction.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Introduction </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This chapter is intended to give you an overview of the main components of OGRE and why they have been put together that way.</p>
<h1>Object Orientation - more than just a buzzword</h1>
<p>The name is a dead giveaway. It says Object-Oriented Graphics Rendering Engine, and that’s exactly what it is. Ok, but why? Why did I choose to make such a big deal about this?</p>
<p>Well, nowadays graphics engines are like any other large software system. They start small, but soon they balloon into monstrously complex beasts which just can’t be all understood at once. It’s pretty hard to manage systems of this size, and even harder to make changes to them reliably, and that’s pretty important in a field where new techniques and approaches seem to appear every other week. Designing systems around huge files full of C function calls just doesn’t cut it anymore - even if the whole thing is written by one person (not likely) they will find it hard to locate that elusive bit of code after a few months and even harder to work out how it all fits together.</p>
<p>Object orientation is a very popular approach to addressing the complexity problem. It’s a step up from decomposing your code into separate functions, it groups function and state data together in classes which are designed to represent real concepts. It allows you to hide complexity inside easily recognised packages with a conceptually simple interface so they are easy to recognise and have a feel of ’building blocks’ which you can plug together again later. You can also organise these blocks so that some of them look the same on the outside, but have very different ways of achieving their objectives on the inside, again reducing the complexity for the developers because they only have to learn one interface.</p>
<p>I’m not going to teach you OO here, that’s a subject for many other books, but suffice to say I’d seen enough benefits of OO in business systems that I was surprised most graphics code seemed to be written in C function style. I was interested to see whether I could apply my design experience in other types of software to an area which has long held a place in my heart - 3D graphics engines. Some people I spoke to were of the opinion that using full C++ wouldn’t be fast enough for a real-time graphics engine, but others (including me) were of the opinion that, with care, and object-oriented framework can be performant. We were right.</p>
<p>In summary, here’s the benefits an object-oriented approach brings to OGRE:</p>
<dl compact="compact">
<dt>Abstraction </dt>
<dd><p class="startdd"></p>
<p>Common interfaces hide the nuances between different implementations of 3D API and operating systems</p>
<p class="enddd"></p>
</dd>
<dt>Encapsulation </dt>
<dd><p class="startdd"></p>
<p>There is a lot of state management and context-specific actions to be done in a graphics engine - encapsulation allows me to put the code and data nearest to where it is used which makes the code cleaner and easier to understand, and more reliable because duplication is avoided</p>
<p class="enddd"></p>
</dd>
<dt>Polymorphism </dt>
<dd><p class="startdd"></p>
<p>The behaviour of methods changes depending on the type of object you are using, even if you only learn one interface, e.g. a class specialised for managing indoor levels behaves completely differently from the standard scene manager, but looks identical to other classes in the system and has the same methods called on it</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Multi_002deverything"></a> <a class="anchor" id="Multi_002deverything-1"></a></p>
<h1>Multi-everything</h1>
<p>I wanted to do more than create a 3D engine that ran on one 3D API, on one platform, with one type of scene (indoor levels are most popular). I wanted OGRE to be able to extend to any kind of scene (but yet still implement scene-specific optimisations under the surface), any platform and any 3D API.</p>
<p>Therefore all the ’visible’ parts of OGRE are completely independent of platform, 3D API and scene type. There are no dependencies on Windows types, no assumptions about the type of scene you are creating, and the principles of the 3D aspects are based on core maths texts rather than one particular API implementation.</p>
<p>Now of course somewhere OGRE has to get down to the nitty-gritty of the specifics of the platform, API and scene, but it does this in subclasses specially designed for the environment in question, but which still expose the same interface as the abstract versions.</p>
<p>For example, there is a ’Win32Window’ class which handles all the details about rendering windows on a Win32 platform - however the application designer only has to manipulate it via the superclass interface ’RenderWindow’, which will be the same across all platforms.</p>
<p>Similarly the ’SceneManager’ class looks after the arrangement of objects in the scene and their rendering sequence. Applications only have to use this interface, but there is a ’BspSceneManager’ class which optimises the scene management for indoor levels, meaning you get both performance and an easy to learn interface. All applications have to do is hint about the kind of scene they will be creating and let OGRE choose the most appropriate implementation - this is covered in a later tutorial.</p>
<p>OGRE’s object-oriented nature makes all this possible. Currently OGRE runs on Windows, Linux and Mac OSX using plugins to drive the underlying rendering API (currently Direct3D or OpenGL). Applications use OGRE at the abstract level, thus ensuring that they automatically operate on all platforms and rendering subsystems that OGRE provides without any need for platform or API specific code. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: The Core Objects</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_the-_core-_objects.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">The Core Objects </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#The-Root-Object">The Root object</a></li>
<li class="level1"><a href="#The-RenderSystem-object">The RenderSystem object</a></li>
<li class="level1"><a href="#The-SceneManager-object">The SceneManager object</a></li>
<li class="level1"><a href="#The-ResourceGroupManager-Object">The ResourceGroupManager Object</a></li>
<li class="level1"><a href="#The-Mesh-Object">The Mesh Object</a></li>
<li class="level1"><a href="#Entities">Entities</a></li>
<li class="level1"><a href="#Materials">Materials</a></li>
<li class="level1"><a href="#Overlays">Overlays</a></li>
</ul>
</div>
<div class="textblock"><p>This tutorial gives you a quick summary of the core objects that you will use in OGRE and what they are used for.</p>
<p><a class="anchor" id="A-Word-About-Namespaces"></a></p>
<h1>A Word About Namespaces</h1>
<p>OGRE uses a C++ feature called namespaces. This lets you put classes, enums, structures, anything really within a ’namespace’ scope which is an easy way to prevent name clashes, i.e. situations where you have 2 things called the same thing. Since OGRE is designed to be used inside other applications, I wanted to be sure that name clashes would not be a problem. Some people prefix their classes/types with a short code because some compilers don’t support namespaces, but I chose to use them because they are the ’right’ way to do it. Sorry if you have a non-compliant compiler, but hey, the C++ standard has been defined for years, so compiler writers really have no excuse anymore. If your compiler doesn’t support namespaces then it’s probably because it’s sh*t - get a better one. ;)</p>
<p>This means every class, type etc should be prefixed with <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, e.g. <a class="el" href="class_ogre_1_1_camera.html" title="A viewpoint from which the scene will be rendered. ">Ogre::Camera</a>, <a class="el" href="class_ogre_1_1_vector3.html" title="Standard 3-dimensional vector. ">Ogre::Vector3</a> etc which means if elsewhere in your application you have used a Vector3 type you won’t get name clashes. To avoid lots of extra typing you can add a ’using namespace <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>;’ statement to your code which means you don’t have to type the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> prefix unless there is ambiguity (in the situation where you have another definition with the same name).</p>
<p><a class="anchor" id="Overview-from-10_002c000-feet"></a></p>
<h1>Overview from 10,000 feet</h1>
<p>Shown below is a diagram of some of the core objects and where they ’sit’ in the grand scheme of things. This is not all the classes by a long shot, just a few examples of the more more significant ones to give you an idea of how it slots together. </p><div class="image">
<object type="image/svg+xml" data="uml-overview.svg">uml-overview.svg</object>
</div>
<p>At the very top of the diagram is the Root object. This is your ’way in’ to the OGRE system, and it’s where you tend to create the top-level objects that you need to deal with, like scene managers, rendering systems and render windows, loading plugins, all the fundamental stuff. If you don’t know where to start, Root is it for almost everything, although often it will just give you another object which will actually do the detail work, since Root itself is more of an organiser and facilitator object.</p>
<p>The majority of rest of OGRE’s classes fall into one of 3 roles:</p>
<dl compact="compact">
<dt>Scene Management </dt>
<dd><p class="startdd"></p>
<p>This is about the contents of your scene, how it’s structured, how it’s viewed from cameras, etc. Objects in this area are responsible for giving you a natural declarative interface to the world you’re building; i.e. you don’t tell OGRE "set these render states and then render 3 polygons", you tell it "I want an object here, here and here, with these materials on them, rendered from this view", and let it get on with it.</p>
<p class="enddd"></p>
</dd>
<dt>Resource Management </dt>
<dd><p class="startdd"></p>
<p>All rendering needs resources, whether it’s geometry, textures, fonts, whatever. It’s important to manage the loading, re-use and unloading of these things carefully, so that’s what classes in this area do.</p>
<p class="enddd"></p>
</dd>
<dt>Rendering </dt>
<dd><p class="startdd"></p>
<p>Finally, there’s getting the visuals on the screen - this is about the lower-level end of the rendering pipeline, the specific rendering system API objects like buffers, render states and the like and pushing it all down the pipeline. Classes in the Scene Management subsystem use this to get their higher-level scene information onto the screen.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You’ll notice that scattered around the edge are a number of plugins. OGRE is designed to be extended, and plugins are the usual way to go about it. Many of the classes in OGRE can be subclassed and extended, whether it’s changing the scene organisation through a custom SceneManager, adding a new render system implementation (e.g. Direct3D or OpenGL), or providing a way to load resources from another source (say from a web location or a database). Again this is just a small smattering of the kinds of things plugins can do, but as you can see they can plug in to almost any aspect of the system. This way, OGRE isn’t just a solution for one narrowly defined problem, it can extend to pretty much anything you need it to do.</p>
<h1><a class="anchor" id="The-Root-Object"></a>
The Root object</h1>
<p>The <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object is the entry point to the OGRE system. This object MUST be the first one to be created, and the last one to be destroyed. In the example applications I chose to make an instance of Root a member of my application object which ensured that it was created as soon as my application object was, and deleted when the application object was deleted.</p>
<p>The root object lets you configure the system, for example through the <a class="el" href="class_ogre_1_1_root.html#aedc76ef3bdc6514bd4655be0d70d6bcd" title="Displays a dialog asking the user to choose system settings. ">Ogre::Root::showConfigDialog</a> method which is an extremely handy method which performs all render system options detection and shows a dialog for the user to customise resolution, colour depth, full screen options etc. It also sets the options the user selects so that you can initialise the system directly afterwards.</p>
<p>The root object is also your method for obtaining pointers to other objects in the system, such as the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> and various other resource managers. See below for details.</p>
<p>Finally, if you run OGRE in continuous rendering mode, i.e. you want to always refresh all the rendering targets as fast as possible (the norm for games and demos, but not for windowed utilities), the root object has a method called <a class="el" href="class_ogre_1_1_root.html#a8eda253befda1255cbfd3b298f62449e" title="Starts / restarts the automatic rendering cycle. ">Ogre::Root::startRendering</a>, which when called will enter a continuous rendering loop which will only end when all rendering windows are closed, or any <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects indicate that they want to stop the cycle (see below for details of <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects).</p>
<h1><a class="anchor" id="The-RenderSystem-object"></a>
The RenderSystem object</h1>
<p>The <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object is actually an abstract class which defines the interface to the underlying 3D API. It is responsible for sending rendering operations to the API and setting all the various rendering options. This class is abstract because all the implementation is rendering API specific - there are API-specific subclasses for each rendering API (e.g. D3DRenderSystem for Direct3D). After the system has been initialised through <a class="el" href="class_ogre_1_1_root.html#a6ab8f204bbfb150ad180eb6e78e4639c" title="Initialises the renderer. ">Ogre::Root::initialise</a>, the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object for the selected rendering API is available via the <a class="el" href="class_ogre_1_1_root.html#ab8ca3d5abebba7a14650db5f618286d0" title="Retrieve a pointer to the currently selected render system. ">Ogre::Root::getRenderSystem()</a> method.</p>
<p>However, a typical application should not normally need to manipulate the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object directly - everything you need for rendering objects and customising settings should be available on the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, Material and other scene-oriented classes. It’s only if you want to create multiple rendering windows (completely separate windows in this case, not multiple viewports like a split-screen effect which is done via the RenderWindow class) or access other advanced features that you need access to the RenderSystem object.</p>
<p>For this reason I will not discuss the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object further in these tutorials. You can assume the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> handles the calls to the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> at the appropriate times.</p>
<h1><a class="anchor" id="The-SceneManager-object"></a>
The SceneManager object</h1>
<p>Apart from the <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object, this is probably the most critical part of the system from the application’s point of view. Certainly it will be the object which is most used by the application. The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> is in charge of the contents of the scene which is to be rendered by the engine. It is responsible for organising the contents using whatever technique it deems best, for creating and managing all the cameras, movable objects (entities), lights and materials (surface properties of objects), and for managing the ’world geometry’ which is the sprawling static geometry usually used to represent the immovable parts of a scene.</p>
<p>It is to the SceneManager that you go when you want to create a camera for the scene. It’s also where you go to retrieve or to remove a light from the scene. There is no need for your application to keep lists of objects, the SceneManager keeps a named set of all of the scene objects for you to access, should you need them. Look in the main documentation under the getCamera, getLight, getEntity etc methods.</p>
<p>The SceneManager also sends the scene to the RenderSystem object when it is time to render the scene. You never have to call the <a class="el" href="class_ogre_1_1_scene_manager.html#a29ba8a2cad151d307d04781f749e7e84" title="Prompts the class to send its contents to the renderer. ">Ogre::SceneManager::_renderScene</a> method directly though - it is called automatically whenever a rendering target is asked to update.</p>
<p>So most of your interaction with the SceneManager is during scene setup. You’re likely to call a great number of methods (perhaps driven by some input file containing the scene data) in order to set up your scene. You can also modify the contents of the scene dynamically during the rendering cycle if you create your own FrameListener object (see later).</p>
<p>Because different scene types require very different algorithmic approaches to deciding which objects get sent to the RenderSystem in order to attain good rendering performance, the SceneManager class is designed to be subclassed for different scene types. The default SceneManager object will render a scene, but it does little or no scene organisation and you should not expect the results to be high performance in the case of large scenes. The intention is that specialisations will be created for each type of scene such that under the surface the subclass will optimise the scene organisation for best performance given assumptions which can be made for that scene type. An example is the BspSceneManager which optimises rendering for large indoor levels based on a Binary Space Partition (BSP) tree.</p>
<p>The application using OGRE does not have to know which subclasses are available. The application simply calls <a class="el" href="class_ogre_1_1_root.html#a8bb8655f823a98007775f69278c1b0f9" title="create a default scene manager ">Ogre::Root::createSceneManager</a>(..) passing as a parameter one of a number of scene types (e.g. <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a0af8f844019e52fe112e9a946a919fcd">Ogre::ST_GENERIC</a>, <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a93f2170ad3a8132f29264dc3642a2dcf">Ogre::ST_INTERIOR</a> etc). OGRE will automatically use the best SceneManager subclass available for that scene type, or default to the basic SceneManager if a specialist one is not available. This allows the developers of OGRE to add new scene specialisations later and thus optimise previously unoptimised scene types without the user applications having to change any code.</p>
<h1><a class="anchor" id="The-ResourceGroupManager-Object"></a>
The ResourceGroupManager Object</h1>
<p>The <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a> class is actually a ’hub’ for loading of reusable resources like textures and meshes. It is the place that you define groups for your resources, so they may be unloaded and reloaded when you want. Servicing it are a number of ResourceManagers which manage the individual types of resource, like <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> or <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a>. In this context, resources are sets of data which must be loaded from somewhere to provide OGRE with the data it needs.</p>
<p>ResourceManagers ensure that resources are only loaded once and shared throughout the OGRE engine. They also manage the memory requirements of the resources they look after. They can also search in a number of locations for the resources they need, including multiple search paths and compressed archives (ZIP files).</p>
<p>Most of the time you won’t interact with resource managers directly. Resource managers will be called by other parts of the OGRE system as required, for example when you request for a texture to be added to a Material, the <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> will be called for you. If you like, you can call the appropriate resource manager directly to preload resources (if for example you want to prevent disk access later on) but most of the time it’s ok to let OGRE decide when to do it.</p>
<p>One thing you will want to do is to tell the resource managers where to look for resources. You do this via Root::getSingleton().addResourceLocation, which actually passes the information on to <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a>.</p>
<p>Because there is only ever 1 instance of each resource manager in the engine, if you do want to get a reference to a resource manager use the following syntax:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().someMethod()</div><div class="line"><a class="code" href="class_ogre_1_1_mesh_manager.html#a2efefa307b7987c5fcff7e8c47f79758">Ogre::MeshManager::getSingleton</a>().someMethod()</div></div><!-- fragment --> <h1><a class="anchor" id="The-Mesh-Object"></a>
The Mesh Object</h1>
<p>A <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object represents a discrete model, a set of geometry which is self-contained and is typically fairly small on a world scale. <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are assumed to represent movable objects and are not used for the sprawling level geometry typically used to create backgrounds.</p>
<p><a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are a type of resource, and are managed by the MeshManager resource manager. They are typically loaded from OGRE’s custom object format, the ’.mesh’ format. Mesh files are typically created by exporting from a modelling tool See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> and can be manipulated through various <a class="el" href="_mesh-_tools.html">Mesh Tools</a></p>
<p>You can also create Mesh objects manually by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html#ad99f5f2e2d045e5a4af301974e45c918" title="Creates a new Mesh specifically for manual definition rather than loading from an object file...">Ogre::MeshManager::createManual</a> method. This way you can define the geometry yourself, but this is outside the scope of this manual.</p>
<p>Mesh objects are the basis for the individual movable objects in the world, which are called <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>.</p>
<p>Mesh objects can also be animated using See <a class="el" href="_animation.html#Skeletal-Animation">Skeletal Animation</a>.</p>
<h1><a class="anchor" id="Entities"></a>
Entities</h1>
<p>An entity is an instance of a movable object in the scene. It could be a car, a person, a dog, a shuriken, whatever. The only assumption is that it does not necessarily have a fixed position in the world.</p>
<p>Entities are based on discrete meshes, i.e. collections of geometry which are self-contained and typically fairly small on a world scale, which are represented by the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object. Multiple entities can be based on the same mesh, since often you want to create multiple copies of the same type of object in a scene.</p>
<p>You create an entity by calling the <a class="el" href="class_ogre_1_1_scene_manager.html#afb393cca49de8b928f7dd60838047185" title="Create an Entity (instance of a discrete mesh). ">Ogre::SceneManager::createEntity</a> method, giving it a name and specifying the name of the mesh object which it will be based on (e.g. ’muscleboundhero.mesh’). The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> will ensure that the mesh is loaded by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a> resource manager for you. Only one copy of the Mesh will be loaded.</p>
<p>Ogre::Entities are not deemed to be a part of the scene until you attach them to a <a class="el" href="class_ogre_1_1_scene_node.html" title="Class representing a node in the scene graph. ">Ogre::SceneNode</a> (see the section below). By attaching entities to SceneNodes, you can create complex hierarchical relationships between the positions and orientations of entities. You then modify the positions of the nodes to indirectly affect the entity positions.</p>
<p>When a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> is loaded, it automatically comes with a number of materials defined. It is possible to have more than one material attached to a mesh - different parts of the mesh may use different materials. Any entity created from the mesh will automatically use the default materials. However, you can change this on a per-entity basis if you like so you can create a number of entities based on the same mesh but with different textures etc.</p>
<p>To understand how this works, you have to know that all Mesh objects are actually composed of <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects, each of which represents a part of the mesh using one Material. If a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> uses only one <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a>, it will only have one <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a>.</p>
<p>When an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> is created based on this Mesh, it is composed of (possibly) multiple <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects, each matching 1 for 1 with the <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects from the original Mesh. You can access the <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects using the <a class="el" href="class_ogre_1_1_entity.html#a23fc34c5e2eff03aae4cbf800c142724" title="Gets a pointer to a SubEntity, ie a part of an Entity. ">Ogre::Entity::getSubEntity</a> method. Once you have a reference to a <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a>, you can change the material it uses by calling it’s setMaterialName method. In this way you can make an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> deviate from the default materials and thus create an individual looking version of it.</p>
<h1><a class="anchor" id="Materials"></a>
Materials</h1>
<p>The <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a> object controls how objects in the scene are rendered. It specifies what basic surface properties objects have such as reflectance of colours, shininess etc, how many texture layers are present, what images are on them and how they are blended together, what special effects are applied such as environment mapping, what culling mode is used, how the textures are filtered etc.</p>
<p>Materials can either be set up programmatically, by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a> and tweaking the settings, or by specifying it in a ’script’ which is loaded at runtime. See <a class="el" href="_material-_scripts.html">Material Scripts</a> for more info.</p>
<p>Basically everything about the appearance of an object apart from it’s shape is controlled by the Material class.</p>
<p>The SceneManager class manages the master list of materials available to the scene. The list can be added to by the application by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a>, or by loading a Mesh (which will in turn load material properties). Whenever materials are added to the SceneManager, they start off with a default set of properties; these are defined by OGRE as the following:</p>
<ul>
<li>ambient reflectance = ColourValue::White (full)</li>
<li>diffuse reflectance = ColourValue::White (full)</li>
<li>specular reflectance = ColourValue::Black (none)</li>
<li>emissive = ColourValue::Black (none)</li>
<li>shininess = 0 (not shiny)</li>
<li>No texture layers (&amp; hence no textures)</li>
<li>SourceBlendFactor = SBF_ONE, DestBlendFactor = SBF_ZERO (opaque)</li>
<li>Depth buffer checking on</li>
<li>Depth buffer writing on</li>
<li>Depth buffer comparison function = CMPF_LESS_EQUAL</li>
<li>Culling mode = CULL_CLOCKWISE</li>
<li>Ambient lighting in scene = ColourValue(0.5, 0.5, 0.5) (mid-grey)</li>
<li>Dynamic lighting enabled</li>
<li>Gourad shading mode</li>
<li>Solid polygon mode</li>
<li>Bilinear texture filtering</li>
</ul>
<p>You can alter these settings by calling <a class="el" href="class_ogre_1_1_material_manager.html#ac70230fa9f123976d7f2b1257bd94bd4" title="Returns a pointer to the default Material settings. ">Ogre::MaterialManager::getDefaultSettings()</a> and making the required changes to the Material which is returned.</p>
<p>Entities automatically have Material’s associated with them if they use a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object, since the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object typically sets up it’s required materials on loading. You can also customise the material used by an entity as described in <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>. Just create a new Material, set it up how you like (you can copy an existing material into it if you like using a standard assignment statement) and point the SubEntity entries at it using <a class="el" href="class_ogre_1_1_sub_entity.html#ab8af6685128c68a5d7863bb5238ebc0f" title="Sets the name of the Material to be used. ">Ogre::SubEntity::setMaterialName()</a>.</p>
<h1><a class="anchor" id="Overlays"></a>
Overlays</h1>
<p>Overlays allow you to render 2D and 3D elements on top of the normal scene contents to create effects like heads-up displays (HUDs), menu systems, status panels etc. The frame rate statistics panel which comes as standard with OGRE is an example of an overlay. Overlays can contain 2D or 3D elements. 2D elements are used for HUDs, and 3D elements can be used to create cockpits or any other 3D object which you wish to be rendered on top of the rest of the scene.</p>
<p>You can create overlays either through the <a class="el" href="class_ogre_1_1_overlay_manager.html#ab6eb3dcc484ba5b06904ab2fb6b1aa23" title="Create a new Overlay. ">Ogre::OverlayManager::create</a> method, or you can define them in an .overlay script. See <a class="el" href="_overlay-_scripts.html">Overlay Scripts</a> for more info. In reality the latter is likely to be the most practical because it is easier to tweak (without the need to recompile the code). Note that you can define as many overlays as you like: they all start off life hidden, and you display them by calling <a class="el" href="class_ogre_1_1_overlay.html#a3c565f3e5cf61f43fa33542d713e8ccf" title="Shows the overlay if it was hidden. ">Ogre::Overlay::show</a>. You can also show multiple overlays at once, and their Z order is determined by the <a class="el" href="class_ogre_1_1_overlay.html#a5d8481d5fa0990846e53345f1a1cc705" title="Alters the Z-order of this overlay. ">Ogre::Overlay::setZOrder()</a> method.</p>
<p><a class="anchor" id="Notes-on-Integration"></a></p>
<h2>Notes on Integration</h2>
<p>The OverlaySystem is its own component, you need to manually initialize it, with the following two lines of code (mSceneMgr is a pointer to your current <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>):</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>* pOverlaySystem = <span class="keyword">new</span> <a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>();</div><div class="line">mSceneMgr-&gt;addRenderQueueListener(pOverlaySystem);</div></div><!-- fragment --><p>One <a class="el" href="class_ogre_1_1_overlay_system.html" title="This class simplify initialization / finalization of the overlay system. ">Ogre::OverlaySystem</a> per application is enough but you need to call addRenderQueueListener once per SceneManager.</p>
<p><a class="anchor" id="Creating-2D-Elements"></a></p>
<h2>Creating 2D Elements</h2>
<p>The <a class="el" href="class_ogre_1_1_overlay_element.html" title="Abstract definition of a 2D element to be displayed in an Overlay. ">Ogre::OverlayElement</a> class abstracts the details of 2D elements which are added to overlays. All items which can be added to overlays are derived from this class. It is possible (and encouraged) for users of OGRE to define their own custom subclasses of OverlayElement in order to provide their own user controls. The key common features of all OverlayElements are things like size, position, basic material name etc. Subclasses extend this behaviour to include more complex properties and behaviour.</p>
<p>An important built-in subclass of OverlayElement is <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a>. OverlayContainer is the same as a OverlayElement, except that it can contain other OverlayElements, grouping them together (allowing them to be moved together for example) and providing them with a local coordinate origin for easier lineup.</p>
<p>The third important class is <a class="el" href="class_ogre_1_1_overlay_manager.html" title="Manages Overlay objects, parsing them from .overlay files and storing a lookup library of them...">Ogre::OverlayManager</a>. Whenever an application wishes to create a 2D element to add to an overlay (or a container), it should call <a class="el" href="class_ogre_1_1_overlay_manager.html#a58166a773d27c22437323e8ef710406b" title="Creates a new OverlayElement of the type requested. ">Ogre::OverlayManager::createOverlayElement</a>. The type of element you wish to create is identified by a string, the reason being that it allows plugins to register new types of OverlayElement for you to create without you having to link specifically to those libraries. For example, to create a panel (a plain rectangular area which can contain other OverlayElements) you would call <code>OverlayManager::getSingleton().createOverlayElement("Panel", "myNewPanel");</code>.</p>
<p><a class="anchor" id="Adding-2D-Elements-to-the-Overlay"></a></p>
<h2>Adding 2D Elements to the Overlay</h2>
<p>Only OverlayContainers can be added direct to an overlay. The reason is that each level of container establishes the Zorder of the elements contained within it, so if you nest several containers, inner containers have a higher Z-order than outer ones to ensure they are displayed correctly. To add a container (such as a Panel) to the overlay, simply call <a class="el" href="class_ogre_1_1_overlay.html#a702b15ebaa6d3f8dbb2918500b76c9f0" title="Adds a 2D &#39;container&#39; to the overlay. ">Ogre::Overlay::add2D</a>.</p>
<p>If you wish to add child elements to that container, call <a class="el" href="class_ogre_1_1_overlay_container.html#aa9eec8f6dca98ac61766fc4785b84661" title="Adds another OverlayElement to this container. ">Ogre::OverlayContainer::addChild</a>. Child elements can be Ogre::OverlayElements or <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a> instances themselves. Remember that the position of a child element is relative to the top-left corner of it’s parent.</p>
<p><a class="anchor" id="A-word-about-2D-coordinates"></a></p>
<h2>A word about 2D coordinates</h2>
<p>OGRE allows you to place and size elements based on 2 coordinate systems: <b>relative</b> and <b>pixel</b> based.</p>
<dl compact="compact">
<dt>Pixel Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want to specify an exact size for your overlay items, and you don’t mind if those items get smaller on the screen if you increase the screen resolution (in fact you might want this). In this mode the only way to put something in the middle or at the right or bottom of the screen reliably in any resolution is to use the aligning options, whilst in relative mode you can do it just by using the right relative coordinates. This mode is very simple, the top-left of the screen is (0,0) and the bottom-right of the screen depends on the resolution. As mentioned above, you can use the aligning options to make the horizontal and vertical coordinate origins the right, bottom or center of the screen if you want to place pixel items in these locations without knowing the resolution.</p>
<p class="enddd"></p>
</dd>
<dt>Relative Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want items in the overlay to be the same size on the screen no matter what the resolution. In relative mode, the top-left of the screen is (0,0) and the bottom-right is (1,1). So if you place an element at (0.5, 0.5), it’s top-left corner is placed exactly in the center of the screen, no matter what resolution the application is running in. The same principle applies to sizes; if you set the width of an element to 0.5, it covers half the width of the screen. Note that because the aspect ratio of the screen is typically 1.3333 : 1 (width : height), an element with dimensions (0.25, 0.25) will not be square, but it will take up exactly 1/16th of the screen in area terms. If you want square-looking areas you will have to compensate using the typical aspect ratio e.g. use (0.1875, 0.25) instead.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Transforming-Overlays"></a> </p><h2>Transforming Overlays</h2>
<p>Another nice feature of overlays is being able to rotate, scroll and scale them as a whole. You can use this for zooming in / out menu systems, dropping them in from off screen and other nice effects. See the <a class="el" href="class_ogre_1_1_overlay.html#ae07b1a57498aad0299f0781b09ca9b39" title="Scrolls the overlay by the offsets provided. ">Ogre::Overlay::scroll</a>, <a class="el" href="class_ogre_1_1_overlay.html#a8282553a59253170ddd0488ab7179792" title="Adds the passed in angle to the rotation applied to this overlay. ">Ogre::Overlay::rotate</a> and <a class="el" href="class_ogre_1_1_overlay.html#adea3b88baa3697a657e7b5785f0ffedb" title="Sets the scaling factor of this overlay. ">Ogre::Overlay::setScale</a> methods for more information.</p>
<p><a class="anchor" id="GUI-systems"></a></p>
<h2>GUI systems</h2>
<p>Overlays are only really designed for non-interactive screen elements, although you can create a simple GUI using the <a class="el" href="trays.html">Trays System</a>. For a far more complete GUI solution, we recommend or <a href="https://github.com/OGRECave/ogre-imgui">Dear ImGui</a>, <a href="http://www.cegui.org.uk">CEGui</a> or <a href="http://mygui.info/">MyGUI</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: The Core Objects</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_the-_core-_objects.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">The Core Objects </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#The-Root-Object">The Root object</a></li>
<li class="level1"><a href="#The-RenderSystem-object">The RenderSystem object</a></li>
<li class="level1"><a href="#The-SceneManager-object">The SceneManager object</a></li>
<li class="level1"><a href="#The-ResourceGroupManager-Object">The ResourceGroupManager Object</a></li>
<li class="level1"><a href="#The-Mesh-Object">The Mesh Object</a></li>
<li class="level1"><a href="#Entities">Entities</a></li>
<li class="level1"><a href="#Materials">Materials</a></li>
<li class="level1"><a href="#Overlays">Overlays</a></li>
</ul>
</div>
<div class="textblock"><p>This tutorial gives you a quick summary of the core objects that you will use in OGRE and what they are used for.</p>
<p><a class="anchor" id="A-Word-About-Namespaces"></a></p>
<h1>A Word About Namespaces</h1>
<p>OGRE uses a C++ feature called namespaces. This lets you put classes, enums, structures, anything really within a ’namespace’ scope which is an easy way to prevent name clashes, i.e. situations where you have 2 things called the same thing. Since OGRE is designed to be used inside other applications, I wanted to be sure that name clashes would not be a problem. Some people prefix their classes/types with a short code because some compilers don’t support namespaces, but I chose to use them because they are the ’right’ way to do it. Sorry if you have a non-compliant compiler, but hey, the C++ standard has been defined for years, so compiler writers really have no excuse anymore. If your compiler doesn’t support namespaces then it’s probably because it’s sh*t - get a better one. ;)</p>
<p>This means every class, type etc should be prefixed with <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, e.g. <a class="el" href="class_ogre_1_1_camera.html" title="A viewpoint from which the scene will be rendered. ">Ogre::Camera</a>, <a class="el" href="class_ogre_1_1_vector3.html" title="Standard 3-dimensional vector. ">Ogre::Vector3</a> etc which means if elsewhere in your application you have used a Vector3 type you won’t get name clashes. To avoid lots of extra typing you can add a ’using namespace <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>;’ statement to your code which means you don’t have to type the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> prefix unless there is ambiguity (in the situation where you have another definition with the same name).</p>
<p><a class="anchor" id="Overview-from-10_002c000-feet"></a></p>
<h1>Overview from 10,000 feet</h1>
<p>Shown below is a diagram of some of the core objects and where they ’sit’ in the grand scheme of things. This is not all the classes by a long shot, just a few examples of the more more significant ones to give you an idea of how it slots together. </p><div class="image">
<object type="image/svg+xml" data="uml-overview.svg">uml-overview.svg</object>
</div>
<p>At the very top of the diagram is the Root object. This is your ’way in’ to the OGRE system, and it’s where you tend to create the top-level objects that you need to deal with, like scene managers, rendering systems and render windows, loading plugins, all the fundamental stuff. If you don’t know where to start, Root is it for almost everything, although often it will just give you another object which will actually do the detail work, since Root itself is more of an organiser and facilitator object.</p>
<p>The majority of rest of OGRE’s classes fall into one of 3 roles:</p>
<dl compact="compact">
<dt>Scene Management </dt>
<dd><p class="startdd"></p>
<p>This is about the contents of your scene, how it’s structured, how it’s viewed from cameras, etc. Objects in this area are responsible for giving you a natural declarative interface to the world you’re building; i.e. you don’t tell OGRE "set these render states and then render 3 polygons", you tell it "I want an object here, here and here, with these materials on them, rendered from this view", and let it get on with it.</p>
<p class="enddd"></p>
</dd>
<dt>Resource Management </dt>
<dd><p class="startdd"></p>
<p>All rendering needs resources, whether it’s geometry, textures, fonts, whatever. It’s important to manage the loading, re-use and unloading of these things carefully, so that’s what classes in this area do.</p>
<p class="enddd"></p>
</dd>
<dt>Rendering </dt>
<dd><p class="startdd"></p>
<p>Finally, there’s getting the visuals on the screen - this is about the lower-level end of the rendering pipeline, the specific rendering system API objects like buffers, render states and the like and pushing it all down the pipeline. Classes in the Scene Management subsystem use this to get their higher-level scene information onto the screen.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You’ll notice that scattered around the edge are a number of plugins. OGRE is designed to be extended, and plugins are the usual way to go about it. Many of the classes in OGRE can be subclassed and extended, whether it’s changing the scene organisation through a custom SceneManager, adding a new render system implementation (e.g. Direct3D or OpenGL), or providing a way to load resources from another source (say from a web location or a database). Again this is just a small smattering of the kinds of things plugins can do, but as you can see they can plug in to almost any aspect of the system. This way, OGRE isn’t just a solution for one narrowly defined problem, it can extend to pretty much anything you need it to do.</p>
<h1><a class="anchor" id="The-Root-Object"></a>
The Root object</h1>
<p>The <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object is the entry point to the OGRE system. This object MUST be the first one to be created, and the last one to be destroyed. In the example applications I chose to make an instance of Root a member of my application object which ensured that it was created as soon as my application object was, and deleted when the application object was deleted.</p>
<p>The root object lets you configure the system, for example through the <a class="el" href="class_ogre_1_1_root.html#aedc76ef3bdc6514bd4655be0d70d6bcd" title="Displays a dialog asking the user to choose system settings. ">Ogre::Root::showConfigDialog</a> method which is an extremely handy method which performs all render system options detection and shows a dialog for the user to customise resolution, colour depth, full screen options etc. It also sets the options the user selects so that you can initialise the system directly afterwards.</p>
<p>The root object is also your method for obtaining pointers to other objects in the system, such as the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> and various other resource managers. See below for details.</p>
<p>Finally, if you run OGRE in continuous rendering mode, i.e. you want to always refresh all the rendering targets as fast as possible (the norm for games and demos, but not for windowed utilities), the root object has a method called <a class="el" href="class_ogre_1_1_root.html#a8eda253befda1255cbfd3b298f62449e" title="Starts / restarts the automatic rendering cycle. ">Ogre::Root::startRendering</a>, which when called will enter a continuous rendering loop which will only end when all rendering windows are closed, or any <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects indicate that they want to stop the cycle (see below for details of <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects).</p>
<h1><a class="anchor" id="The-RenderSystem-object"></a>
The RenderSystem object</h1>
<p>The <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object is actually an abstract class which defines the interface to the underlying 3D API. It is responsible for sending rendering operations to the API and setting all the various rendering options. This class is abstract because all the implementation is rendering API specific - there are API-specific subclasses for each rendering API (e.g. D3DRenderSystem for Direct3D). After the system has been initialised through <a class="el" href="class_ogre_1_1_root.html#a6ab8f204bbfb150ad180eb6e78e4639c" title="Initialises the renderer. ">Ogre::Root::initialise</a>, the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object for the selected rendering API is available via the <a class="el" href="class_ogre_1_1_root.html#ab8ca3d5abebba7a14650db5f618286d0" title="Retrieve a pointer to the currently selected render system. ">Ogre::Root::getRenderSystem()</a> method.</p>
<p>However, a typical application should not normally need to manipulate the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object directly - everything you need for rendering objects and customising settings should be available on the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, Material and other scene-oriented classes. It’s only if you want to create multiple rendering windows (completely separate windows in this case, not multiple viewports like a split-screen effect which is done via the RenderWindow class) or access other advanced features that you need access to the RenderSystem object.</p>
<p>For this reason I will not discuss the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object further in these tutorials. You can assume the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> handles the calls to the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> at the appropriate times.</p>
<h1><a class="anchor" id="The-SceneManager-object"></a>
The SceneManager object</h1>
<p>Apart from the <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object, this is probably the most critical part of the system from the application’s point of view. Certainly it will be the object which is most used by the application. The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> is in charge of the contents of the scene which is to be rendered by the engine. It is responsible for organising the contents using whatever technique it deems best, for creating and managing all the cameras, movable objects (entities), lights and materials (surface properties of objects), and for managing the ’world geometry’ which is the sprawling static geometry usually used to represent the immovable parts of a scene.</p>
<p>It is to the SceneManager that you go when you want to create a camera for the scene. It’s also where you go to retrieve or to remove a light from the scene. There is no need for your application to keep lists of objects, the SceneManager keeps a named set of all of the scene objects for you to access, should you need them. Look in the main documentation under the getCamera, getLight, getEntity etc methods.</p>
<p>The SceneManager also sends the scene to the RenderSystem object when it is time to render the scene. You never have to call the <a class="el" href="class_ogre_1_1_scene_manager.html#a29ba8a2cad151d307d04781f749e7e84" title="Prompts the class to send its contents to the renderer. ">Ogre::SceneManager::_renderScene</a> method directly though - it is called automatically whenever a rendering target is asked to update.</p>
<p>So most of your interaction with the SceneManager is during scene setup. You’re likely to call a great number of methods (perhaps driven by some input file containing the scene data) in order to set up your scene. You can also modify the contents of the scene dynamically during the rendering cycle if you create your own FrameListener object (see later).</p>
<p>Because different scene types require very different algorithmic approaches to deciding which objects get sent to the RenderSystem in order to attain good rendering performance, the SceneManager class is designed to be subclassed for different scene types. The default SceneManager object will render a scene, but it does little or no scene organisation and you should not expect the results to be high performance in the case of large scenes. The intention is that specialisations will be created for each type of scene such that under the surface the subclass will optimise the scene organisation for best performance given assumptions which can be made for that scene type. An example is the BspSceneManager which optimises rendering for large indoor levels based on a Binary Space Partition (BSP) tree.</p>
<p>The application using OGRE does not have to know which subclasses are available. The application simply calls <a class="el" href="class_ogre_1_1_root.html#a8bb8655f823a98007775f69278c1b0f9" title="create a default scene manager ">Ogre::Root::createSceneManager</a>(..) passing as a parameter one of a number of scene types (e.g. <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a0af8f844019e52fe112e9a946a919fcd">Ogre::ST_GENERIC</a>, <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a93f2170ad3a8132f29264dc3642a2dcf">Ogre::ST_INTERIOR</a> etc). OGRE will automatically use the best SceneManager subclass available for that scene type, or default to the basic SceneManager if a specialist one is not available. This allows the developers of OGRE to add new scene specialisations later and thus optimise previously unoptimised scene types without the user applications having to change any code.</p>
<h1><a class="anchor" id="The-ResourceGroupManager-Object"></a>
The ResourceGroupManager Object</h1>
<p>The <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a> class is actually a ’hub’ for loading of reusable resources like textures and meshes. It is the place that you define groups for your resources, so they may be unloaded and reloaded when you want. Servicing it are a number of ResourceManagers which manage the individual types of resource, like <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> or <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a>. In this context, resources are sets of data which must be loaded from somewhere to provide OGRE with the data it needs.</p>
<p>ResourceManagers ensure that resources are only loaded once and shared throughout the OGRE engine. They also manage the memory requirements of the resources they look after. They can also search in a number of locations for the resources they need, including multiple search paths and compressed archives (ZIP files).</p>
<p>Most of the time you won’t interact with resource managers directly. Resource managers will be called by other parts of the OGRE system as required, for example when you request for a texture to be added to a Material, the <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> will be called for you. If you like, you can call the appropriate resource manager directly to preload resources (if for example you want to prevent disk access later on) but most of the time it’s ok to let OGRE decide when to do it.</p>
<p>One thing you will want to do is to tell the resource managers where to look for resources. You do this via Root::getSingleton().addResourceLocation, which actually passes the information on to <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a>.</p>
<p>Because there is only ever 1 instance of each resource manager in the engine, if you do want to get a reference to a resource manager use the following syntax:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().someMethod()</div><div class="line"><a class="code" href="class_ogre_1_1_mesh_manager.html#a2efefa307b7987c5fcff7e8c47f79758">Ogre::MeshManager::getSingleton</a>().someMethod()</div></div><!-- fragment --> <h1><a class="anchor" id="The-Mesh-Object"></a>
The Mesh Object</h1>
<p>A <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object represents a discrete model, a set of geometry which is self-contained and is typically fairly small on a world scale. <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are assumed to represent movable objects and are not used for the sprawling level geometry typically used to create backgrounds.</p>
<p><a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are a type of resource, and are managed by the MeshManager resource manager. They are typically loaded from OGRE’s custom object format, the ’.mesh’ format. Mesh files are typically created by exporting from a modelling tool See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> and can be manipulated through various <a class="el" href="_mesh-_tools.html">Mesh Tools</a></p>
<p>You can also create Mesh objects manually by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html#ad99f5f2e2d045e5a4af301974e45c918" title="Creates a new Mesh specifically for manual definition rather than loading from an object file...">Ogre::MeshManager::createManual</a> method. This way you can define the geometry yourself, but this is outside the scope of this manual.</p>
<p>Mesh objects are the basis for the individual movable objects in the world, which are called <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>.</p>
<p>Mesh objects can also be animated using See <a class="el" href="_animation.html#Skeletal-Animation">Skeletal Animation</a>.</p>
<h1><a class="anchor" id="Entities"></a>
Entities</h1>
<p>An entity is an instance of a movable object in the scene. It could be a car, a person, a dog, a shuriken, whatever. The only assumption is that it does not necessarily have a fixed position in the world.</p>
<p>Entities are based on discrete meshes, i.e. collections of geometry which are self-contained and typically fairly small on a world scale, which are represented by the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object. Multiple entities can be based on the same mesh, since often you want to create multiple copies of the same type of object in a scene.</p>
<p>You create an entity by calling the <a class="el" href="class_ogre_1_1_scene_manager.html#afb393cca49de8b928f7dd60838047185" title="Create an Entity (instance of a discrete mesh). ">Ogre::SceneManager::createEntity</a> method, giving it a name and specifying the name of the mesh object which it will be based on (e.g. ’muscleboundhero.mesh’). The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> will ensure that the mesh is loaded by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a> resource manager for you. Only one copy of the Mesh will be loaded.</p>
<p>Ogre::Entities are not deemed to be a part of the scene until you attach them to a <a class="el" href="class_ogre_1_1_scene_node.html" title="Class representing a node in the scene graph. ">Ogre::SceneNode</a> (see the section below). By attaching entities to SceneNodes, you can create complex hierarchical relationships between the positions and orientations of entities. You then modify the positions of the nodes to indirectly affect the entity positions.</p>
<p>When a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> is loaded, it automatically comes with a number of materials defined. It is possible to have more than one material attached to a mesh - different parts of the mesh may use different materials. Any entity created from the mesh will automatically use the default materials. However, you can change this on a per-entity basis if you like so you can create a number of entities based on the same mesh but with different textures etc.</p>
<p>To understand how this works, you have to know that all Mesh objects are actually composed of <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects, each of which represents a part of the mesh using one Material. If a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> uses only one <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a>, it will only have one <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a>.</p>
<p>When an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> is created based on this Mesh, it is composed of (possibly) multiple <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects, each matching 1 for 1 with the <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects from the original Mesh. You can access the <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects using the <a class="el" href="class_ogre_1_1_entity.html#a23fc34c5e2eff03aae4cbf800c142724" title="Gets a pointer to a SubEntity, ie a part of an Entity. ">Ogre::Entity::getSubEntity</a> method. Once you have a reference to a <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a>, you can change the material it uses by calling it’s setMaterialName method. In this way you can make an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> deviate from the default materials and thus create an individual looking version of it.</p>
<h1><a class="anchor" id="Materials"></a>
Materials</h1>
<p>The <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a> object controls how objects in the scene are rendered. It specifies what basic surface properties objects have such as reflectance of colours, shininess etc, how many texture layers are present, what images are on them and how they are blended together, what special effects are applied such as environment mapping, what culling mode is used, how the textures are filtered etc.</p>
<p>Materials can either be set up programmatically, by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a> and tweaking the settings, or by specifying it in a ’script’ which is loaded at runtime. See <a class="el" href="_material-_scripts.html">Material Scripts</a> for more info.</p>
<p>Basically everything about the appearance of an object apart from it’s shape is controlled by the Material class.</p>
<p>The SceneManager class manages the master list of materials available to the scene. The list can be added to by the application by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a>, or by loading a Mesh (which will in turn load material properties). Whenever materials are added to the SceneManager, they start off with a default set of properties; these are defined by OGRE as the following:</p>
<ul>
<li>ambient reflectance = ColourValue::White (full)</li>
<li>diffuse reflectance = ColourValue::White (full)</li>
<li>specular reflectance = ColourValue::Black (none)</li>
<li>emissive = ColourValue::Black (none)</li>
<li>shininess = 0 (not shiny)</li>
<li>No texture layers (&amp; hence no textures)</li>
<li>SourceBlendFactor = SBF_ONE, DestBlendFactor = SBF_ZERO (opaque)</li>
<li>Depth buffer checking on</li>
<li>Depth buffer writing on</li>
<li>Depth buffer comparison function = CMPF_LESS_EQUAL</li>
<li>Culling mode = CULL_CLOCKWISE</li>
<li>Ambient lighting in scene = ColourValue(0.5, 0.5, 0.5) (mid-grey)</li>
<li>Dynamic lighting enabled</li>
<li>Gourad shading mode</li>
<li>Solid polygon mode</li>
<li>Bilinear texture filtering</li>
</ul>
<p>You can alter these settings by calling <a class="el" href="class_ogre_1_1_material_manager.html#ac70230fa9f123976d7f2b1257bd94bd4" title="Returns a pointer to the default Material settings. ">Ogre::MaterialManager::getDefaultSettings()</a> and making the required changes to the Material which is returned.</p>
<p>Entities automatically have Material’s associated with them if they use a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object, since the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object typically sets up it’s required materials on loading. You can also customise the material used by an entity as described in <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>. Just create a new Material, set it up how you like (you can copy an existing material into it if you like using a standard assignment statement) and point the SubEntity entries at it using <a class="el" href="class_ogre_1_1_sub_entity.html#ab8af6685128c68a5d7863bb5238ebc0f" title="Sets the name of the Material to be used. ">Ogre::SubEntity::setMaterialName()</a>.</p>
<h1><a class="anchor" id="Overlays"></a>
Overlays</h1>
<p>Overlays allow you to render 2D and 3D elements on top of the normal scene contents to create effects like heads-up displays (HUDs), menu systems, status panels etc. The frame rate statistics panel which comes as standard with OGRE is an example of an overlay. Overlays can contain 2D or 3D elements. 2D elements are used for HUDs, and 3D elements can be used to create cockpits or any other 3D object which you wish to be rendered on top of the rest of the scene.</p>
<p>You can create overlays either through the <a class="el" href="class_ogre_1_1_overlay_manager.html#ab6eb3dcc484ba5b06904ab2fb6b1aa23" title="Create a new Overlay. ">Ogre::OverlayManager::create</a> method, or you can define them in an .overlay script. See <a class="el" href="_overlay-_scripts.html">Overlay Scripts</a> for more info. In reality the latter is likely to be the most practical because it is easier to tweak (without the need to recompile the code). Note that you can define as many overlays as you like: they all start off life hidden, and you display them by calling <a class="el" href="class_ogre_1_1_overlay.html#a3c565f3e5cf61f43fa33542d713e8ccf" title="Shows the overlay if it was hidden. ">Ogre::Overlay::show</a>. You can also show multiple overlays at once, and their Z order is determined by the <a class="el" href="class_ogre_1_1_overlay.html#a5d8481d5fa0990846e53345f1a1cc705" title="Alters the Z-order of this overlay. ">Ogre::Overlay::setZOrder()</a> method.</p>
<p><a class="anchor" id="Notes-on-Integration"></a></p>
<h2>Notes on Integration</h2>
<p>The OverlaySystem is its own component, you need to manually initialize it, with the following two lines of code (mSceneMgr is a pointer to your current <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>):</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>* pOverlaySystem = <span class="keyword">new</span> <a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>();</div><div class="line">mSceneMgr-&gt;addRenderQueueListener(pOverlaySystem);</div></div><!-- fragment --><p>One <a class="el" href="class_ogre_1_1_overlay_system.html" title="This class simplify initialization / finalization of the overlay system. ">Ogre::OverlaySystem</a> per application is enough but you need to call addRenderQueueListener once per SceneManager.</p>
<p><a class="anchor" id="Creating-2D-Elements"></a></p>
<h2>Creating 2D Elements</h2>
<p>The <a class="el" href="class_ogre_1_1_overlay_element.html" title="Abstract definition of a 2D element to be displayed in an Overlay. ">Ogre::OverlayElement</a> class abstracts the details of 2D elements which are added to overlays. All items which can be added to overlays are derived from this class. It is possible (and encouraged) for users of OGRE to define their own custom subclasses of OverlayElement in order to provide their own user controls. The key common features of all OverlayElements are things like size, position, basic material name etc. Subclasses extend this behaviour to include more complex properties and behaviour.</p>
<p>An important built-in subclass of OverlayElement is <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a>. OverlayContainer is the same as a OverlayElement, except that it can contain other OverlayElements, grouping them together (allowing them to be moved together for example) and providing them with a local coordinate origin for easier lineup.</p>
<p>The third important class is <a class="el" href="class_ogre_1_1_overlay_manager.html" title="Manages Overlay objects, parsing them from .overlay files and storing a lookup library of them...">Ogre::OverlayManager</a>. Whenever an application wishes to create a 2D element to add to an overlay (or a container), it should call <a class="el" href="class_ogre_1_1_overlay_manager.html#a58166a773d27c22437323e8ef710406b" title="Creates a new OverlayElement of the type requested. ">Ogre::OverlayManager::createOverlayElement</a>. The type of element you wish to create is identified by a string, the reason being that it allows plugins to register new types of OverlayElement for you to create without you having to link specifically to those libraries. For example, to create a panel (a plain rectangular area which can contain other OverlayElements) you would call <code>OverlayManager::getSingleton().createOverlayElement("Panel", "myNewPanel");</code>.</p>
<p><a class="anchor" id="Adding-2D-Elements-to-the-Overlay"></a></p>
<h2>Adding 2D Elements to the Overlay</h2>
<p>Only OverlayContainers can be added direct to an overlay. The reason is that each level of container establishes the Zorder of the elements contained within it, so if you nest several containers, inner containers have a higher Z-order than outer ones to ensure they are displayed correctly. To add a container (such as a Panel) to the overlay, simply call <a class="el" href="class_ogre_1_1_overlay.html#a702b15ebaa6d3f8dbb2918500b76c9f0" title="Adds a 2D &#39;container&#39; to the overlay. ">Ogre::Overlay::add2D</a>.</p>
<p>If you wish to add child elements to that container, call <a class="el" href="class_ogre_1_1_overlay_container.html#aa9eec8f6dca98ac61766fc4785b84661" title="Adds another OverlayElement to this container. ">Ogre::OverlayContainer::addChild</a>. Child elements can be Ogre::OverlayElements or <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a> instances themselves. Remember that the position of a child element is relative to the top-left corner of it’s parent.</p>
<p><a class="anchor" id="A-word-about-2D-coordinates"></a></p>
<h2>A word about 2D coordinates</h2>
<p>OGRE allows you to place and size elements based on 2 coordinate systems: <b>relative</b> and <b>pixel</b> based.</p>
<dl compact="compact">
<dt>Pixel Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want to specify an exact size for your overlay items, and you don’t mind if those items get smaller on the screen if you increase the screen resolution (in fact you might want this). In this mode the only way to put something in the middle or at the right or bottom of the screen reliably in any resolution is to use the aligning options, whilst in relative mode you can do it just by using the right relative coordinates. This mode is very simple, the top-left of the screen is (0,0) and the bottom-right of the screen depends on the resolution. As mentioned above, you can use the aligning options to make the horizontal and vertical coordinate origins the right, bottom or center of the screen if you want to place pixel items in these locations without knowing the resolution.</p>
<p class="enddd"></p>
</dd>
<dt>Relative Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want items in the overlay to be the same size on the screen no matter what the resolution. In relative mode, the top-left of the screen is (0,0) and the bottom-right is (1,1). So if you place an element at (0.5, 0.5), it’s top-left corner is placed exactly in the center of the screen, no matter what resolution the application is running in. The same principle applies to sizes; if you set the width of an element to 0.5, it covers half the width of the screen. Note that because the aspect ratio of the screen is typically 1.3333 : 1 (width : height), an element with dimensions (0.25, 0.25) will not be square, but it will take up exactly 1/16th of the screen in area terms. If you want square-looking areas you will have to compensate using the typical aspect ratio e.g. use (0.1875, 0.25) instead.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Transforming-Overlays"></a> </p><h2>Transforming Overlays</h2>
<p>Another nice feature of overlays is being able to rotate, scroll and scale them as a whole. You can use this for zooming in / out menu systems, dropping them in from off screen and other nice effects. See the <a class="el" href="class_ogre_1_1_overlay.html#ae07b1a57498aad0299f0781b09ca9b39" title="Scrolls the overlay by the offsets provided. ">Ogre::Overlay::scroll</a>, <a class="el" href="class_ogre_1_1_overlay.html#a8282553a59253170ddd0488ab7179792" title="Adds the passed in angle to the rotation applied to this overlay. ">Ogre::Overlay::rotate</a> and <a class="el" href="class_ogre_1_1_overlay.html#adea3b88baa3697a657e7b5785f0ffedb" title="Sets the scaling factor of this overlay. ">Ogre::Overlay::setScale</a> methods for more information.</p>
<p><a class="anchor" id="GUI-systems"></a></p>
<h2>GUI systems</h2>
<p>Overlays are only really designed for non-interactive screen elements, although you can create a simple GUI using the <a class="el" href="trays.html">Trays System</a>. For a far more complete GUI solution, we recommend or <a href="https://github.com/OGRECave/ogre-imgui">Dear ImGui</a>, <a href="http://www.cegui.org.uk">CEGui</a> or <a href="http://mygui.info/">MyGUI</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: The Core Objects</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_the-_core-_objects.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">The Core Objects </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#The-Root-Object">The Root object</a></li>
<li class="level1"><a href="#The-RenderSystem-object">The RenderSystem object</a></li>
<li class="level1"><a href="#The-SceneManager-object">The SceneManager object</a></li>
<li class="level1"><a href="#The-ResourceGroupManager-Object">The ResourceGroupManager Object</a></li>
<li class="level1"><a href="#The-Mesh-Object">The Mesh Object</a></li>
<li class="level1"><a href="#Entities">Entities</a></li>
<li class="level1"><a href="#Materials">Materials</a></li>
<li class="level1"><a href="#Overlays">Overlays</a></li>
</ul>
</div>
<div class="textblock"><p>This tutorial gives you a quick summary of the core objects that you will use in OGRE and what they are used for.</p>
<p><a class="anchor" id="A-Word-About-Namespaces"></a></p>
<h1>A Word About Namespaces</h1>
<p>OGRE uses a C++ feature called namespaces. This lets you put classes, enums, structures, anything really within a ’namespace’ scope which is an easy way to prevent name clashes, i.e. situations where you have 2 things called the same thing. Since OGRE is designed to be used inside other applications, I wanted to be sure that name clashes would not be a problem. Some people prefix their classes/types with a short code because some compilers don’t support namespaces, but I chose to use them because they are the ’right’ way to do it. Sorry if you have a non-compliant compiler, but hey, the C++ standard has been defined for years, so compiler writers really have no excuse anymore. If your compiler doesn’t support namespaces then it’s probably because it’s sh*t - get a better one. ;)</p>
<p>This means every class, type etc should be prefixed with <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, e.g. <a class="el" href="class_ogre_1_1_camera.html" title="A viewpoint from which the scene will be rendered. ">Ogre::Camera</a>, <a class="el" href="class_ogre_1_1_vector3.html" title="Standard 3-dimensional vector. ">Ogre::Vector3</a> etc which means if elsewhere in your application you have used a Vector3 type you won’t get name clashes. To avoid lots of extra typing you can add a ’using namespace <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>;’ statement to your code which means you don’t have to type the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> prefix unless there is ambiguity (in the situation where you have another definition with the same name).</p>
<p><a class="anchor" id="Overview-from-10_002c000-feet"></a></p>
<h1>Overview from 10,000 feet</h1>
<p>Shown below is a diagram of some of the core objects and where they ’sit’ in the grand scheme of things. This is not all the classes by a long shot, just a few examples of the more more significant ones to give you an idea of how it slots together. </p><div class="image">
<object type="image/svg+xml" data="uml-overview.svg">uml-overview.svg</object>
</div>
<p>At the very top of the diagram is the Root object. This is your ’way in’ to the OGRE system, and it’s where you tend to create the top-level objects that you need to deal with, like scene managers, rendering systems and render windows, loading plugins, all the fundamental stuff. If you don’t know where to start, Root is it for almost everything, although often it will just give you another object which will actually do the detail work, since Root itself is more of an organiser and facilitator object.</p>
<p>The majority of rest of OGRE’s classes fall into one of 3 roles:</p>
<dl compact="compact">
<dt>Scene Management </dt>
<dd><p class="startdd"></p>
<p>This is about the contents of your scene, how it’s structured, how it’s viewed from cameras, etc. Objects in this area are responsible for giving you a natural declarative interface to the world you’re building; i.e. you don’t tell OGRE "set these render states and then render 3 polygons", you tell it "I want an object here, here and here, with these materials on them, rendered from this view", and let it get on with it.</p>
<p class="enddd"></p>
</dd>
<dt>Resource Management </dt>
<dd><p class="startdd"></p>
<p>All rendering needs resources, whether it’s geometry, textures, fonts, whatever. It’s important to manage the loading, re-use and unloading of these things carefully, so that’s what classes in this area do.</p>
<p class="enddd"></p>
</dd>
<dt>Rendering </dt>
<dd><p class="startdd"></p>
<p>Finally, there’s getting the visuals on the screen - this is about the lower-level end of the rendering pipeline, the specific rendering system API objects like buffers, render states and the like and pushing it all down the pipeline. Classes in the Scene Management subsystem use this to get their higher-level scene information onto the screen.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You’ll notice that scattered around the edge are a number of plugins. OGRE is designed to be extended, and plugins are the usual way to go about it. Many of the classes in OGRE can be subclassed and extended, whether it’s changing the scene organisation through a custom SceneManager, adding a new render system implementation (e.g. Direct3D or OpenGL), or providing a way to load resources from another source (say from a web location or a database). Again this is just a small smattering of the kinds of things plugins can do, but as you can see they can plug in to almost any aspect of the system. This way, OGRE isn’t just a solution for one narrowly defined problem, it can extend to pretty much anything you need it to do.</p>
<h1><a class="anchor" id="The-Root-Object"></a>
The Root object</h1>
<p>The <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object is the entry point to the OGRE system. This object MUST be the first one to be created, and the last one to be destroyed. In the example applications I chose to make an instance of Root a member of my application object which ensured that it was created as soon as my application object was, and deleted when the application object was deleted.</p>
<p>The root object lets you configure the system, for example through the <a class="el" href="class_ogre_1_1_root.html#aedc76ef3bdc6514bd4655be0d70d6bcd" title="Displays a dialog asking the user to choose system settings. ">Ogre::Root::showConfigDialog</a> method which is an extremely handy method which performs all render system options detection and shows a dialog for the user to customise resolution, colour depth, full screen options etc. It also sets the options the user selects so that you can initialise the system directly afterwards.</p>
<p>The root object is also your method for obtaining pointers to other objects in the system, such as the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> and various other resource managers. See below for details.</p>
<p>Finally, if you run OGRE in continuous rendering mode, i.e. you want to always refresh all the rendering targets as fast as possible (the norm for games and demos, but not for windowed utilities), the root object has a method called <a class="el" href="class_ogre_1_1_root.html#a8eda253befda1255cbfd3b298f62449e" title="Starts / restarts the automatic rendering cycle. ">Ogre::Root::startRendering</a>, which when called will enter a continuous rendering loop which will only end when all rendering windows are closed, or any <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects indicate that they want to stop the cycle (see below for details of <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects).</p>
<h1><a class="anchor" id="The-RenderSystem-object"></a>
The RenderSystem object</h1>
<p>The <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object is actually an abstract class which defines the interface to the underlying 3D API. It is responsible for sending rendering operations to the API and setting all the various rendering options. This class is abstract because all the implementation is rendering API specific - there are API-specific subclasses for each rendering API (e.g. D3DRenderSystem for Direct3D). After the system has been initialised through <a class="el" href="class_ogre_1_1_root.html#a6ab8f204bbfb150ad180eb6e78e4639c" title="Initialises the renderer. ">Ogre::Root::initialise</a>, the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object for the selected rendering API is available via the <a class="el" href="class_ogre_1_1_root.html#ab8ca3d5abebba7a14650db5f618286d0" title="Retrieve a pointer to the currently selected render system. ">Ogre::Root::getRenderSystem()</a> method.</p>
<p>However, a typical application should not normally need to manipulate the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object directly - everything you need for rendering objects and customising settings should be available on the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, Material and other scene-oriented classes. It’s only if you want to create multiple rendering windows (completely separate windows in this case, not multiple viewports like a split-screen effect which is done via the RenderWindow class) or access other advanced features that you need access to the RenderSystem object.</p>
<p>For this reason I will not discuss the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object further in these tutorials. You can assume the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> handles the calls to the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> at the appropriate times.</p>
<h1><a class="anchor" id="The-SceneManager-object"></a>
The SceneManager object</h1>
<p>Apart from the <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object, this is probably the most critical part of the system from the application’s point of view. Certainly it will be the object which is most used by the application. The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> is in charge of the contents of the scene which is to be rendered by the engine. It is responsible for organising the contents using whatever technique it deems best, for creating and managing all the cameras, movable objects (entities), lights and materials (surface properties of objects), and for managing the ’world geometry’ which is the sprawling static geometry usually used to represent the immovable parts of a scene.</p>
<p>It is to the SceneManager that you go when you want to create a camera for the scene. It’s also where you go to retrieve or to remove a light from the scene. There is no need for your application to keep lists of objects, the SceneManager keeps a named set of all of the scene objects for you to access, should you need them. Look in the main documentation under the getCamera, getLight, getEntity etc methods.</p>
<p>The SceneManager also sends the scene to the RenderSystem object when it is time to render the scene. You never have to call the <a class="el" href="class_ogre_1_1_scene_manager.html#a29ba8a2cad151d307d04781f749e7e84" title="Prompts the class to send its contents to the renderer. ">Ogre::SceneManager::_renderScene</a> method directly though - it is called automatically whenever a rendering target is asked to update.</p>
<p>So most of your interaction with the SceneManager is during scene setup. You’re likely to call a great number of methods (perhaps driven by some input file containing the scene data) in order to set up your scene. You can also modify the contents of the scene dynamically during the rendering cycle if you create your own FrameListener object (see later).</p>
<p>Because different scene types require very different algorithmic approaches to deciding which objects get sent to the RenderSystem in order to attain good rendering performance, the SceneManager class is designed to be subclassed for different scene types. The default SceneManager object will render a scene, but it does little or no scene organisation and you should not expect the results to be high performance in the case of large scenes. The intention is that specialisations will be created for each type of scene such that under the surface the subclass will optimise the scene organisation for best performance given assumptions which can be made for that scene type. An example is the BspSceneManager which optimises rendering for large indoor levels based on a Binary Space Partition (BSP) tree.</p>
<p>The application using OGRE does not have to know which subclasses are available. The application simply calls <a class="el" href="class_ogre_1_1_root.html#a8bb8655f823a98007775f69278c1b0f9" title="create a default scene manager ">Ogre::Root::createSceneManager</a>(..) passing as a parameter one of a number of scene types (e.g. <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a0af8f844019e52fe112e9a946a919fcd">Ogre::ST_GENERIC</a>, <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a93f2170ad3a8132f29264dc3642a2dcf">Ogre::ST_INTERIOR</a> etc). OGRE will automatically use the best SceneManager subclass available for that scene type, or default to the basic SceneManager if a specialist one is not available. This allows the developers of OGRE to add new scene specialisations later and thus optimise previously unoptimised scene types without the user applications having to change any code.</p>
<h1><a class="anchor" id="The-ResourceGroupManager-Object"></a>
The ResourceGroupManager Object</h1>
<p>The <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a> class is actually a ’hub’ for loading of reusable resources like textures and meshes. It is the place that you define groups for your resources, so they may be unloaded and reloaded when you want. Servicing it are a number of ResourceManagers which manage the individual types of resource, like <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> or <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a>. In this context, resources are sets of data which must be loaded from somewhere to provide OGRE with the data it needs.</p>
<p>ResourceManagers ensure that resources are only loaded once and shared throughout the OGRE engine. They also manage the memory requirements of the resources they look after. They can also search in a number of locations for the resources they need, including multiple search paths and compressed archives (ZIP files).</p>
<p>Most of the time you won’t interact with resource managers directly. Resource managers will be called by other parts of the OGRE system as required, for example when you request for a texture to be added to a Material, the <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> will be called for you. If you like, you can call the appropriate resource manager directly to preload resources (if for example you want to prevent disk access later on) but most of the time it’s ok to let OGRE decide when to do it.</p>
<p>One thing you will want to do is to tell the resource managers where to look for resources. You do this via Root::getSingleton().addResourceLocation, which actually passes the information on to <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a>.</p>
<p>Because there is only ever 1 instance of each resource manager in the engine, if you do want to get a reference to a resource manager use the following syntax:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().someMethod()</div><div class="line"><a class="code" href="class_ogre_1_1_mesh_manager.html#a2efefa307b7987c5fcff7e8c47f79758">Ogre::MeshManager::getSingleton</a>().someMethod()</div></div><!-- fragment --> <h1><a class="anchor" id="The-Mesh-Object"></a>
The Mesh Object</h1>
<p>A <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object represents a discrete model, a set of geometry which is self-contained and is typically fairly small on a world scale. <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are assumed to represent movable objects and are not used for the sprawling level geometry typically used to create backgrounds.</p>
<p><a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are a type of resource, and are managed by the MeshManager resource manager. They are typically loaded from OGRE’s custom object format, the ’.mesh’ format. Mesh files are typically created by exporting from a modelling tool See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> and can be manipulated through various <a class="el" href="_mesh-_tools.html">Mesh Tools</a></p>
<p>You can also create Mesh objects manually by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html#ad99f5f2e2d045e5a4af301974e45c918" title="Creates a new Mesh specifically for manual definition rather than loading from an object file...">Ogre::MeshManager::createManual</a> method. This way you can define the geometry yourself, but this is outside the scope of this manual.</p>
<p>Mesh objects are the basis for the individual movable objects in the world, which are called <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>.</p>
<p>Mesh objects can also be animated using See <a class="el" href="_animation.html#Skeletal-Animation">Skeletal Animation</a>.</p>
<h1><a class="anchor" id="Entities"></a>
Entities</h1>
<p>An entity is an instance of a movable object in the scene. It could be a car, a person, a dog, a shuriken, whatever. The only assumption is that it does not necessarily have a fixed position in the world.</p>
<p>Entities are based on discrete meshes, i.e. collections of geometry which are self-contained and typically fairly small on a world scale, which are represented by the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object. Multiple entities can be based on the same mesh, since often you want to create multiple copies of the same type of object in a scene.</p>
<p>You create an entity by calling the <a class="el" href="class_ogre_1_1_scene_manager.html#afb393cca49de8b928f7dd60838047185" title="Create an Entity (instance of a discrete mesh). ">Ogre::SceneManager::createEntity</a> method, giving it a name and specifying the name of the mesh object which it will be based on (e.g. ’muscleboundhero.mesh’). The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> will ensure that the mesh is loaded by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a> resource manager for you. Only one copy of the Mesh will be loaded.</p>
<p>Ogre::Entities are not deemed to be a part of the scene until you attach them to a <a class="el" href="class_ogre_1_1_scene_node.html" title="Class representing a node in the scene graph. ">Ogre::SceneNode</a> (see the section below). By attaching entities to SceneNodes, you can create complex hierarchical relationships between the positions and orientations of entities. You then modify the positions of the nodes to indirectly affect the entity positions.</p>
<p>When a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> is loaded, it automatically comes with a number of materials defined. It is possible to have more than one material attached to a mesh - different parts of the mesh may use different materials. Any entity created from the mesh will automatically use the default materials. However, you can change this on a per-entity basis if you like so you can create a number of entities based on the same mesh but with different textures etc.</p>
<p>To understand how this works, you have to know that all Mesh objects are actually composed of <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects, each of which represents a part of the mesh using one Material. If a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> uses only one <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a>, it will only have one <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a>.</p>
<p>When an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> is created based on this Mesh, it is composed of (possibly) multiple <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects, each matching 1 for 1 with the <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects from the original Mesh. You can access the <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects using the <a class="el" href="class_ogre_1_1_entity.html#a23fc34c5e2eff03aae4cbf800c142724" title="Gets a pointer to a SubEntity, ie a part of an Entity. ">Ogre::Entity::getSubEntity</a> method. Once you have a reference to a <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a>, you can change the material it uses by calling it’s setMaterialName method. In this way you can make an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> deviate from the default materials and thus create an individual looking version of it.</p>
<h1><a class="anchor" id="Materials"></a>
Materials</h1>
<p>The <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a> object controls how objects in the scene are rendered. It specifies what basic surface properties objects have such as reflectance of colours, shininess etc, how many texture layers are present, what images are on them and how they are blended together, what special effects are applied such as environment mapping, what culling mode is used, how the textures are filtered etc.</p>
<p>Materials can either be set up programmatically, by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a> and tweaking the settings, or by specifying it in a ’script’ which is loaded at runtime. See <a class="el" href="_material-_scripts.html">Material Scripts</a> for more info.</p>
<p>Basically everything about the appearance of an object apart from it’s shape is controlled by the Material class.</p>
<p>The SceneManager class manages the master list of materials available to the scene. The list can be added to by the application by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a>, or by loading a Mesh (which will in turn load material properties). Whenever materials are added to the SceneManager, they start off with a default set of properties; these are defined by OGRE as the following:</p>
<ul>
<li>ambient reflectance = ColourValue::White (full)</li>
<li>diffuse reflectance = ColourValue::White (full)</li>
<li>specular reflectance = ColourValue::Black (none)</li>
<li>emissive = ColourValue::Black (none)</li>
<li>shininess = 0 (not shiny)</li>
<li>No texture layers (&amp; hence no textures)</li>
<li>SourceBlendFactor = SBF_ONE, DestBlendFactor = SBF_ZERO (opaque)</li>
<li>Depth buffer checking on</li>
<li>Depth buffer writing on</li>
<li>Depth buffer comparison function = CMPF_LESS_EQUAL</li>
<li>Culling mode = CULL_CLOCKWISE</li>
<li>Ambient lighting in scene = ColourValue(0.5, 0.5, 0.5) (mid-grey)</li>
<li>Dynamic lighting enabled</li>
<li>Gourad shading mode</li>
<li>Solid polygon mode</li>
<li>Bilinear texture filtering</li>
</ul>
<p>You can alter these settings by calling <a class="el" href="class_ogre_1_1_material_manager.html#ac70230fa9f123976d7f2b1257bd94bd4" title="Returns a pointer to the default Material settings. ">Ogre::MaterialManager::getDefaultSettings()</a> and making the required changes to the Material which is returned.</p>
<p>Entities automatically have Material’s associated with them if they use a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object, since the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object typically sets up it’s required materials on loading. You can also customise the material used by an entity as described in <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>. Just create a new Material, set it up how you like (you can copy an existing material into it if you like using a standard assignment statement) and point the SubEntity entries at it using <a class="el" href="class_ogre_1_1_sub_entity.html#ab8af6685128c68a5d7863bb5238ebc0f" title="Sets the name of the Material to be used. ">Ogre::SubEntity::setMaterialName()</a>.</p>
<h1><a class="anchor" id="Overlays"></a>
Overlays</h1>
<p>Overlays allow you to render 2D and 3D elements on top of the normal scene contents to create effects like heads-up displays (HUDs), menu systems, status panels etc. The frame rate statistics panel which comes as standard with OGRE is an example of an overlay. Overlays can contain 2D or 3D elements. 2D elements are used for HUDs, and 3D elements can be used to create cockpits or any other 3D object which you wish to be rendered on top of the rest of the scene.</p>
<p>You can create overlays either through the <a class="el" href="class_ogre_1_1_overlay_manager.html#ab6eb3dcc484ba5b06904ab2fb6b1aa23" title="Create a new Overlay. ">Ogre::OverlayManager::create</a> method, or you can define them in an .overlay script. See <a class="el" href="_overlay-_scripts.html">Overlay Scripts</a> for more info. In reality the latter is likely to be the most practical because it is easier to tweak (without the need to recompile the code). Note that you can define as many overlays as you like: they all start off life hidden, and you display them by calling <a class="el" href="class_ogre_1_1_overlay.html#a3c565f3e5cf61f43fa33542d713e8ccf" title="Shows the overlay if it was hidden. ">Ogre::Overlay::show</a>. You can also show multiple overlays at once, and their Z order is determined by the <a class="el" href="class_ogre_1_1_overlay.html#a5d8481d5fa0990846e53345f1a1cc705" title="Alters the Z-order of this overlay. ">Ogre::Overlay::setZOrder()</a> method.</p>
<p><a class="anchor" id="Notes-on-Integration"></a></p>
<h2>Notes on Integration</h2>
<p>The OverlaySystem is its own component, you need to manually initialize it, with the following two lines of code (mSceneMgr is a pointer to your current <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>):</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>* pOverlaySystem = <span class="keyword">new</span> <a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>();</div><div class="line">mSceneMgr-&gt;addRenderQueueListener(pOverlaySystem);</div></div><!-- fragment --><p>One <a class="el" href="class_ogre_1_1_overlay_system.html" title="This class simplify initialization / finalization of the overlay system. ">Ogre::OverlaySystem</a> per application is enough but you need to call addRenderQueueListener once per SceneManager.</p>
<p><a class="anchor" id="Creating-2D-Elements"></a></p>
<h2>Creating 2D Elements</h2>
<p>The <a class="el" href="class_ogre_1_1_overlay_element.html" title="Abstract definition of a 2D element to be displayed in an Overlay. ">Ogre::OverlayElement</a> class abstracts the details of 2D elements which are added to overlays. All items which can be added to overlays are derived from this class. It is possible (and encouraged) for users of OGRE to define their own custom subclasses of OverlayElement in order to provide their own user controls. The key common features of all OverlayElements are things like size, position, basic material name etc. Subclasses extend this behaviour to include more complex properties and behaviour.</p>
<p>An important built-in subclass of OverlayElement is <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a>. OverlayContainer is the same as a OverlayElement, except that it can contain other OverlayElements, grouping them together (allowing them to be moved together for example) and providing them with a local coordinate origin for easier lineup.</p>
<p>The third important class is <a class="el" href="class_ogre_1_1_overlay_manager.html" title="Manages Overlay objects, parsing them from .overlay files and storing a lookup library of them...">Ogre::OverlayManager</a>. Whenever an application wishes to create a 2D element to add to an overlay (or a container), it should call <a class="el" href="class_ogre_1_1_overlay_manager.html#a58166a773d27c22437323e8ef710406b" title="Creates a new OverlayElement of the type requested. ">Ogre::OverlayManager::createOverlayElement</a>. The type of element you wish to create is identified by a string, the reason being that it allows plugins to register new types of OverlayElement for you to create without you having to link specifically to those libraries. For example, to create a panel (a plain rectangular area which can contain other OverlayElements) you would call <code>OverlayManager::getSingleton().createOverlayElement("Panel", "myNewPanel");</code>.</p>
<p><a class="anchor" id="Adding-2D-Elements-to-the-Overlay"></a></p>
<h2>Adding 2D Elements to the Overlay</h2>
<p>Only OverlayContainers can be added direct to an overlay. The reason is that each level of container establishes the Zorder of the elements contained within it, so if you nest several containers, inner containers have a higher Z-order than outer ones to ensure they are displayed correctly. To add a container (such as a Panel) to the overlay, simply call <a class="el" href="class_ogre_1_1_overlay.html#a702b15ebaa6d3f8dbb2918500b76c9f0" title="Adds a 2D &#39;container&#39; to the overlay. ">Ogre::Overlay::add2D</a>.</p>
<p>If you wish to add child elements to that container, call <a class="el" href="class_ogre_1_1_overlay_container.html#aa9eec8f6dca98ac61766fc4785b84661" title="Adds another OverlayElement to this container. ">Ogre::OverlayContainer::addChild</a>. Child elements can be Ogre::OverlayElements or <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a> instances themselves. Remember that the position of a child element is relative to the top-left corner of it’s parent.</p>
<p><a class="anchor" id="A-word-about-2D-coordinates"></a></p>
<h2>A word about 2D coordinates</h2>
<p>OGRE allows you to place and size elements based on 2 coordinate systems: <b>relative</b> and <b>pixel</b> based.</p>
<dl compact="compact">
<dt>Pixel Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want to specify an exact size for your overlay items, and you don’t mind if those items get smaller on the screen if you increase the screen resolution (in fact you might want this). In this mode the only way to put something in the middle or at the right or bottom of the screen reliably in any resolution is to use the aligning options, whilst in relative mode you can do it just by using the right relative coordinates. This mode is very simple, the top-left of the screen is (0,0) and the bottom-right of the screen depends on the resolution. As mentioned above, you can use the aligning options to make the horizontal and vertical coordinate origins the right, bottom or center of the screen if you want to place pixel items in these locations without knowing the resolution.</p>
<p class="enddd"></p>
</dd>
<dt>Relative Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want items in the overlay to be the same size on the screen no matter what the resolution. In relative mode, the top-left of the screen is (0,0) and the bottom-right is (1,1). So if you place an element at (0.5, 0.5), it’s top-left corner is placed exactly in the center of the screen, no matter what resolution the application is running in. The same principle applies to sizes; if you set the width of an element to 0.5, it covers half the width of the screen. Note that because the aspect ratio of the screen is typically 1.3333 : 1 (width : height), an element with dimensions (0.25, 0.25) will not be square, but it will take up exactly 1/16th of the screen in area terms. If you want square-looking areas you will have to compensate using the typical aspect ratio e.g. use (0.1875, 0.25) instead.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Transforming-Overlays"></a> </p><h2>Transforming Overlays</h2>
<p>Another nice feature of overlays is being able to rotate, scroll and scale them as a whole. You can use this for zooming in / out menu systems, dropping them in from off screen and other nice effects. See the <a class="el" href="class_ogre_1_1_overlay.html#ae07b1a57498aad0299f0781b09ca9b39" title="Scrolls the overlay by the offsets provided. ">Ogre::Overlay::scroll</a>, <a class="el" href="class_ogre_1_1_overlay.html#a8282553a59253170ddd0488ab7179792" title="Adds the passed in angle to the rotation applied to this overlay. ">Ogre::Overlay::rotate</a> and <a class="el" href="class_ogre_1_1_overlay.html#adea3b88baa3697a657e7b5785f0ffedb" title="Sets the scaling factor of this overlay. ">Ogre::Overlay::setScale</a> methods for more information.</p>
<p><a class="anchor" id="GUI-systems"></a></p>
<h2>GUI systems</h2>
<p>Overlays are only really designed for non-interactive screen elements, although you can create a simple GUI using the <a class="el" href="trays.html">Trays System</a>. For a far more complete GUI solution, we recommend or <a href="https://github.com/OGRECave/ogre-imgui">Dear ImGui</a>, <a href="http://www.cegui.org.uk">CEGui</a> or <a href="http://mygui.info/">MyGUI</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: The Core Objects</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_the-_core-_objects.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">The Core Objects </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#The-Root-Object">The Root object</a></li>
<li class="level1"><a href="#The-RenderSystem-object">The RenderSystem object</a></li>
<li class="level1"><a href="#The-SceneManager-object">The SceneManager object</a></li>
<li class="level1"><a href="#The-ResourceGroupManager-Object">The ResourceGroupManager Object</a></li>
<li class="level1"><a href="#The-Mesh-Object">The Mesh Object</a></li>
<li class="level1"><a href="#Entities">Entities</a></li>
<li class="level1"><a href="#Materials">Materials</a></li>
<li class="level1"><a href="#Overlays">Overlays</a></li>
</ul>
</div>
<div class="textblock"><p>This tutorial gives you a quick summary of the core objects that you will use in OGRE and what they are used for.</p>
<p><a class="anchor" id="A-Word-About-Namespaces"></a></p>
<h1>A Word About Namespaces</h1>
<p>OGRE uses a C++ feature called namespaces. This lets you put classes, enums, structures, anything really within a ’namespace’ scope which is an easy way to prevent name clashes, i.e. situations where you have 2 things called the same thing. Since OGRE is designed to be used inside other applications, I wanted to be sure that name clashes would not be a problem. Some people prefix their classes/types with a short code because some compilers don’t support namespaces, but I chose to use them because they are the ’right’ way to do it. Sorry if you have a non-compliant compiler, but hey, the C++ standard has been defined for years, so compiler writers really have no excuse anymore. If your compiler doesn’t support namespaces then it’s probably because it’s sh*t - get a better one. ;)</p>
<p>This means every class, type etc should be prefixed with <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, e.g. <a class="el" href="class_ogre_1_1_camera.html" title="A viewpoint from which the scene will be rendered. ">Ogre::Camera</a>, <a class="el" href="class_ogre_1_1_vector3.html" title="Standard 3-dimensional vector. ">Ogre::Vector3</a> etc which means if elsewhere in your application you have used a Vector3 type you won’t get name clashes. To avoid lots of extra typing you can add a ’using namespace <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>;’ statement to your code which means you don’t have to type the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> prefix unless there is ambiguity (in the situation where you have another definition with the same name).</p>
<p><a class="anchor" id="Overview-from-10_002c000-feet"></a></p>
<h1>Overview from 10,000 feet</h1>
<p>Shown below is a diagram of some of the core objects and where they ’sit’ in the grand scheme of things. This is not all the classes by a long shot, just a few examples of the more more significant ones to give you an idea of how it slots together. </p><div class="image">
<object type="image/svg+xml" data="uml-overview.svg">uml-overview.svg</object>
</div>
<p>At the very top of the diagram is the Root object. This is your ’way in’ to the OGRE system, and it’s where you tend to create the top-level objects that you need to deal with, like scene managers, rendering systems and render windows, loading plugins, all the fundamental stuff. If you don’t know where to start, Root is it for almost everything, although often it will just give you another object which will actually do the detail work, since Root itself is more of an organiser and facilitator object.</p>
<p>The majority of rest of OGRE’s classes fall into one of 3 roles:</p>
<dl compact="compact">
<dt>Scene Management </dt>
<dd><p class="startdd"></p>
<p>This is about the contents of your scene, how it’s structured, how it’s viewed from cameras, etc. Objects in this area are responsible for giving you a natural declarative interface to the world you’re building; i.e. you don’t tell OGRE "set these render states and then render 3 polygons", you tell it "I want an object here, here and here, with these materials on them, rendered from this view", and let it get on with it.</p>
<p class="enddd"></p>
</dd>
<dt>Resource Management </dt>
<dd><p class="startdd"></p>
<p>All rendering needs resources, whether it’s geometry, textures, fonts, whatever. It’s important to manage the loading, re-use and unloading of these things carefully, so that’s what classes in this area do.</p>
<p class="enddd"></p>
</dd>
<dt>Rendering </dt>
<dd><p class="startdd"></p>
<p>Finally, there’s getting the visuals on the screen - this is about the lower-level end of the rendering pipeline, the specific rendering system API objects like buffers, render states and the like and pushing it all down the pipeline. Classes in the Scene Management subsystem use this to get their higher-level scene information onto the screen.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You’ll notice that scattered around the edge are a number of plugins. OGRE is designed to be extended, and plugins are the usual way to go about it. Many of the classes in OGRE can be subclassed and extended, whether it’s changing the scene organisation through a custom SceneManager, adding a new render system implementation (e.g. Direct3D or OpenGL), or providing a way to load resources from another source (say from a web location or a database). Again this is just a small smattering of the kinds of things plugins can do, but as you can see they can plug in to almost any aspect of the system. This way, OGRE isn’t just a solution for one narrowly defined problem, it can extend to pretty much anything you need it to do.</p>
<h1><a class="anchor" id="The-Root-Object"></a>
The Root object</h1>
<p>The <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object is the entry point to the OGRE system. This object MUST be the first one to be created, and the last one to be destroyed. In the example applications I chose to make an instance of Root a member of my application object which ensured that it was created as soon as my application object was, and deleted when the application object was deleted.</p>
<p>The root object lets you configure the system, for example through the <a class="el" href="class_ogre_1_1_root.html#aedc76ef3bdc6514bd4655be0d70d6bcd" title="Displays a dialog asking the user to choose system settings. ">Ogre::Root::showConfigDialog</a> method which is an extremely handy method which performs all render system options detection and shows a dialog for the user to customise resolution, colour depth, full screen options etc. It also sets the options the user selects so that you can initialise the system directly afterwards.</p>
<p>The root object is also your method for obtaining pointers to other objects in the system, such as the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> and various other resource managers. See below for details.</p>
<p>Finally, if you run OGRE in continuous rendering mode, i.e. you want to always refresh all the rendering targets as fast as possible (the norm for games and demos, but not for windowed utilities), the root object has a method called <a class="el" href="class_ogre_1_1_root.html#a8eda253befda1255cbfd3b298f62449e" title="Starts / restarts the automatic rendering cycle. ">Ogre::Root::startRendering</a>, which when called will enter a continuous rendering loop which will only end when all rendering windows are closed, or any <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects indicate that they want to stop the cycle (see below for details of <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects).</p>
<h1><a class="anchor" id="The-RenderSystem-object"></a>
The RenderSystem object</h1>
<p>The <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object is actually an abstract class which defines the interface to the underlying 3D API. It is responsible for sending rendering operations to the API and setting all the various rendering options. This class is abstract because all the implementation is rendering API specific - there are API-specific subclasses for each rendering API (e.g. D3DRenderSystem for Direct3D). After the system has been initialised through <a class="el" href="class_ogre_1_1_root.html#a6ab8f204bbfb150ad180eb6e78e4639c" title="Initialises the renderer. ">Ogre::Root::initialise</a>, the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object for the selected rendering API is available via the <a class="el" href="class_ogre_1_1_root.html#ab8ca3d5abebba7a14650db5f618286d0" title="Retrieve a pointer to the currently selected render system. ">Ogre::Root::getRenderSystem()</a> method.</p>
<p>However, a typical application should not normally need to manipulate the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object directly - everything you need for rendering objects and customising settings should be available on the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, Material and other scene-oriented classes. It’s only if you want to create multiple rendering windows (completely separate windows in this case, not multiple viewports like a split-screen effect which is done via the RenderWindow class) or access other advanced features that you need access to the RenderSystem object.</p>
<p>For this reason I will not discuss the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object further in these tutorials. You can assume the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> handles the calls to the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> at the appropriate times.</p>
<h1><a class="anchor" id="The-SceneManager-object"></a>
The SceneManager object</h1>
<p>Apart from the <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object, this is probably the most critical part of the system from the application’s point of view. Certainly it will be the object which is most used by the application. The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> is in charge of the contents of the scene which is to be rendered by the engine. It is responsible for organising the contents using whatever technique it deems best, for creating and managing all the cameras, movable objects (entities), lights and materials (surface properties of objects), and for managing the ’world geometry’ which is the sprawling static geometry usually used to represent the immovable parts of a scene.</p>
<p>It is to the SceneManager that you go when you want to create a camera for the scene. It’s also where you go to retrieve or to remove a light from the scene. There is no need for your application to keep lists of objects, the SceneManager keeps a named set of all of the scene objects for you to access, should you need them. Look in the main documentation under the getCamera, getLight, getEntity etc methods.</p>
<p>The SceneManager also sends the scene to the RenderSystem object when it is time to render the scene. You never have to call the <a class="el" href="class_ogre_1_1_scene_manager.html#a29ba8a2cad151d307d04781f749e7e84" title="Prompts the class to send its contents to the renderer. ">Ogre::SceneManager::_renderScene</a> method directly though - it is called automatically whenever a rendering target is asked to update.</p>
<p>So most of your interaction with the SceneManager is during scene setup. You’re likely to call a great number of methods (perhaps driven by some input file containing the scene data) in order to set up your scene. You can also modify the contents of the scene dynamically during the rendering cycle if you create your own FrameListener object (see later).</p>
<p>Because different scene types require very different algorithmic approaches to deciding which objects get sent to the RenderSystem in order to attain good rendering performance, the SceneManager class is designed to be subclassed for different scene types. The default SceneManager object will render a scene, but it does little or no scene organisation and you should not expect the results to be high performance in the case of large scenes. The intention is that specialisations will be created for each type of scene such that under the surface the subclass will optimise the scene organisation for best performance given assumptions which can be made for that scene type. An example is the BspSceneManager which optimises rendering for large indoor levels based on a Binary Space Partition (BSP) tree.</p>
<p>The application using OGRE does not have to know which subclasses are available. The application simply calls <a class="el" href="class_ogre_1_1_root.html#a8bb8655f823a98007775f69278c1b0f9" title="create a default scene manager ">Ogre::Root::createSceneManager</a>(..) passing as a parameter one of a number of scene types (e.g. <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a0af8f844019e52fe112e9a946a919fcd">Ogre::ST_GENERIC</a>, <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a93f2170ad3a8132f29264dc3642a2dcf">Ogre::ST_INTERIOR</a> etc). OGRE will automatically use the best SceneManager subclass available for that scene type, or default to the basic SceneManager if a specialist one is not available. This allows the developers of OGRE to add new scene specialisations later and thus optimise previously unoptimised scene types without the user applications having to change any code.</p>
<h1><a class="anchor" id="The-ResourceGroupManager-Object"></a>
The ResourceGroupManager Object</h1>
<p>The <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a> class is actually a ’hub’ for loading of reusable resources like textures and meshes. It is the place that you define groups for your resources, so they may be unloaded and reloaded when you want. Servicing it are a number of ResourceManagers which manage the individual types of resource, like <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> or <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a>. In this context, resources are sets of data which must be loaded from somewhere to provide OGRE with the data it needs.</p>
<p>ResourceManagers ensure that resources are only loaded once and shared throughout the OGRE engine. They also manage the memory requirements of the resources they look after. They can also search in a number of locations for the resources they need, including multiple search paths and compressed archives (ZIP files).</p>
<p>Most of the time you won’t interact with resource managers directly. Resource managers will be called by other parts of the OGRE system as required, for example when you request for a texture to be added to a Material, the <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> will be called for you. If you like, you can call the appropriate resource manager directly to preload resources (if for example you want to prevent disk access later on) but most of the time it’s ok to let OGRE decide when to do it.</p>
<p>One thing you will want to do is to tell the resource managers where to look for resources. You do this via Root::getSingleton().addResourceLocation, which actually passes the information on to <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a>.</p>
<p>Because there is only ever 1 instance of each resource manager in the engine, if you do want to get a reference to a resource manager use the following syntax:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().someMethod()</div><div class="line"><a class="code" href="class_ogre_1_1_mesh_manager.html#a2efefa307b7987c5fcff7e8c47f79758">Ogre::MeshManager::getSingleton</a>().someMethod()</div></div><!-- fragment --> <h1><a class="anchor" id="The-Mesh-Object"></a>
The Mesh Object</h1>
<p>A <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object represents a discrete model, a set of geometry which is self-contained and is typically fairly small on a world scale. <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are assumed to represent movable objects and are not used for the sprawling level geometry typically used to create backgrounds.</p>
<p><a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are a type of resource, and are managed by the MeshManager resource manager. They are typically loaded from OGRE’s custom object format, the ’.mesh’ format. Mesh files are typically created by exporting from a modelling tool See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> and can be manipulated through various <a class="el" href="_mesh-_tools.html">Mesh Tools</a></p>
<p>You can also create Mesh objects manually by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html#ad99f5f2e2d045e5a4af301974e45c918" title="Creates a new Mesh specifically for manual definition rather than loading from an object file...">Ogre::MeshManager::createManual</a> method. This way you can define the geometry yourself, but this is outside the scope of this manual.</p>
<p>Mesh objects are the basis for the individual movable objects in the world, which are called <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>.</p>
<p>Mesh objects can also be animated using See <a class="el" href="_animation.html#Skeletal-Animation">Skeletal Animation</a>.</p>
<h1><a class="anchor" id="Entities"></a>
Entities</h1>
<p>An entity is an instance of a movable object in the scene. It could be a car, a person, a dog, a shuriken, whatever. The only assumption is that it does not necessarily have a fixed position in the world.</p>
<p>Entities are based on discrete meshes, i.e. collections of geometry which are self-contained and typically fairly small on a world scale, which are represented by the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object. Multiple entities can be based on the same mesh, since often you want to create multiple copies of the same type of object in a scene.</p>
<p>You create an entity by calling the <a class="el" href="class_ogre_1_1_scene_manager.html#afb393cca49de8b928f7dd60838047185" title="Create an Entity (instance of a discrete mesh). ">Ogre::SceneManager::createEntity</a> method, giving it a name and specifying the name of the mesh object which it will be based on (e.g. ’muscleboundhero.mesh’). The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> will ensure that the mesh is loaded by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a> resource manager for you. Only one copy of the Mesh will be loaded.</p>
<p>Ogre::Entities are not deemed to be a part of the scene until you attach them to a <a class="el" href="class_ogre_1_1_scene_node.html" title="Class representing a node in the scene graph. ">Ogre::SceneNode</a> (see the section below). By attaching entities to SceneNodes, you can create complex hierarchical relationships between the positions and orientations of entities. You then modify the positions of the nodes to indirectly affect the entity positions.</p>
<p>When a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> is loaded, it automatically comes with a number of materials defined. It is possible to have more than one material attached to a mesh - different parts of the mesh may use different materials. Any entity created from the mesh will automatically use the default materials. However, you can change this on a per-entity basis if you like so you can create a number of entities based on the same mesh but with different textures etc.</p>
<p>To understand how this works, you have to know that all Mesh objects are actually composed of <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects, each of which represents a part of the mesh using one Material. If a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> uses only one <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a>, it will only have one <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a>.</p>
<p>When an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> is created based on this Mesh, it is composed of (possibly) multiple <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects, each matching 1 for 1 with the <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects from the original Mesh. You can access the <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects using the <a class="el" href="class_ogre_1_1_entity.html#a23fc34c5e2eff03aae4cbf800c142724" title="Gets a pointer to a SubEntity, ie a part of an Entity. ">Ogre::Entity::getSubEntity</a> method. Once you have a reference to a <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a>, you can change the material it uses by calling it’s setMaterialName method. In this way you can make an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> deviate from the default materials and thus create an individual looking version of it.</p>
<h1><a class="anchor" id="Materials"></a>
Materials</h1>
<p>The <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a> object controls how objects in the scene are rendered. It specifies what basic surface properties objects have such as reflectance of colours, shininess etc, how many texture layers are present, what images are on them and how they are blended together, what special effects are applied such as environment mapping, what culling mode is used, how the textures are filtered etc.</p>
<p>Materials can either be set up programmatically, by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a> and tweaking the settings, or by specifying it in a ’script’ which is loaded at runtime. See <a class="el" href="_material-_scripts.html">Material Scripts</a> for more info.</p>
<p>Basically everything about the appearance of an object apart from it’s shape is controlled by the Material class.</p>
<p>The SceneManager class manages the master list of materials available to the scene. The list can be added to by the application by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a>, or by loading a Mesh (which will in turn load material properties). Whenever materials are added to the SceneManager, they start off with a default set of properties; these are defined by OGRE as the following:</p>
<ul>
<li>ambient reflectance = ColourValue::White (full)</li>
<li>diffuse reflectance = ColourValue::White (full)</li>
<li>specular reflectance = ColourValue::Black (none)</li>
<li>emissive = ColourValue::Black (none)</li>
<li>shininess = 0 (not shiny)</li>
<li>No texture layers (&amp; hence no textures)</li>
<li>SourceBlendFactor = SBF_ONE, DestBlendFactor = SBF_ZERO (opaque)</li>
<li>Depth buffer checking on</li>
<li>Depth buffer writing on</li>
<li>Depth buffer comparison function = CMPF_LESS_EQUAL</li>
<li>Culling mode = CULL_CLOCKWISE</li>
<li>Ambient lighting in scene = ColourValue(0.5, 0.5, 0.5) (mid-grey)</li>
<li>Dynamic lighting enabled</li>
<li>Gourad shading mode</li>
<li>Solid polygon mode</li>
<li>Bilinear texture filtering</li>
</ul>
<p>You can alter these settings by calling <a class="el" href="class_ogre_1_1_material_manager.html#ac70230fa9f123976d7f2b1257bd94bd4" title="Returns a pointer to the default Material settings. ">Ogre::MaterialManager::getDefaultSettings()</a> and making the required changes to the Material which is returned.</p>
<p>Entities automatically have Material’s associated with them if they use a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object, since the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object typically sets up it’s required materials on loading. You can also customise the material used by an entity as described in <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>. Just create a new Material, set it up how you like (you can copy an existing material into it if you like using a standard assignment statement) and point the SubEntity entries at it using <a class="el" href="class_ogre_1_1_sub_entity.html#ab8af6685128c68a5d7863bb5238ebc0f" title="Sets the name of the Material to be used. ">Ogre::SubEntity::setMaterialName()</a>.</p>
<h1><a class="anchor" id="Overlays"></a>
Overlays</h1>
<p>Overlays allow you to render 2D and 3D elements on top of the normal scene contents to create effects like heads-up displays (HUDs), menu systems, status panels etc. The frame rate statistics panel which comes as standard with OGRE is an example of an overlay. Overlays can contain 2D or 3D elements. 2D elements are used for HUDs, and 3D elements can be used to create cockpits or any other 3D object which you wish to be rendered on top of the rest of the scene.</p>
<p>You can create overlays either through the <a class="el" href="class_ogre_1_1_overlay_manager.html#ab6eb3dcc484ba5b06904ab2fb6b1aa23" title="Create a new Overlay. ">Ogre::OverlayManager::create</a> method, or you can define them in an .overlay script. See <a class="el" href="_overlay-_scripts.html">Overlay Scripts</a> for more info. In reality the latter is likely to be the most practical because it is easier to tweak (without the need to recompile the code). Note that you can define as many overlays as you like: they all start off life hidden, and you display them by calling <a class="el" href="class_ogre_1_1_overlay.html#a3c565f3e5cf61f43fa33542d713e8ccf" title="Shows the overlay if it was hidden. ">Ogre::Overlay::show</a>. You can also show multiple overlays at once, and their Z order is determined by the <a class="el" href="class_ogre_1_1_overlay.html#a5d8481d5fa0990846e53345f1a1cc705" title="Alters the Z-order of this overlay. ">Ogre::Overlay::setZOrder()</a> method.</p>
<p><a class="anchor" id="Notes-on-Integration"></a></p>
<h2>Notes on Integration</h2>
<p>The OverlaySystem is its own component, you need to manually initialize it, with the following two lines of code (mSceneMgr is a pointer to your current <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>):</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>* pOverlaySystem = <span class="keyword">new</span> <a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>();</div><div class="line">mSceneMgr-&gt;addRenderQueueListener(pOverlaySystem);</div></div><!-- fragment --><p>One <a class="el" href="class_ogre_1_1_overlay_system.html" title="This class simplify initialization / finalization of the overlay system. ">Ogre::OverlaySystem</a> per application is enough but you need to call addRenderQueueListener once per SceneManager.</p>
<p><a class="anchor" id="Creating-2D-Elements"></a></p>
<h2>Creating 2D Elements</h2>
<p>The <a class="el" href="class_ogre_1_1_overlay_element.html" title="Abstract definition of a 2D element to be displayed in an Overlay. ">Ogre::OverlayElement</a> class abstracts the details of 2D elements which are added to overlays. All items which can be added to overlays are derived from this class. It is possible (and encouraged) for users of OGRE to define their own custom subclasses of OverlayElement in order to provide their own user controls. The key common features of all OverlayElements are things like size, position, basic material name etc. Subclasses extend this behaviour to include more complex properties and behaviour.</p>
<p>An important built-in subclass of OverlayElement is <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a>. OverlayContainer is the same as a OverlayElement, except that it can contain other OverlayElements, grouping them together (allowing them to be moved together for example) and providing them with a local coordinate origin for easier lineup.</p>
<p>The third important class is <a class="el" href="class_ogre_1_1_overlay_manager.html" title="Manages Overlay objects, parsing them from .overlay files and storing a lookup library of them...">Ogre::OverlayManager</a>. Whenever an application wishes to create a 2D element to add to an overlay (or a container), it should call <a class="el" href="class_ogre_1_1_overlay_manager.html#a58166a773d27c22437323e8ef710406b" title="Creates a new OverlayElement of the type requested. ">Ogre::OverlayManager::createOverlayElement</a>. The type of element you wish to create is identified by a string, the reason being that it allows plugins to register new types of OverlayElement for you to create without you having to link specifically to those libraries. For example, to create a panel (a plain rectangular area which can contain other OverlayElements) you would call <code>OverlayManager::getSingleton().createOverlayElement("Panel", "myNewPanel");</code>.</p>
<p><a class="anchor" id="Adding-2D-Elements-to-the-Overlay"></a></p>
<h2>Adding 2D Elements to the Overlay</h2>
<p>Only OverlayContainers can be added direct to an overlay. The reason is that each level of container establishes the Zorder of the elements contained within it, so if you nest several containers, inner containers have a higher Z-order than outer ones to ensure they are displayed correctly. To add a container (such as a Panel) to the overlay, simply call <a class="el" href="class_ogre_1_1_overlay.html#a702b15ebaa6d3f8dbb2918500b76c9f0" title="Adds a 2D &#39;container&#39; to the overlay. ">Ogre::Overlay::add2D</a>.</p>
<p>If you wish to add child elements to that container, call <a class="el" href="class_ogre_1_1_overlay_container.html#aa9eec8f6dca98ac61766fc4785b84661" title="Adds another OverlayElement to this container. ">Ogre::OverlayContainer::addChild</a>. Child elements can be Ogre::OverlayElements or <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a> instances themselves. Remember that the position of a child element is relative to the top-left corner of it’s parent.</p>
<p><a class="anchor" id="A-word-about-2D-coordinates"></a></p>
<h2>A word about 2D coordinates</h2>
<p>OGRE allows you to place and size elements based on 2 coordinate systems: <b>relative</b> and <b>pixel</b> based.</p>
<dl compact="compact">
<dt>Pixel Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want to specify an exact size for your overlay items, and you don’t mind if those items get smaller on the screen if you increase the screen resolution (in fact you might want this). In this mode the only way to put something in the middle or at the right or bottom of the screen reliably in any resolution is to use the aligning options, whilst in relative mode you can do it just by using the right relative coordinates. This mode is very simple, the top-left of the screen is (0,0) and the bottom-right of the screen depends on the resolution. As mentioned above, you can use the aligning options to make the horizontal and vertical coordinate origins the right, bottom or center of the screen if you want to place pixel items in these locations without knowing the resolution.</p>
<p class="enddd"></p>
</dd>
<dt>Relative Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want items in the overlay to be the same size on the screen no matter what the resolution. In relative mode, the top-left of the screen is (0,0) and the bottom-right is (1,1). So if you place an element at (0.5, 0.5), it’s top-left corner is placed exactly in the center of the screen, no matter what resolution the application is running in. The same principle applies to sizes; if you set the width of an element to 0.5, it covers half the width of the screen. Note that because the aspect ratio of the screen is typically 1.3333 : 1 (width : height), an element with dimensions (0.25, 0.25) will not be square, but it will take up exactly 1/16th of the screen in area terms. If you want square-looking areas you will have to compensate using the typical aspect ratio e.g. use (0.1875, 0.25) instead.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Transforming-Overlays"></a> </p><h2>Transforming Overlays</h2>
<p>Another nice feature of overlays is being able to rotate, scroll and scale them as a whole. You can use this for zooming in / out menu systems, dropping them in from off screen and other nice effects. See the <a class="el" href="class_ogre_1_1_overlay.html#ae07b1a57498aad0299f0781b09ca9b39" title="Scrolls the overlay by the offsets provided. ">Ogre::Overlay::scroll</a>, <a class="el" href="class_ogre_1_1_overlay.html#a8282553a59253170ddd0488ab7179792" title="Adds the passed in angle to the rotation applied to this overlay. ">Ogre::Overlay::rotate</a> and <a class="el" href="class_ogre_1_1_overlay.html#adea3b88baa3697a657e7b5785f0ffedb" title="Sets the scaling factor of this overlay. ">Ogre::Overlay::setScale</a> methods for more information.</p>
<p><a class="anchor" id="GUI-systems"></a></p>
<h2>GUI systems</h2>
<p>Overlays are only really designed for non-interactive screen elements, although you can create a simple GUI using the <a class="el" href="trays.html">Trays System</a>. For a far more complete GUI solution, we recommend or <a href="https://github.com/OGRECave/ogre-imgui">Dear ImGui</a>, <a href="http://www.cegui.org.uk">CEGui</a> or <a href="http://mygui.info/">MyGUI</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: The Core Objects</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_the-_core-_objects.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">The Core Objects </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#The-Root-Object">The Root object</a></li>
<li class="level1"><a href="#The-RenderSystem-object">The RenderSystem object</a></li>
<li class="level1"><a href="#The-SceneManager-object">The SceneManager object</a></li>
<li class="level1"><a href="#The-ResourceGroupManager-Object">The ResourceGroupManager Object</a></li>
<li class="level1"><a href="#The-Mesh-Object">The Mesh Object</a></li>
<li class="level1"><a href="#Entities">Entities</a></li>
<li class="level1"><a href="#Materials">Materials</a></li>
<li class="level1"><a href="#Overlays">Overlays</a></li>
</ul>
</div>
<div class="textblock"><p>This tutorial gives you a quick summary of the core objects that you will use in OGRE and what they are used for.</p>
<p><a class="anchor" id="A-Word-About-Namespaces"></a></p>
<h1>A Word About Namespaces</h1>
<p>OGRE uses a C++ feature called namespaces. This lets you put classes, enums, structures, anything really within a ’namespace’ scope which is an easy way to prevent name clashes, i.e. situations where you have 2 things called the same thing. Since OGRE is designed to be used inside other applications, I wanted to be sure that name clashes would not be a problem. Some people prefix their classes/types with a short code because some compilers don’t support namespaces, but I chose to use them because they are the ’right’ way to do it. Sorry if you have a non-compliant compiler, but hey, the C++ standard has been defined for years, so compiler writers really have no excuse anymore. If your compiler doesn’t support namespaces then it’s probably because it’s sh*t - get a better one. ;)</p>
<p>This means every class, type etc should be prefixed with <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, e.g. <a class="el" href="class_ogre_1_1_camera.html" title="A viewpoint from which the scene will be rendered. ">Ogre::Camera</a>, <a class="el" href="class_ogre_1_1_vector3.html" title="Standard 3-dimensional vector. ">Ogre::Vector3</a> etc which means if elsewhere in your application you have used a Vector3 type you won’t get name clashes. To avoid lots of extra typing you can add a ’using namespace <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>;’ statement to your code which means you don’t have to type the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> prefix unless there is ambiguity (in the situation where you have another definition with the same name).</p>
<p><a class="anchor" id="Overview-from-10_002c000-feet"></a></p>
<h1>Overview from 10,000 feet</h1>
<p>Shown below is a diagram of some of the core objects and where they ’sit’ in the grand scheme of things. This is not all the classes by a long shot, just a few examples of the more more significant ones to give you an idea of how it slots together. </p><div class="image">
<object type="image/svg+xml" data="uml-overview.svg">uml-overview.svg</object>
</div>
<p>At the very top of the diagram is the Root object. This is your ’way in’ to the OGRE system, and it’s where you tend to create the top-level objects that you need to deal with, like scene managers, rendering systems and render windows, loading plugins, all the fundamental stuff. If you don’t know where to start, Root is it for almost everything, although often it will just give you another object which will actually do the detail work, since Root itself is more of an organiser and facilitator object.</p>
<p>The majority of rest of OGRE’s classes fall into one of 3 roles:</p>
<dl compact="compact">
<dt>Scene Management </dt>
<dd><p class="startdd"></p>
<p>This is about the contents of your scene, how it’s structured, how it’s viewed from cameras, etc. Objects in this area are responsible for giving you a natural declarative interface to the world you’re building; i.e. you don’t tell OGRE "set these render states and then render 3 polygons", you tell it "I want an object here, here and here, with these materials on them, rendered from this view", and let it get on with it.</p>
<p class="enddd"></p>
</dd>
<dt>Resource Management </dt>
<dd><p class="startdd"></p>
<p>All rendering needs resources, whether it’s geometry, textures, fonts, whatever. It’s important to manage the loading, re-use and unloading of these things carefully, so that’s what classes in this area do.</p>
<p class="enddd"></p>
</dd>
<dt>Rendering </dt>
<dd><p class="startdd"></p>
<p>Finally, there’s getting the visuals on the screen - this is about the lower-level end of the rendering pipeline, the specific rendering system API objects like buffers, render states and the like and pushing it all down the pipeline. Classes in the Scene Management subsystem use this to get their higher-level scene information onto the screen.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You’ll notice that scattered around the edge are a number of plugins. OGRE is designed to be extended, and plugins are the usual way to go about it. Many of the classes in OGRE can be subclassed and extended, whether it’s changing the scene organisation through a custom SceneManager, adding a new render system implementation (e.g. Direct3D or OpenGL), or providing a way to load resources from another source (say from a web location or a database). Again this is just a small smattering of the kinds of things plugins can do, but as you can see they can plug in to almost any aspect of the system. This way, OGRE isn’t just a solution for one narrowly defined problem, it can extend to pretty much anything you need it to do.</p>
<h1><a class="anchor" id="The-Root-Object"></a>
The Root object</h1>
<p>The <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object is the entry point to the OGRE system. This object MUST be the first one to be created, and the last one to be destroyed. In the example applications I chose to make an instance of Root a member of my application object which ensured that it was created as soon as my application object was, and deleted when the application object was deleted.</p>
<p>The root object lets you configure the system, for example through the <a class="el" href="class_ogre_1_1_root.html#aedc76ef3bdc6514bd4655be0d70d6bcd" title="Displays a dialog asking the user to choose system settings. ">Ogre::Root::showConfigDialog</a> method which is an extremely handy method which performs all render system options detection and shows a dialog for the user to customise resolution, colour depth, full screen options etc. It also sets the options the user selects so that you can initialise the system directly afterwards.</p>
<p>The root object is also your method for obtaining pointers to other objects in the system, such as the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> and various other resource managers. See below for details.</p>
<p>Finally, if you run OGRE in continuous rendering mode, i.e. you want to always refresh all the rendering targets as fast as possible (the norm for games and demos, but not for windowed utilities), the root object has a method called <a class="el" href="class_ogre_1_1_root.html#a8eda253befda1255cbfd3b298f62449e" title="Starts / restarts the automatic rendering cycle. ">Ogre::Root::startRendering</a>, which when called will enter a continuous rendering loop which will only end when all rendering windows are closed, or any <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects indicate that they want to stop the cycle (see below for details of <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects).</p>
<h1><a class="anchor" id="The-RenderSystem-object"></a>
The RenderSystem object</h1>
<p>The <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object is actually an abstract class which defines the interface to the underlying 3D API. It is responsible for sending rendering operations to the API and setting all the various rendering options. This class is abstract because all the implementation is rendering API specific - there are API-specific subclasses for each rendering API (e.g. D3DRenderSystem for Direct3D). After the system has been initialised through <a class="el" href="class_ogre_1_1_root.html#a6ab8f204bbfb150ad180eb6e78e4639c" title="Initialises the renderer. ">Ogre::Root::initialise</a>, the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object for the selected rendering API is available via the <a class="el" href="class_ogre_1_1_root.html#ab8ca3d5abebba7a14650db5f618286d0" title="Retrieve a pointer to the currently selected render system. ">Ogre::Root::getRenderSystem()</a> method.</p>
<p>However, a typical application should not normally need to manipulate the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object directly - everything you need for rendering objects and customising settings should be available on the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, Material and other scene-oriented classes. It’s only if you want to create multiple rendering windows (completely separate windows in this case, not multiple viewports like a split-screen effect which is done via the RenderWindow class) or access other advanced features that you need access to the RenderSystem object.</p>
<p>For this reason I will not discuss the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object further in these tutorials. You can assume the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> handles the calls to the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> at the appropriate times.</p>
<h1><a class="anchor" id="The-SceneManager-object"></a>
The SceneManager object</h1>
<p>Apart from the <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object, this is probably the most critical part of the system from the application’s point of view. Certainly it will be the object which is most used by the application. The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> is in charge of the contents of the scene which is to be rendered by the engine. It is responsible for organising the contents using whatever technique it deems best, for creating and managing all the cameras, movable objects (entities), lights and materials (surface properties of objects), and for managing the ’world geometry’ which is the sprawling static geometry usually used to represent the immovable parts of a scene.</p>
<p>It is to the SceneManager that you go when you want to create a camera for the scene. It’s also where you go to retrieve or to remove a light from the scene. There is no need for your application to keep lists of objects, the SceneManager keeps a named set of all of the scene objects for you to access, should you need them. Look in the main documentation under the getCamera, getLight, getEntity etc methods.</p>
<p>The SceneManager also sends the scene to the RenderSystem object when it is time to render the scene. You never have to call the <a class="el" href="class_ogre_1_1_scene_manager.html#a29ba8a2cad151d307d04781f749e7e84" title="Prompts the class to send its contents to the renderer. ">Ogre::SceneManager::_renderScene</a> method directly though - it is called automatically whenever a rendering target is asked to update.</p>
<p>So most of your interaction with the SceneManager is during scene setup. You’re likely to call a great number of methods (perhaps driven by some input file containing the scene data) in order to set up your scene. You can also modify the contents of the scene dynamically during the rendering cycle if you create your own FrameListener object (see later).</p>
<p>Because different scene types require very different algorithmic approaches to deciding which objects get sent to the RenderSystem in order to attain good rendering performance, the SceneManager class is designed to be subclassed for different scene types. The default SceneManager object will render a scene, but it does little or no scene organisation and you should not expect the results to be high performance in the case of large scenes. The intention is that specialisations will be created for each type of scene such that under the surface the subclass will optimise the scene organisation for best performance given assumptions which can be made for that scene type. An example is the BspSceneManager which optimises rendering for large indoor levels based on a Binary Space Partition (BSP) tree.</p>
<p>The application using OGRE does not have to know which subclasses are available. The application simply calls <a class="el" href="class_ogre_1_1_root.html#a8bb8655f823a98007775f69278c1b0f9" title="create a default scene manager ">Ogre::Root::createSceneManager</a>(..) passing as a parameter one of a number of scene types (e.g. <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a0af8f844019e52fe112e9a946a919fcd">Ogre::ST_GENERIC</a>, <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a93f2170ad3a8132f29264dc3642a2dcf">Ogre::ST_INTERIOR</a> etc). OGRE will automatically use the best SceneManager subclass available for that scene type, or default to the basic SceneManager if a specialist one is not available. This allows the developers of OGRE to add new scene specialisations later and thus optimise previously unoptimised scene types without the user applications having to change any code.</p>
<h1><a class="anchor" id="The-ResourceGroupManager-Object"></a>
The ResourceGroupManager Object</h1>
<p>The <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a> class is actually a ’hub’ for loading of reusable resources like textures and meshes. It is the place that you define groups for your resources, so they may be unloaded and reloaded when you want. Servicing it are a number of ResourceManagers which manage the individual types of resource, like <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> or <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a>. In this context, resources are sets of data which must be loaded from somewhere to provide OGRE with the data it needs.</p>
<p>ResourceManagers ensure that resources are only loaded once and shared throughout the OGRE engine. They also manage the memory requirements of the resources they look after. They can also search in a number of locations for the resources they need, including multiple search paths and compressed archives (ZIP files).</p>
<p>Most of the time you won’t interact with resource managers directly. Resource managers will be called by other parts of the OGRE system as required, for example when you request for a texture to be added to a Material, the <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> will be called for you. If you like, you can call the appropriate resource manager directly to preload resources (if for example you want to prevent disk access later on) but most of the time it’s ok to let OGRE decide when to do it.</p>
<p>One thing you will want to do is to tell the resource managers where to look for resources. You do this via Root::getSingleton().addResourceLocation, which actually passes the information on to <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a>.</p>
<p>Because there is only ever 1 instance of each resource manager in the engine, if you do want to get a reference to a resource manager use the following syntax:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().someMethod()</div><div class="line"><a class="code" href="class_ogre_1_1_mesh_manager.html#a2efefa307b7987c5fcff7e8c47f79758">Ogre::MeshManager::getSingleton</a>().someMethod()</div></div><!-- fragment --> <h1><a class="anchor" id="The-Mesh-Object"></a>
The Mesh Object</h1>
<p>A <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object represents a discrete model, a set of geometry which is self-contained and is typically fairly small on a world scale. <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are assumed to represent movable objects and are not used for the sprawling level geometry typically used to create backgrounds.</p>
<p><a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are a type of resource, and are managed by the MeshManager resource manager. They are typically loaded from OGRE’s custom object format, the ’.mesh’ format. Mesh files are typically created by exporting from a modelling tool See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> and can be manipulated through various <a class="el" href="_mesh-_tools.html">Mesh Tools</a></p>
<p>You can also create Mesh objects manually by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html#ad99f5f2e2d045e5a4af301974e45c918" title="Creates a new Mesh specifically for manual definition rather than loading from an object file...">Ogre::MeshManager::createManual</a> method. This way you can define the geometry yourself, but this is outside the scope of this manual.</p>
<p>Mesh objects are the basis for the individual movable objects in the world, which are called <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>.</p>
<p>Mesh objects can also be animated using See <a class="el" href="_animation.html#Skeletal-Animation">Skeletal Animation</a>.</p>
<h1><a class="anchor" id="Entities"></a>
Entities</h1>
<p>An entity is an instance of a movable object in the scene. It could be a car, a person, a dog, a shuriken, whatever. The only assumption is that it does not necessarily have a fixed position in the world.</p>
<p>Entities are based on discrete meshes, i.e. collections of geometry which are self-contained and typically fairly small on a world scale, which are represented by the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object. Multiple entities can be based on the same mesh, since often you want to create multiple copies of the same type of object in a scene.</p>
<p>You create an entity by calling the <a class="el" href="class_ogre_1_1_scene_manager.html#afb393cca49de8b928f7dd60838047185" title="Create an Entity (instance of a discrete mesh). ">Ogre::SceneManager::createEntity</a> method, giving it a name and specifying the name of the mesh object which it will be based on (e.g. ’muscleboundhero.mesh’). The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> will ensure that the mesh is loaded by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a> resource manager for you. Only one copy of the Mesh will be loaded.</p>
<p>Ogre::Entities are not deemed to be a part of the scene until you attach them to a <a class="el" href="class_ogre_1_1_scene_node.html" title="Class representing a node in the scene graph. ">Ogre::SceneNode</a> (see the section below). By attaching entities to SceneNodes, you can create complex hierarchical relationships between the positions and orientations of entities. You then modify the positions of the nodes to indirectly affect the entity positions.</p>
<p>When a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> is loaded, it automatically comes with a number of materials defined. It is possible to have more than one material attached to a mesh - different parts of the mesh may use different materials. Any entity created from the mesh will automatically use the default materials. However, you can change this on a per-entity basis if you like so you can create a number of entities based on the same mesh but with different textures etc.</p>
<p>To understand how this works, you have to know that all Mesh objects are actually composed of <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects, each of which represents a part of the mesh using one Material. If a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> uses only one <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a>, it will only have one <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a>.</p>
<p>When an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> is created based on this Mesh, it is composed of (possibly) multiple <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects, each matching 1 for 1 with the <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects from the original Mesh. You can access the <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects using the <a class="el" href="class_ogre_1_1_entity.html#a23fc34c5e2eff03aae4cbf800c142724" title="Gets a pointer to a SubEntity, ie a part of an Entity. ">Ogre::Entity::getSubEntity</a> method. Once you have a reference to a <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a>, you can change the material it uses by calling it’s setMaterialName method. In this way you can make an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> deviate from the default materials and thus create an individual looking version of it.</p>
<h1><a class="anchor" id="Materials"></a>
Materials</h1>
<p>The <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a> object controls how objects in the scene are rendered. It specifies what basic surface properties objects have such as reflectance of colours, shininess etc, how many texture layers are present, what images are on them and how they are blended together, what special effects are applied such as environment mapping, what culling mode is used, how the textures are filtered etc.</p>
<p>Materials can either be set up programmatically, by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a> and tweaking the settings, or by specifying it in a ’script’ which is loaded at runtime. See <a class="el" href="_material-_scripts.html">Material Scripts</a> for more info.</p>
<p>Basically everything about the appearance of an object apart from it’s shape is controlled by the Material class.</p>
<p>The SceneManager class manages the master list of materials available to the scene. The list can be added to by the application by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a>, or by loading a Mesh (which will in turn load material properties). Whenever materials are added to the SceneManager, they start off with a default set of properties; these are defined by OGRE as the following:</p>
<ul>
<li>ambient reflectance = ColourValue::White (full)</li>
<li>diffuse reflectance = ColourValue::White (full)</li>
<li>specular reflectance = ColourValue::Black (none)</li>
<li>emissive = ColourValue::Black (none)</li>
<li>shininess = 0 (not shiny)</li>
<li>No texture layers (&amp; hence no textures)</li>
<li>SourceBlendFactor = SBF_ONE, DestBlendFactor = SBF_ZERO (opaque)</li>
<li>Depth buffer checking on</li>
<li>Depth buffer writing on</li>
<li>Depth buffer comparison function = CMPF_LESS_EQUAL</li>
<li>Culling mode = CULL_CLOCKWISE</li>
<li>Ambient lighting in scene = ColourValue(0.5, 0.5, 0.5) (mid-grey)</li>
<li>Dynamic lighting enabled</li>
<li>Gourad shading mode</li>
<li>Solid polygon mode</li>
<li>Bilinear texture filtering</li>
</ul>
<p>You can alter these settings by calling <a class="el" href="class_ogre_1_1_material_manager.html#ac70230fa9f123976d7f2b1257bd94bd4" title="Returns a pointer to the default Material settings. ">Ogre::MaterialManager::getDefaultSettings()</a> and making the required changes to the Material which is returned.</p>
<p>Entities automatically have Material’s associated with them if they use a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object, since the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object typically sets up it’s required materials on loading. You can also customise the material used by an entity as described in <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>. Just create a new Material, set it up how you like (you can copy an existing material into it if you like using a standard assignment statement) and point the SubEntity entries at it using <a class="el" href="class_ogre_1_1_sub_entity.html#ab8af6685128c68a5d7863bb5238ebc0f" title="Sets the name of the Material to be used. ">Ogre::SubEntity::setMaterialName()</a>.</p>
<h1><a class="anchor" id="Overlays"></a>
Overlays</h1>
<p>Overlays allow you to render 2D and 3D elements on top of the normal scene contents to create effects like heads-up displays (HUDs), menu systems, status panels etc. The frame rate statistics panel which comes as standard with OGRE is an example of an overlay. Overlays can contain 2D or 3D elements. 2D elements are used for HUDs, and 3D elements can be used to create cockpits or any other 3D object which you wish to be rendered on top of the rest of the scene.</p>
<p>You can create overlays either through the <a class="el" href="class_ogre_1_1_overlay_manager.html#ab6eb3dcc484ba5b06904ab2fb6b1aa23" title="Create a new Overlay. ">Ogre::OverlayManager::create</a> method, or you can define them in an .overlay script. See <a class="el" href="_overlay-_scripts.html">Overlay Scripts</a> for more info. In reality the latter is likely to be the most practical because it is easier to tweak (without the need to recompile the code). Note that you can define as many overlays as you like: they all start off life hidden, and you display them by calling <a class="el" href="class_ogre_1_1_overlay.html#a3c565f3e5cf61f43fa33542d713e8ccf" title="Shows the overlay if it was hidden. ">Ogre::Overlay::show</a>. You can also show multiple overlays at once, and their Z order is determined by the <a class="el" href="class_ogre_1_1_overlay.html#a5d8481d5fa0990846e53345f1a1cc705" title="Alters the Z-order of this overlay. ">Ogre::Overlay::setZOrder()</a> method.</p>
<p><a class="anchor" id="Notes-on-Integration"></a></p>
<h2>Notes on Integration</h2>
<p>The OverlaySystem is its own component, you need to manually initialize it, with the following two lines of code (mSceneMgr is a pointer to your current <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>):</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>* pOverlaySystem = <span class="keyword">new</span> <a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>();</div><div class="line">mSceneMgr-&gt;addRenderQueueListener(pOverlaySystem);</div></div><!-- fragment --><p>One <a class="el" href="class_ogre_1_1_overlay_system.html" title="This class simplify initialization / finalization of the overlay system. ">Ogre::OverlaySystem</a> per application is enough but you need to call addRenderQueueListener once per SceneManager.</p>
<p><a class="anchor" id="Creating-2D-Elements"></a></p>
<h2>Creating 2D Elements</h2>
<p>The <a class="el" href="class_ogre_1_1_overlay_element.html" title="Abstract definition of a 2D element to be displayed in an Overlay. ">Ogre::OverlayElement</a> class abstracts the details of 2D elements which are added to overlays. All items which can be added to overlays are derived from this class. It is possible (and encouraged) for users of OGRE to define their own custom subclasses of OverlayElement in order to provide their own user controls. The key common features of all OverlayElements are things like size, position, basic material name etc. Subclasses extend this behaviour to include more complex properties and behaviour.</p>
<p>An important built-in subclass of OverlayElement is <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a>. OverlayContainer is the same as a OverlayElement, except that it can contain other OverlayElements, grouping them together (allowing them to be moved together for example) and providing them with a local coordinate origin for easier lineup.</p>
<p>The third important class is <a class="el" href="class_ogre_1_1_overlay_manager.html" title="Manages Overlay objects, parsing them from .overlay files and storing a lookup library of them...">Ogre::OverlayManager</a>. Whenever an application wishes to create a 2D element to add to an overlay (or a container), it should call <a class="el" href="class_ogre_1_1_overlay_manager.html#a58166a773d27c22437323e8ef710406b" title="Creates a new OverlayElement of the type requested. ">Ogre::OverlayManager::createOverlayElement</a>. The type of element you wish to create is identified by a string, the reason being that it allows plugins to register new types of OverlayElement for you to create without you having to link specifically to those libraries. For example, to create a panel (a plain rectangular area which can contain other OverlayElements) you would call <code>OverlayManager::getSingleton().createOverlayElement("Panel", "myNewPanel");</code>.</p>
<p><a class="anchor" id="Adding-2D-Elements-to-the-Overlay"></a></p>
<h2>Adding 2D Elements to the Overlay</h2>
<p>Only OverlayContainers can be added direct to an overlay. The reason is that each level of container establishes the Zorder of the elements contained within it, so if you nest several containers, inner containers have a higher Z-order than outer ones to ensure they are displayed correctly. To add a container (such as a Panel) to the overlay, simply call <a class="el" href="class_ogre_1_1_overlay.html#a702b15ebaa6d3f8dbb2918500b76c9f0" title="Adds a 2D &#39;container&#39; to the overlay. ">Ogre::Overlay::add2D</a>.</p>
<p>If you wish to add child elements to that container, call <a class="el" href="class_ogre_1_1_overlay_container.html#aa9eec8f6dca98ac61766fc4785b84661" title="Adds another OverlayElement to this container. ">Ogre::OverlayContainer::addChild</a>. Child elements can be Ogre::OverlayElements or <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a> instances themselves. Remember that the position of a child element is relative to the top-left corner of it’s parent.</p>
<p><a class="anchor" id="A-word-about-2D-coordinates"></a></p>
<h2>A word about 2D coordinates</h2>
<p>OGRE allows you to place and size elements based on 2 coordinate systems: <b>relative</b> and <b>pixel</b> based.</p>
<dl compact="compact">
<dt>Pixel Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want to specify an exact size for your overlay items, and you don’t mind if those items get smaller on the screen if you increase the screen resolution (in fact you might want this). In this mode the only way to put something in the middle or at the right or bottom of the screen reliably in any resolution is to use the aligning options, whilst in relative mode you can do it just by using the right relative coordinates. This mode is very simple, the top-left of the screen is (0,0) and the bottom-right of the screen depends on the resolution. As mentioned above, you can use the aligning options to make the horizontal and vertical coordinate origins the right, bottom or center of the screen if you want to place pixel items in these locations without knowing the resolution.</p>
<p class="enddd"></p>
</dd>
<dt>Relative Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want items in the overlay to be the same size on the screen no matter what the resolution. In relative mode, the top-left of the screen is (0,0) and the bottom-right is (1,1). So if you place an element at (0.5, 0.5), it’s top-left corner is placed exactly in the center of the screen, no matter what resolution the application is running in. The same principle applies to sizes; if you set the width of an element to 0.5, it covers half the width of the screen. Note that because the aspect ratio of the screen is typically 1.3333 : 1 (width : height), an element with dimensions (0.25, 0.25) will not be square, but it will take up exactly 1/16th of the screen in area terms. If you want square-looking areas you will have to compensate using the typical aspect ratio e.g. use (0.1875, 0.25) instead.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Transforming-Overlays"></a> </p><h2>Transforming Overlays</h2>
<p>Another nice feature of overlays is being able to rotate, scroll and scale them as a whole. You can use this for zooming in / out menu systems, dropping them in from off screen and other nice effects. See the <a class="el" href="class_ogre_1_1_overlay.html#ae07b1a57498aad0299f0781b09ca9b39" title="Scrolls the overlay by the offsets provided. ">Ogre::Overlay::scroll</a>, <a class="el" href="class_ogre_1_1_overlay.html#a8282553a59253170ddd0488ab7179792" title="Adds the passed in angle to the rotation applied to this overlay. ">Ogre::Overlay::rotate</a> and <a class="el" href="class_ogre_1_1_overlay.html#adea3b88baa3697a657e7b5785f0ffedb" title="Sets the scaling factor of this overlay. ">Ogre::Overlay::setScale</a> methods for more information.</p>
<p><a class="anchor" id="GUI-systems"></a></p>
<h2>GUI systems</h2>
<p>Overlays are only really designed for non-interactive screen elements, although you can create a simple GUI using the <a class="el" href="trays.html">Trays System</a>. For a far more complete GUI solution, we recommend or <a href="https://github.com/OGRECave/ogre-imgui">Dear ImGui</a>, <a href="http://www.cegui.org.uk">CEGui</a> or <a href="http://mygui.info/">MyGUI</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: The Core Objects</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_the-_core-_objects.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">The Core Objects </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#The-Root-Object">The Root object</a></li>
<li class="level1"><a href="#The-RenderSystem-object">The RenderSystem object</a></li>
<li class="level1"><a href="#The-SceneManager-object">The SceneManager object</a></li>
<li class="level1"><a href="#The-ResourceGroupManager-Object">The ResourceGroupManager Object</a></li>
<li class="level1"><a href="#The-Mesh-Object">The Mesh Object</a></li>
<li class="level1"><a href="#Entities">Entities</a></li>
<li class="level1"><a href="#Materials">Materials</a></li>
<li class="level1"><a href="#Overlays">Overlays</a></li>
</ul>
</div>
<div class="textblock"><p>This tutorial gives you a quick summary of the core objects that you will use in OGRE and what they are used for.</p>
<p><a class="anchor" id="A-Word-About-Namespaces"></a></p>
<h1>A Word About Namespaces</h1>
<p>OGRE uses a C++ feature called namespaces. This lets you put classes, enums, structures, anything really within a ’namespace’ scope which is an easy way to prevent name clashes, i.e. situations where you have 2 things called the same thing. Since OGRE is designed to be used inside other applications, I wanted to be sure that name clashes would not be a problem. Some people prefix their classes/types with a short code because some compilers don’t support namespaces, but I chose to use them because they are the ’right’ way to do it. Sorry if you have a non-compliant compiler, but hey, the C++ standard has been defined for years, so compiler writers really have no excuse anymore. If your compiler doesn’t support namespaces then it’s probably because it’s sh*t - get a better one. ;)</p>
<p>This means every class, type etc should be prefixed with <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, e.g. <a class="el" href="class_ogre_1_1_camera.html" title="A viewpoint from which the scene will be rendered. ">Ogre::Camera</a>, <a class="el" href="class_ogre_1_1_vector3.html" title="Standard 3-dimensional vector. ">Ogre::Vector3</a> etc which means if elsewhere in your application you have used a Vector3 type you won’t get name clashes. To avoid lots of extra typing you can add a ’using namespace <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>;’ statement to your code which means you don’t have to type the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> prefix unless there is ambiguity (in the situation where you have another definition with the same name).</p>
<p><a class="anchor" id="Overview-from-10_002c000-feet"></a></p>
<h1>Overview from 10,000 feet</h1>
<p>Shown below is a diagram of some of the core objects and where they ’sit’ in the grand scheme of things. This is not all the classes by a long shot, just a few examples of the more more significant ones to give you an idea of how it slots together. </p><div class="image">
<object type="image/svg+xml" data="uml-overview.svg">uml-overview.svg</object>
</div>
<p>At the very top of the diagram is the Root object. This is your ’way in’ to the OGRE system, and it’s where you tend to create the top-level objects that you need to deal with, like scene managers, rendering systems and render windows, loading plugins, all the fundamental stuff. If you don’t know where to start, Root is it for almost everything, although often it will just give you another object which will actually do the detail work, since Root itself is more of an organiser and facilitator object.</p>
<p>The majority of rest of OGRE’s classes fall into one of 3 roles:</p>
<dl compact="compact">
<dt>Scene Management </dt>
<dd><p class="startdd"></p>
<p>This is about the contents of your scene, how it’s structured, how it’s viewed from cameras, etc. Objects in this area are responsible for giving you a natural declarative interface to the world you’re building; i.e. you don’t tell OGRE "set these render states and then render 3 polygons", you tell it "I want an object here, here and here, with these materials on them, rendered from this view", and let it get on with it.</p>
<p class="enddd"></p>
</dd>
<dt>Resource Management </dt>
<dd><p class="startdd"></p>
<p>All rendering needs resources, whether it’s geometry, textures, fonts, whatever. It’s important to manage the loading, re-use and unloading of these things carefully, so that’s what classes in this area do.</p>
<p class="enddd"></p>
</dd>
<dt>Rendering </dt>
<dd><p class="startdd"></p>
<p>Finally, there’s getting the visuals on the screen - this is about the lower-level end of the rendering pipeline, the specific rendering system API objects like buffers, render states and the like and pushing it all down the pipeline. Classes in the Scene Management subsystem use this to get their higher-level scene information onto the screen.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You’ll notice that scattered around the edge are a number of plugins. OGRE is designed to be extended, and plugins are the usual way to go about it. Many of the classes in OGRE can be subclassed and extended, whether it’s changing the scene organisation through a custom SceneManager, adding a new render system implementation (e.g. Direct3D or OpenGL), or providing a way to load resources from another source (say from a web location or a database). Again this is just a small smattering of the kinds of things plugins can do, but as you can see they can plug in to almost any aspect of the system. This way, OGRE isn’t just a solution for one narrowly defined problem, it can extend to pretty much anything you need it to do.</p>
<h1><a class="anchor" id="The-Root-Object"></a>
The Root object</h1>
<p>The <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object is the entry point to the OGRE system. This object MUST be the first one to be created, and the last one to be destroyed. In the example applications I chose to make an instance of Root a member of my application object which ensured that it was created as soon as my application object was, and deleted when the application object was deleted.</p>
<p>The root object lets you configure the system, for example through the <a class="el" href="class_ogre_1_1_root.html#aedc76ef3bdc6514bd4655be0d70d6bcd" title="Displays a dialog asking the user to choose system settings. ">Ogre::Root::showConfigDialog</a> method which is an extremely handy method which performs all render system options detection and shows a dialog for the user to customise resolution, colour depth, full screen options etc. It also sets the options the user selects so that you can initialise the system directly afterwards.</p>
<p>The root object is also your method for obtaining pointers to other objects in the system, such as the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> and various other resource managers. See below for details.</p>
<p>Finally, if you run OGRE in continuous rendering mode, i.e. you want to always refresh all the rendering targets as fast as possible (the norm for games and demos, but not for windowed utilities), the root object has a method called <a class="el" href="class_ogre_1_1_root.html#a8eda253befda1255cbfd3b298f62449e" title="Starts / restarts the automatic rendering cycle. ">Ogre::Root::startRendering</a>, which when called will enter a continuous rendering loop which will only end when all rendering windows are closed, or any <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects indicate that they want to stop the cycle (see below for details of <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects).</p>
<h1><a class="anchor" id="The-RenderSystem-object"></a>
The RenderSystem object</h1>
<p>The <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object is actually an abstract class which defines the interface to the underlying 3D API. It is responsible for sending rendering operations to the API and setting all the various rendering options. This class is abstract because all the implementation is rendering API specific - there are API-specific subclasses for each rendering API (e.g. D3DRenderSystem for Direct3D). After the system has been initialised through <a class="el" href="class_ogre_1_1_root.html#a6ab8f204bbfb150ad180eb6e78e4639c" title="Initialises the renderer. ">Ogre::Root::initialise</a>, the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object for the selected rendering API is available via the <a class="el" href="class_ogre_1_1_root.html#ab8ca3d5abebba7a14650db5f618286d0" title="Retrieve a pointer to the currently selected render system. ">Ogre::Root::getRenderSystem()</a> method.</p>
<p>However, a typical application should not normally need to manipulate the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object directly - everything you need for rendering objects and customising settings should be available on the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, Material and other scene-oriented classes. It’s only if you want to create multiple rendering windows (completely separate windows in this case, not multiple viewports like a split-screen effect which is done via the RenderWindow class) or access other advanced features that you need access to the RenderSystem object.</p>
<p>For this reason I will not discuss the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object further in these tutorials. You can assume the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> handles the calls to the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> at the appropriate times.</p>
<h1><a class="anchor" id="The-SceneManager-object"></a>
The SceneManager object</h1>
<p>Apart from the <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object, this is probably the most critical part of the system from the application’s point of view. Certainly it will be the object which is most used by the application. The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> is in charge of the contents of the scene which is to be rendered by the engine. It is responsible for organising the contents using whatever technique it deems best, for creating and managing all the cameras, movable objects (entities), lights and materials (surface properties of objects), and for managing the ’world geometry’ which is the sprawling static geometry usually used to represent the immovable parts of a scene.</p>
<p>It is to the SceneManager that you go when you want to create a camera for the scene. It’s also where you go to retrieve or to remove a light from the scene. There is no need for your application to keep lists of objects, the SceneManager keeps a named set of all of the scene objects for you to access, should you need them. Look in the main documentation under the getCamera, getLight, getEntity etc methods.</p>
<p>The SceneManager also sends the scene to the RenderSystem object when it is time to render the scene. You never have to call the <a class="el" href="class_ogre_1_1_scene_manager.html#a29ba8a2cad151d307d04781f749e7e84" title="Prompts the class to send its contents to the renderer. ">Ogre::SceneManager::_renderScene</a> method directly though - it is called automatically whenever a rendering target is asked to update.</p>
<p>So most of your interaction with the SceneManager is during scene setup. You’re likely to call a great number of methods (perhaps driven by some input file containing the scene data) in order to set up your scene. You can also modify the contents of the scene dynamically during the rendering cycle if you create your own FrameListener object (see later).</p>
<p>Because different scene types require very different algorithmic approaches to deciding which objects get sent to the RenderSystem in order to attain good rendering performance, the SceneManager class is designed to be subclassed for different scene types. The default SceneManager object will render a scene, but it does little or no scene organisation and you should not expect the results to be high performance in the case of large scenes. The intention is that specialisations will be created for each type of scene such that under the surface the subclass will optimise the scene organisation for best performance given assumptions which can be made for that scene type. An example is the BspSceneManager which optimises rendering for large indoor levels based on a Binary Space Partition (BSP) tree.</p>
<p>The application using OGRE does not have to know which subclasses are available. The application simply calls <a class="el" href="class_ogre_1_1_root.html#a8bb8655f823a98007775f69278c1b0f9" title="create a default scene manager ">Ogre::Root::createSceneManager</a>(..) passing as a parameter one of a number of scene types (e.g. <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a0af8f844019e52fe112e9a946a919fcd">Ogre::ST_GENERIC</a>, <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a93f2170ad3a8132f29264dc3642a2dcf">Ogre::ST_INTERIOR</a> etc). OGRE will automatically use the best SceneManager subclass available for that scene type, or default to the basic SceneManager if a specialist one is not available. This allows the developers of OGRE to add new scene specialisations later and thus optimise previously unoptimised scene types without the user applications having to change any code.</p>
<h1><a class="anchor" id="The-ResourceGroupManager-Object"></a>
The ResourceGroupManager Object</h1>
<p>The <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a> class is actually a ’hub’ for loading of reusable resources like textures and meshes. It is the place that you define groups for your resources, so they may be unloaded and reloaded when you want. Servicing it are a number of ResourceManagers which manage the individual types of resource, like <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> or <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a>. In this context, resources are sets of data which must be loaded from somewhere to provide OGRE with the data it needs.</p>
<p>ResourceManagers ensure that resources are only loaded once and shared throughout the OGRE engine. They also manage the memory requirements of the resources they look after. They can also search in a number of locations for the resources they need, including multiple search paths and compressed archives (ZIP files).</p>
<p>Most of the time you won’t interact with resource managers directly. Resource managers will be called by other parts of the OGRE system as required, for example when you request for a texture to be added to a Material, the <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> will be called for you. If you like, you can call the appropriate resource manager directly to preload resources (if for example you want to prevent disk access later on) but most of the time it’s ok to let OGRE decide when to do it.</p>
<p>One thing you will want to do is to tell the resource managers where to look for resources. You do this via Root::getSingleton().addResourceLocation, which actually passes the information on to <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a>.</p>
<p>Because there is only ever 1 instance of each resource manager in the engine, if you do want to get a reference to a resource manager use the following syntax:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().someMethod()</div><div class="line"><a class="code" href="class_ogre_1_1_mesh_manager.html#a2efefa307b7987c5fcff7e8c47f79758">Ogre::MeshManager::getSingleton</a>().someMethod()</div></div><!-- fragment --> <h1><a class="anchor" id="The-Mesh-Object"></a>
The Mesh Object</h1>
<p>A <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object represents a discrete model, a set of geometry which is self-contained and is typically fairly small on a world scale. <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are assumed to represent movable objects and are not used for the sprawling level geometry typically used to create backgrounds.</p>
<p><a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are a type of resource, and are managed by the MeshManager resource manager. They are typically loaded from OGRE’s custom object format, the ’.mesh’ format. Mesh files are typically created by exporting from a modelling tool See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> and can be manipulated through various <a class="el" href="_mesh-_tools.html">Mesh Tools</a></p>
<p>You can also create Mesh objects manually by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html#ad99f5f2e2d045e5a4af301974e45c918" title="Creates a new Mesh specifically for manual definition rather than loading from an object file...">Ogre::MeshManager::createManual</a> method. This way you can define the geometry yourself, but this is outside the scope of this manual.</p>
<p>Mesh objects are the basis for the individual movable objects in the world, which are called <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>.</p>
<p>Mesh objects can also be animated using See <a class="el" href="_animation.html#Skeletal-Animation">Skeletal Animation</a>.</p>
<h1><a class="anchor" id="Entities"></a>
Entities</h1>
<p>An entity is an instance of a movable object in the scene. It could be a car, a person, a dog, a shuriken, whatever. The only assumption is that it does not necessarily have a fixed position in the world.</p>
<p>Entities are based on discrete meshes, i.e. collections of geometry which are self-contained and typically fairly small on a world scale, which are represented by the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object. Multiple entities can be based on the same mesh, since often you want to create multiple copies of the same type of object in a scene.</p>
<p>You create an entity by calling the <a class="el" href="class_ogre_1_1_scene_manager.html#afb393cca49de8b928f7dd60838047185" title="Create an Entity (instance of a discrete mesh). ">Ogre::SceneManager::createEntity</a> method, giving it a name and specifying the name of the mesh object which it will be based on (e.g. ’muscleboundhero.mesh’). The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> will ensure that the mesh is loaded by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a> resource manager for you. Only one copy of the Mesh will be loaded.</p>
<p>Ogre::Entities are not deemed to be a part of the scene until you attach them to a <a class="el" href="class_ogre_1_1_scene_node.html" title="Class representing a node in the scene graph. ">Ogre::SceneNode</a> (see the section below). By attaching entities to SceneNodes, you can create complex hierarchical relationships between the positions and orientations of entities. You then modify the positions of the nodes to indirectly affect the entity positions.</p>
<p>When a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> is loaded, it automatically comes with a number of materials defined. It is possible to have more than one material attached to a mesh - different parts of the mesh may use different materials. Any entity created from the mesh will automatically use the default materials. However, you can change this on a per-entity basis if you like so you can create a number of entities based on the same mesh but with different textures etc.</p>
<p>To understand how this works, you have to know that all Mesh objects are actually composed of <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects, each of which represents a part of the mesh using one Material. If a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> uses only one <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a>, it will only have one <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a>.</p>
<p>When an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> is created based on this Mesh, it is composed of (possibly) multiple <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects, each matching 1 for 1 with the <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects from the original Mesh. You can access the <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects using the <a class="el" href="class_ogre_1_1_entity.html#a23fc34c5e2eff03aae4cbf800c142724" title="Gets a pointer to a SubEntity, ie a part of an Entity. ">Ogre::Entity::getSubEntity</a> method. Once you have a reference to a <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a>, you can change the material it uses by calling it’s setMaterialName method. In this way you can make an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> deviate from the default materials and thus create an individual looking version of it.</p>
<h1><a class="anchor" id="Materials"></a>
Materials</h1>
<p>The <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a> object controls how objects in the scene are rendered. It specifies what basic surface properties objects have such as reflectance of colours, shininess etc, how many texture layers are present, what images are on them and how they are blended together, what special effects are applied such as environment mapping, what culling mode is used, how the textures are filtered etc.</p>
<p>Materials can either be set up programmatically, by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a> and tweaking the settings, or by specifying it in a ’script’ which is loaded at runtime. See <a class="el" href="_material-_scripts.html">Material Scripts</a> for more info.</p>
<p>Basically everything about the appearance of an object apart from it’s shape is controlled by the Material class.</p>
<p>The SceneManager class manages the master list of materials available to the scene. The list can be added to by the application by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a>, or by loading a Mesh (which will in turn load material properties). Whenever materials are added to the SceneManager, they start off with a default set of properties; these are defined by OGRE as the following:</p>
<ul>
<li>ambient reflectance = ColourValue::White (full)</li>
<li>diffuse reflectance = ColourValue::White (full)</li>
<li>specular reflectance = ColourValue::Black (none)</li>
<li>emissive = ColourValue::Black (none)</li>
<li>shininess = 0 (not shiny)</li>
<li>No texture layers (&amp; hence no textures)</li>
<li>SourceBlendFactor = SBF_ONE, DestBlendFactor = SBF_ZERO (opaque)</li>
<li>Depth buffer checking on</li>
<li>Depth buffer writing on</li>
<li>Depth buffer comparison function = CMPF_LESS_EQUAL</li>
<li>Culling mode = CULL_CLOCKWISE</li>
<li>Ambient lighting in scene = ColourValue(0.5, 0.5, 0.5) (mid-grey)</li>
<li>Dynamic lighting enabled</li>
<li>Gourad shading mode</li>
<li>Solid polygon mode</li>
<li>Bilinear texture filtering</li>
</ul>
<p>You can alter these settings by calling <a class="el" href="class_ogre_1_1_material_manager.html#ac70230fa9f123976d7f2b1257bd94bd4" title="Returns a pointer to the default Material settings. ">Ogre::MaterialManager::getDefaultSettings()</a> and making the required changes to the Material which is returned.</p>
<p>Entities automatically have Material’s associated with them if they use a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object, since the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object typically sets up it’s required materials on loading. You can also customise the material used by an entity as described in <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>. Just create a new Material, set it up how you like (you can copy an existing material into it if you like using a standard assignment statement) and point the SubEntity entries at it using <a class="el" href="class_ogre_1_1_sub_entity.html#ab8af6685128c68a5d7863bb5238ebc0f" title="Sets the name of the Material to be used. ">Ogre::SubEntity::setMaterialName()</a>.</p>
<h1><a class="anchor" id="Overlays"></a>
Overlays</h1>
<p>Overlays allow you to render 2D and 3D elements on top of the normal scene contents to create effects like heads-up displays (HUDs), menu systems, status panels etc. The frame rate statistics panel which comes as standard with OGRE is an example of an overlay. Overlays can contain 2D or 3D elements. 2D elements are used for HUDs, and 3D elements can be used to create cockpits or any other 3D object which you wish to be rendered on top of the rest of the scene.</p>
<p>You can create overlays either through the <a class="el" href="class_ogre_1_1_overlay_manager.html#ab6eb3dcc484ba5b06904ab2fb6b1aa23" title="Create a new Overlay. ">Ogre::OverlayManager::create</a> method, or you can define them in an .overlay script. See <a class="el" href="_overlay-_scripts.html">Overlay Scripts</a> for more info. In reality the latter is likely to be the most practical because it is easier to tweak (without the need to recompile the code). Note that you can define as many overlays as you like: they all start off life hidden, and you display them by calling <a class="el" href="class_ogre_1_1_overlay.html#a3c565f3e5cf61f43fa33542d713e8ccf" title="Shows the overlay if it was hidden. ">Ogre::Overlay::show</a>. You can also show multiple overlays at once, and their Z order is determined by the <a class="el" href="class_ogre_1_1_overlay.html#a5d8481d5fa0990846e53345f1a1cc705" title="Alters the Z-order of this overlay. ">Ogre::Overlay::setZOrder()</a> method.</p>
<p><a class="anchor" id="Notes-on-Integration"></a></p>
<h2>Notes on Integration</h2>
<p>The OverlaySystem is its own component, you need to manually initialize it, with the following two lines of code (mSceneMgr is a pointer to your current <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>):</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>* pOverlaySystem = <span class="keyword">new</span> <a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>();</div><div class="line">mSceneMgr-&gt;addRenderQueueListener(pOverlaySystem);</div></div><!-- fragment --><p>One <a class="el" href="class_ogre_1_1_overlay_system.html" title="This class simplify initialization / finalization of the overlay system. ">Ogre::OverlaySystem</a> per application is enough but you need to call addRenderQueueListener once per SceneManager.</p>
<p><a class="anchor" id="Creating-2D-Elements"></a></p>
<h2>Creating 2D Elements</h2>
<p>The <a class="el" href="class_ogre_1_1_overlay_element.html" title="Abstract definition of a 2D element to be displayed in an Overlay. ">Ogre::OverlayElement</a> class abstracts the details of 2D elements which are added to overlays. All items which can be added to overlays are derived from this class. It is possible (and encouraged) for users of OGRE to define their own custom subclasses of OverlayElement in order to provide their own user controls. The key common features of all OverlayElements are things like size, position, basic material name etc. Subclasses extend this behaviour to include more complex properties and behaviour.</p>
<p>An important built-in subclass of OverlayElement is <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a>. OverlayContainer is the same as a OverlayElement, except that it can contain other OverlayElements, grouping them together (allowing them to be moved together for example) and providing them with a local coordinate origin for easier lineup.</p>
<p>The third important class is <a class="el" href="class_ogre_1_1_overlay_manager.html" title="Manages Overlay objects, parsing them from .overlay files and storing a lookup library of them...">Ogre::OverlayManager</a>. Whenever an application wishes to create a 2D element to add to an overlay (or a container), it should call <a class="el" href="class_ogre_1_1_overlay_manager.html#a58166a773d27c22437323e8ef710406b" title="Creates a new OverlayElement of the type requested. ">Ogre::OverlayManager::createOverlayElement</a>. The type of element you wish to create is identified by a string, the reason being that it allows plugins to register new types of OverlayElement for you to create without you having to link specifically to those libraries. For example, to create a panel (a plain rectangular area which can contain other OverlayElements) you would call <code>OverlayManager::getSingleton().createOverlayElement("Panel", "myNewPanel");</code>.</p>
<p><a class="anchor" id="Adding-2D-Elements-to-the-Overlay"></a></p>
<h2>Adding 2D Elements to the Overlay</h2>
<p>Only OverlayContainers can be added direct to an overlay. The reason is that each level of container establishes the Zorder of the elements contained within it, so if you nest several containers, inner containers have a higher Z-order than outer ones to ensure they are displayed correctly. To add a container (such as a Panel) to the overlay, simply call <a class="el" href="class_ogre_1_1_overlay.html#a702b15ebaa6d3f8dbb2918500b76c9f0" title="Adds a 2D &#39;container&#39; to the overlay. ">Ogre::Overlay::add2D</a>.</p>
<p>If you wish to add child elements to that container, call <a class="el" href="class_ogre_1_1_overlay_container.html#aa9eec8f6dca98ac61766fc4785b84661" title="Adds another OverlayElement to this container. ">Ogre::OverlayContainer::addChild</a>. Child elements can be Ogre::OverlayElements or <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a> instances themselves. Remember that the position of a child element is relative to the top-left corner of it’s parent.</p>
<p><a class="anchor" id="A-word-about-2D-coordinates"></a></p>
<h2>A word about 2D coordinates</h2>
<p>OGRE allows you to place and size elements based on 2 coordinate systems: <b>relative</b> and <b>pixel</b> based.</p>
<dl compact="compact">
<dt>Pixel Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want to specify an exact size for your overlay items, and you don’t mind if those items get smaller on the screen if you increase the screen resolution (in fact you might want this). In this mode the only way to put something in the middle or at the right or bottom of the screen reliably in any resolution is to use the aligning options, whilst in relative mode you can do it just by using the right relative coordinates. This mode is very simple, the top-left of the screen is (0,0) and the bottom-right of the screen depends on the resolution. As mentioned above, you can use the aligning options to make the horizontal and vertical coordinate origins the right, bottom or center of the screen if you want to place pixel items in these locations without knowing the resolution.</p>
<p class="enddd"></p>
</dd>
<dt>Relative Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want items in the overlay to be the same size on the screen no matter what the resolution. In relative mode, the top-left of the screen is (0,0) and the bottom-right is (1,1). So if you place an element at (0.5, 0.5), it’s top-left corner is placed exactly in the center of the screen, no matter what resolution the application is running in. The same principle applies to sizes; if you set the width of an element to 0.5, it covers half the width of the screen. Note that because the aspect ratio of the screen is typically 1.3333 : 1 (width : height), an element with dimensions (0.25, 0.25) will not be square, but it will take up exactly 1/16th of the screen in area terms. If you want square-looking areas you will have to compensate using the typical aspect ratio e.g. use (0.1875, 0.25) instead.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Transforming-Overlays"></a> </p><h2>Transforming Overlays</h2>
<p>Another nice feature of overlays is being able to rotate, scroll and scale them as a whole. You can use this for zooming in / out menu systems, dropping them in from off screen and other nice effects. See the <a class="el" href="class_ogre_1_1_overlay.html#ae07b1a57498aad0299f0781b09ca9b39" title="Scrolls the overlay by the offsets provided. ">Ogre::Overlay::scroll</a>, <a class="el" href="class_ogre_1_1_overlay.html#a8282553a59253170ddd0488ab7179792" title="Adds the passed in angle to the rotation applied to this overlay. ">Ogre::Overlay::rotate</a> and <a class="el" href="class_ogre_1_1_overlay.html#adea3b88baa3697a657e7b5785f0ffedb" title="Sets the scaling factor of this overlay. ">Ogre::Overlay::setScale</a> methods for more information.</p>
<p><a class="anchor" id="GUI-systems"></a></p>
<h2>GUI systems</h2>
<p>Overlays are only really designed for non-interactive screen elements, although you can create a simple GUI using the <a class="el" href="trays.html">Trays System</a>. For a far more complete GUI solution, we recommend or <a href="https://github.com/OGRECave/ogre-imgui">Dear ImGui</a>, <a href="http://www.cegui.org.uk">CEGui</a> or <a href="http://mygui.info/">MyGUI</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: The Core Objects</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_the-_core-_objects.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">The Core Objects </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#The-Root-Object">The Root object</a></li>
<li class="level1"><a href="#The-RenderSystem-object">The RenderSystem object</a></li>
<li class="level1"><a href="#The-SceneManager-object">The SceneManager object</a></li>
<li class="level1"><a href="#The-ResourceGroupManager-Object">The ResourceGroupManager Object</a></li>
<li class="level1"><a href="#The-Mesh-Object">The Mesh Object</a></li>
<li class="level1"><a href="#Entities">Entities</a></li>
<li class="level1"><a href="#Materials">Materials</a></li>
<li class="level1"><a href="#Overlays">Overlays</a></li>
</ul>
</div>
<div class="textblock"><p>This tutorial gives you a quick summary of the core objects that you will use in OGRE and what they are used for.</p>
<p><a class="anchor" id="A-Word-About-Namespaces"></a></p>
<h1>A Word About Namespaces</h1>
<p>OGRE uses a C++ feature called namespaces. This lets you put classes, enums, structures, anything really within a ’namespace’ scope which is an easy way to prevent name clashes, i.e. situations where you have 2 things called the same thing. Since OGRE is designed to be used inside other applications, I wanted to be sure that name clashes would not be a problem. Some people prefix their classes/types with a short code because some compilers don’t support namespaces, but I chose to use them because they are the ’right’ way to do it. Sorry if you have a non-compliant compiler, but hey, the C++ standard has been defined for years, so compiler writers really have no excuse anymore. If your compiler doesn’t support namespaces then it’s probably because it’s sh*t - get a better one. ;)</p>
<p>This means every class, type etc should be prefixed with <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, e.g. <a class="el" href="class_ogre_1_1_camera.html" title="A viewpoint from which the scene will be rendered. ">Ogre::Camera</a>, <a class="el" href="class_ogre_1_1_vector3.html" title="Standard 3-dimensional vector. ">Ogre::Vector3</a> etc which means if elsewhere in your application you have used a Vector3 type you won’t get name clashes. To avoid lots of extra typing you can add a ’using namespace <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>;’ statement to your code which means you don’t have to type the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> prefix unless there is ambiguity (in the situation where you have another definition with the same name).</p>
<p><a class="anchor" id="Overview-from-10_002c000-feet"></a></p>
<h1>Overview from 10,000 feet</h1>
<p>Shown below is a diagram of some of the core objects and where they ’sit’ in the grand scheme of things. This is not all the classes by a long shot, just a few examples of the more more significant ones to give you an idea of how it slots together. </p><div class="image">
<object type="image/svg+xml" data="uml-overview.svg">uml-overview.svg</object>
</div>
<p>At the very top of the diagram is the Root object. This is your ’way in’ to the OGRE system, and it’s where you tend to create the top-level objects that you need to deal with, like scene managers, rendering systems and render windows, loading plugins, all the fundamental stuff. If you don’t know where to start, Root is it for almost everything, although often it will just give you another object which will actually do the detail work, since Root itself is more of an organiser and facilitator object.</p>
<p>The majority of rest of OGRE’s classes fall into one of 3 roles:</p>
<dl compact="compact">
<dt>Scene Management </dt>
<dd><p class="startdd"></p>
<p>This is about the contents of your scene, how it’s structured, how it’s viewed from cameras, etc. Objects in this area are responsible for giving you a natural declarative interface to the world you’re building; i.e. you don’t tell OGRE "set these render states and then render 3 polygons", you tell it "I want an object here, here and here, with these materials on them, rendered from this view", and let it get on with it.</p>
<p class="enddd"></p>
</dd>
<dt>Resource Management </dt>
<dd><p class="startdd"></p>
<p>All rendering needs resources, whether it’s geometry, textures, fonts, whatever. It’s important to manage the loading, re-use and unloading of these things carefully, so that’s what classes in this area do.</p>
<p class="enddd"></p>
</dd>
<dt>Rendering </dt>
<dd><p class="startdd"></p>
<p>Finally, there’s getting the visuals on the screen - this is about the lower-level end of the rendering pipeline, the specific rendering system API objects like buffers, render states and the like and pushing it all down the pipeline. Classes in the Scene Management subsystem use this to get their higher-level scene information onto the screen.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You’ll notice that scattered around the edge are a number of plugins. OGRE is designed to be extended, and plugins are the usual way to go about it. Many of the classes in OGRE can be subclassed and extended, whether it’s changing the scene organisation through a custom SceneManager, adding a new render system implementation (e.g. Direct3D or OpenGL), or providing a way to load resources from another source (say from a web location or a database). Again this is just a small smattering of the kinds of things plugins can do, but as you can see they can plug in to almost any aspect of the system. This way, OGRE isn’t just a solution for one narrowly defined problem, it can extend to pretty much anything you need it to do.</p>
<h1><a class="anchor" id="The-Root-Object"></a>
The Root object</h1>
<p>The <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object is the entry point to the OGRE system. This object MUST be the first one to be created, and the last one to be destroyed. In the example applications I chose to make an instance of Root a member of my application object which ensured that it was created as soon as my application object was, and deleted when the application object was deleted.</p>
<p>The root object lets you configure the system, for example through the <a class="el" href="class_ogre_1_1_root.html#aedc76ef3bdc6514bd4655be0d70d6bcd" title="Displays a dialog asking the user to choose system settings. ">Ogre::Root::showConfigDialog</a> method which is an extremely handy method which performs all render system options detection and shows a dialog for the user to customise resolution, colour depth, full screen options etc. It also sets the options the user selects so that you can initialise the system directly afterwards.</p>
<p>The root object is also your method for obtaining pointers to other objects in the system, such as the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> and various other resource managers. See below for details.</p>
<p>Finally, if you run OGRE in continuous rendering mode, i.e. you want to always refresh all the rendering targets as fast as possible (the norm for games and demos, but not for windowed utilities), the root object has a method called <a class="el" href="class_ogre_1_1_root.html#a8eda253befda1255cbfd3b298f62449e" title="Starts / restarts the automatic rendering cycle. ">Ogre::Root::startRendering</a>, which when called will enter a continuous rendering loop which will only end when all rendering windows are closed, or any <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects indicate that they want to stop the cycle (see below for details of <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects).</p>
<h1><a class="anchor" id="The-RenderSystem-object"></a>
The RenderSystem object</h1>
<p>The <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object is actually an abstract class which defines the interface to the underlying 3D API. It is responsible for sending rendering operations to the API and setting all the various rendering options. This class is abstract because all the implementation is rendering API specific - there are API-specific subclasses for each rendering API (e.g. D3DRenderSystem for Direct3D). After the system has been initialised through <a class="el" href="class_ogre_1_1_root.html#a6ab8f204bbfb150ad180eb6e78e4639c" title="Initialises the renderer. ">Ogre::Root::initialise</a>, the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object for the selected rendering API is available via the <a class="el" href="class_ogre_1_1_root.html#ab8ca3d5abebba7a14650db5f618286d0" title="Retrieve a pointer to the currently selected render system. ">Ogre::Root::getRenderSystem()</a> method.</p>
<p>However, a typical application should not normally need to manipulate the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object directly - everything you need for rendering objects and customising settings should be available on the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, Material and other scene-oriented classes. It’s only if you want to create multiple rendering windows (completely separate windows in this case, not multiple viewports like a split-screen effect which is done via the RenderWindow class) or access other advanced features that you need access to the RenderSystem object.</p>
<p>For this reason I will not discuss the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object further in these tutorials. You can assume the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> handles the calls to the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> at the appropriate times.</p>
<h1><a class="anchor" id="The-SceneManager-object"></a>
The SceneManager object</h1>
<p>Apart from the <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object, this is probably the most critical part of the system from the application’s point of view. Certainly it will be the object which is most used by the application. The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> is in charge of the contents of the scene which is to be rendered by the engine. It is responsible for organising the contents using whatever technique it deems best, for creating and managing all the cameras, movable objects (entities), lights and materials (surface properties of objects), and for managing the ’world geometry’ which is the sprawling static geometry usually used to represent the immovable parts of a scene.</p>
<p>It is to the SceneManager that you go when you want to create a camera for the scene. It’s also where you go to retrieve or to remove a light from the scene. There is no need for your application to keep lists of objects, the SceneManager keeps a named set of all of the scene objects for you to access, should you need them. Look in the main documentation under the getCamera, getLight, getEntity etc methods.</p>
<p>The SceneManager also sends the scene to the RenderSystem object when it is time to render the scene. You never have to call the <a class="el" href="class_ogre_1_1_scene_manager.html#a29ba8a2cad151d307d04781f749e7e84" title="Prompts the class to send its contents to the renderer. ">Ogre::SceneManager::_renderScene</a> method directly though - it is called automatically whenever a rendering target is asked to update.</p>
<p>So most of your interaction with the SceneManager is during scene setup. You’re likely to call a great number of methods (perhaps driven by some input file containing the scene data) in order to set up your scene. You can also modify the contents of the scene dynamically during the rendering cycle if you create your own FrameListener object (see later).</p>
<p>Because different scene types require very different algorithmic approaches to deciding which objects get sent to the RenderSystem in order to attain good rendering performance, the SceneManager class is designed to be subclassed for different scene types. The default SceneManager object will render a scene, but it does little or no scene organisation and you should not expect the results to be high performance in the case of large scenes. The intention is that specialisations will be created for each type of scene such that under the surface the subclass will optimise the scene organisation for best performance given assumptions which can be made for that scene type. An example is the BspSceneManager which optimises rendering for large indoor levels based on a Binary Space Partition (BSP) tree.</p>
<p>The application using OGRE does not have to know which subclasses are available. The application simply calls <a class="el" href="class_ogre_1_1_root.html#a8bb8655f823a98007775f69278c1b0f9" title="create a default scene manager ">Ogre::Root::createSceneManager</a>(..) passing as a parameter one of a number of scene types (e.g. <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a0af8f844019e52fe112e9a946a919fcd">Ogre::ST_GENERIC</a>, <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a93f2170ad3a8132f29264dc3642a2dcf">Ogre::ST_INTERIOR</a> etc). OGRE will automatically use the best SceneManager subclass available for that scene type, or default to the basic SceneManager if a specialist one is not available. This allows the developers of OGRE to add new scene specialisations later and thus optimise previously unoptimised scene types without the user applications having to change any code.</p>
<h1><a class="anchor" id="The-ResourceGroupManager-Object"></a>
The ResourceGroupManager Object</h1>
<p>The <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a> class is actually a ’hub’ for loading of reusable resources like textures and meshes. It is the place that you define groups for your resources, so they may be unloaded and reloaded when you want. Servicing it are a number of ResourceManagers which manage the individual types of resource, like <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> or <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a>. In this context, resources are sets of data which must be loaded from somewhere to provide OGRE with the data it needs.</p>
<p>ResourceManagers ensure that resources are only loaded once and shared throughout the OGRE engine. They also manage the memory requirements of the resources they look after. They can also search in a number of locations for the resources they need, including multiple search paths and compressed archives (ZIP files).</p>
<p>Most of the time you won’t interact with resource managers directly. Resource managers will be called by other parts of the OGRE system as required, for example when you request for a texture to be added to a Material, the <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> will be called for you. If you like, you can call the appropriate resource manager directly to preload resources (if for example you want to prevent disk access later on) but most of the time it’s ok to let OGRE decide when to do it.</p>
<p>One thing you will want to do is to tell the resource managers where to look for resources. You do this via Root::getSingleton().addResourceLocation, which actually passes the information on to <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a>.</p>
<p>Because there is only ever 1 instance of each resource manager in the engine, if you do want to get a reference to a resource manager use the following syntax:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().someMethod()</div><div class="line"><a class="code" href="class_ogre_1_1_mesh_manager.html#a2efefa307b7987c5fcff7e8c47f79758">Ogre::MeshManager::getSingleton</a>().someMethod()</div></div><!-- fragment --> <h1><a class="anchor" id="The-Mesh-Object"></a>
The Mesh Object</h1>
<p>A <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object represents a discrete model, a set of geometry which is self-contained and is typically fairly small on a world scale. <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are assumed to represent movable objects and are not used for the sprawling level geometry typically used to create backgrounds.</p>
<p><a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are a type of resource, and are managed by the MeshManager resource manager. They are typically loaded from OGRE’s custom object format, the ’.mesh’ format. Mesh files are typically created by exporting from a modelling tool See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> and can be manipulated through various <a class="el" href="_mesh-_tools.html">Mesh Tools</a></p>
<p>You can also create Mesh objects manually by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html#ad99f5f2e2d045e5a4af301974e45c918" title="Creates a new Mesh specifically for manual definition rather than loading from an object file...">Ogre::MeshManager::createManual</a> method. This way you can define the geometry yourself, but this is outside the scope of this manual.</p>
<p>Mesh objects are the basis for the individual movable objects in the world, which are called <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>.</p>
<p>Mesh objects can also be animated using See <a class="el" href="_animation.html#Skeletal-Animation">Skeletal Animation</a>.</p>
<h1><a class="anchor" id="Entities"></a>
Entities</h1>
<p>An entity is an instance of a movable object in the scene. It could be a car, a person, a dog, a shuriken, whatever. The only assumption is that it does not necessarily have a fixed position in the world.</p>
<p>Entities are based on discrete meshes, i.e. collections of geometry which are self-contained and typically fairly small on a world scale, which are represented by the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object. Multiple entities can be based on the same mesh, since often you want to create multiple copies of the same type of object in a scene.</p>
<p>You create an entity by calling the <a class="el" href="class_ogre_1_1_scene_manager.html#afb393cca49de8b928f7dd60838047185" title="Create an Entity (instance of a discrete mesh). ">Ogre::SceneManager::createEntity</a> method, giving it a name and specifying the name of the mesh object which it will be based on (e.g. ’muscleboundhero.mesh’). The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> will ensure that the mesh is loaded by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a> resource manager for you. Only one copy of the Mesh will be loaded.</p>
<p>Ogre::Entities are not deemed to be a part of the scene until you attach them to a <a class="el" href="class_ogre_1_1_scene_node.html" title="Class representing a node in the scene graph. ">Ogre::SceneNode</a> (see the section below). By attaching entities to SceneNodes, you can create complex hierarchical relationships between the positions and orientations of entities. You then modify the positions of the nodes to indirectly affect the entity positions.</p>
<p>When a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> is loaded, it automatically comes with a number of materials defined. It is possible to have more than one material attached to a mesh - different parts of the mesh may use different materials. Any entity created from the mesh will automatically use the default materials. However, you can change this on a per-entity basis if you like so you can create a number of entities based on the same mesh but with different textures etc.</p>
<p>To understand how this works, you have to know that all Mesh objects are actually composed of <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects, each of which represents a part of the mesh using one Material. If a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> uses only one <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a>, it will only have one <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a>.</p>
<p>When an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> is created based on this Mesh, it is composed of (possibly) multiple <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects, each matching 1 for 1 with the <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects from the original Mesh. You can access the <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects using the <a class="el" href="class_ogre_1_1_entity.html#a23fc34c5e2eff03aae4cbf800c142724" title="Gets a pointer to a SubEntity, ie a part of an Entity. ">Ogre::Entity::getSubEntity</a> method. Once you have a reference to a <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a>, you can change the material it uses by calling it’s setMaterialName method. In this way you can make an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> deviate from the default materials and thus create an individual looking version of it.</p>
<h1><a class="anchor" id="Materials"></a>
Materials</h1>
<p>The <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a> object controls how objects in the scene are rendered. It specifies what basic surface properties objects have such as reflectance of colours, shininess etc, how many texture layers are present, what images are on them and how they are blended together, what special effects are applied such as environment mapping, what culling mode is used, how the textures are filtered etc.</p>
<p>Materials can either be set up programmatically, by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a> and tweaking the settings, or by specifying it in a ’script’ which is loaded at runtime. See <a class="el" href="_material-_scripts.html">Material Scripts</a> for more info.</p>
<p>Basically everything about the appearance of an object apart from it’s shape is controlled by the Material class.</p>
<p>The SceneManager class manages the master list of materials available to the scene. The list can be added to by the application by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a>, or by loading a Mesh (which will in turn load material properties). Whenever materials are added to the SceneManager, they start off with a default set of properties; these are defined by OGRE as the following:</p>
<ul>
<li>ambient reflectance = ColourValue::White (full)</li>
<li>diffuse reflectance = ColourValue::White (full)</li>
<li>specular reflectance = ColourValue::Black (none)</li>
<li>emissive = ColourValue::Black (none)</li>
<li>shininess = 0 (not shiny)</li>
<li>No texture layers (&amp; hence no textures)</li>
<li>SourceBlendFactor = SBF_ONE, DestBlendFactor = SBF_ZERO (opaque)</li>
<li>Depth buffer checking on</li>
<li>Depth buffer writing on</li>
<li>Depth buffer comparison function = CMPF_LESS_EQUAL</li>
<li>Culling mode = CULL_CLOCKWISE</li>
<li>Ambient lighting in scene = ColourValue(0.5, 0.5, 0.5) (mid-grey)</li>
<li>Dynamic lighting enabled</li>
<li>Gourad shading mode</li>
<li>Solid polygon mode</li>
<li>Bilinear texture filtering</li>
</ul>
<p>You can alter these settings by calling <a class="el" href="class_ogre_1_1_material_manager.html#ac70230fa9f123976d7f2b1257bd94bd4" title="Returns a pointer to the default Material settings. ">Ogre::MaterialManager::getDefaultSettings()</a> and making the required changes to the Material which is returned.</p>
<p>Entities automatically have Material’s associated with them if they use a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object, since the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object typically sets up it’s required materials on loading. You can also customise the material used by an entity as described in <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>. Just create a new Material, set it up how you like (you can copy an existing material into it if you like using a standard assignment statement) and point the SubEntity entries at it using <a class="el" href="class_ogre_1_1_sub_entity.html#ab8af6685128c68a5d7863bb5238ebc0f" title="Sets the name of the Material to be used. ">Ogre::SubEntity::setMaterialName()</a>.</p>
<h1><a class="anchor" id="Overlays"></a>
Overlays</h1>
<p>Overlays allow you to render 2D and 3D elements on top of the normal scene contents to create effects like heads-up displays (HUDs), menu systems, status panels etc. The frame rate statistics panel which comes as standard with OGRE is an example of an overlay. Overlays can contain 2D or 3D elements. 2D elements are used for HUDs, and 3D elements can be used to create cockpits or any other 3D object which you wish to be rendered on top of the rest of the scene.</p>
<p>You can create overlays either through the <a class="el" href="class_ogre_1_1_overlay_manager.html#ab6eb3dcc484ba5b06904ab2fb6b1aa23" title="Create a new Overlay. ">Ogre::OverlayManager::create</a> method, or you can define them in an .overlay script. See <a class="el" href="_overlay-_scripts.html">Overlay Scripts</a> for more info. In reality the latter is likely to be the most practical because it is easier to tweak (without the need to recompile the code). Note that you can define as many overlays as you like: they all start off life hidden, and you display them by calling <a class="el" href="class_ogre_1_1_overlay.html#a3c565f3e5cf61f43fa33542d713e8ccf" title="Shows the overlay if it was hidden. ">Ogre::Overlay::show</a>. You can also show multiple overlays at once, and their Z order is determined by the <a class="el" href="class_ogre_1_1_overlay.html#a5d8481d5fa0990846e53345f1a1cc705" title="Alters the Z-order of this overlay. ">Ogre::Overlay::setZOrder()</a> method.</p>
<p><a class="anchor" id="Notes-on-Integration"></a></p>
<h2>Notes on Integration</h2>
<p>The OverlaySystem is its own component, you need to manually initialize it, with the following two lines of code (mSceneMgr is a pointer to your current <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>):</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>* pOverlaySystem = <span class="keyword">new</span> <a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>();</div><div class="line">mSceneMgr-&gt;addRenderQueueListener(pOverlaySystem);</div></div><!-- fragment --><p>One <a class="el" href="class_ogre_1_1_overlay_system.html" title="This class simplify initialization / finalization of the overlay system. ">Ogre::OverlaySystem</a> per application is enough but you need to call addRenderQueueListener once per SceneManager.</p>
<p><a class="anchor" id="Creating-2D-Elements"></a></p>
<h2>Creating 2D Elements</h2>
<p>The <a class="el" href="class_ogre_1_1_overlay_element.html" title="Abstract definition of a 2D element to be displayed in an Overlay. ">Ogre::OverlayElement</a> class abstracts the details of 2D elements which are added to overlays. All items which can be added to overlays are derived from this class. It is possible (and encouraged) for users of OGRE to define their own custom subclasses of OverlayElement in order to provide their own user controls. The key common features of all OverlayElements are things like size, position, basic material name etc. Subclasses extend this behaviour to include more complex properties and behaviour.</p>
<p>An important built-in subclass of OverlayElement is <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a>. OverlayContainer is the same as a OverlayElement, except that it can contain other OverlayElements, grouping them together (allowing them to be moved together for example) and providing them with a local coordinate origin for easier lineup.</p>
<p>The third important class is <a class="el" href="class_ogre_1_1_overlay_manager.html" title="Manages Overlay objects, parsing them from .overlay files and storing a lookup library of them...">Ogre::OverlayManager</a>. Whenever an application wishes to create a 2D element to add to an overlay (or a container), it should call <a class="el" href="class_ogre_1_1_overlay_manager.html#a58166a773d27c22437323e8ef710406b" title="Creates a new OverlayElement of the type requested. ">Ogre::OverlayManager::createOverlayElement</a>. The type of element you wish to create is identified by a string, the reason being that it allows plugins to register new types of OverlayElement for you to create without you having to link specifically to those libraries. For example, to create a panel (a plain rectangular area which can contain other OverlayElements) you would call <code>OverlayManager::getSingleton().createOverlayElement("Panel", "myNewPanel");</code>.</p>
<p><a class="anchor" id="Adding-2D-Elements-to-the-Overlay"></a></p>
<h2>Adding 2D Elements to the Overlay</h2>
<p>Only OverlayContainers can be added direct to an overlay. The reason is that each level of container establishes the Zorder of the elements contained within it, so if you nest several containers, inner containers have a higher Z-order than outer ones to ensure they are displayed correctly. To add a container (such as a Panel) to the overlay, simply call <a class="el" href="class_ogre_1_1_overlay.html#a702b15ebaa6d3f8dbb2918500b76c9f0" title="Adds a 2D &#39;container&#39; to the overlay. ">Ogre::Overlay::add2D</a>.</p>
<p>If you wish to add child elements to that container, call <a class="el" href="class_ogre_1_1_overlay_container.html#aa9eec8f6dca98ac61766fc4785b84661" title="Adds another OverlayElement to this container. ">Ogre::OverlayContainer::addChild</a>. Child elements can be Ogre::OverlayElements or <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a> instances themselves. Remember that the position of a child element is relative to the top-left corner of it’s parent.</p>
<p><a class="anchor" id="A-word-about-2D-coordinates"></a></p>
<h2>A word about 2D coordinates</h2>
<p>OGRE allows you to place and size elements based on 2 coordinate systems: <b>relative</b> and <b>pixel</b> based.</p>
<dl compact="compact">
<dt>Pixel Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want to specify an exact size for your overlay items, and you don’t mind if those items get smaller on the screen if you increase the screen resolution (in fact you might want this). In this mode the only way to put something in the middle or at the right or bottom of the screen reliably in any resolution is to use the aligning options, whilst in relative mode you can do it just by using the right relative coordinates. This mode is very simple, the top-left of the screen is (0,0) and the bottom-right of the screen depends on the resolution. As mentioned above, you can use the aligning options to make the horizontal and vertical coordinate origins the right, bottom or center of the screen if you want to place pixel items in these locations without knowing the resolution.</p>
<p class="enddd"></p>
</dd>
<dt>Relative Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want items in the overlay to be the same size on the screen no matter what the resolution. In relative mode, the top-left of the screen is (0,0) and the bottom-right is (1,1). So if you place an element at (0.5, 0.5), it’s top-left corner is placed exactly in the center of the screen, no matter what resolution the application is running in. The same principle applies to sizes; if you set the width of an element to 0.5, it covers half the width of the screen. Note that because the aspect ratio of the screen is typically 1.3333 : 1 (width : height), an element with dimensions (0.25, 0.25) will not be square, but it will take up exactly 1/16th of the screen in area terms. If you want square-looking areas you will have to compensate using the typical aspect ratio e.g. use (0.1875, 0.25) instead.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Transforming-Overlays"></a> </p><h2>Transforming Overlays</h2>
<p>Another nice feature of overlays is being able to rotate, scroll and scale them as a whole. You can use this for zooming in / out menu systems, dropping them in from off screen and other nice effects. See the <a class="el" href="class_ogre_1_1_overlay.html#ae07b1a57498aad0299f0781b09ca9b39" title="Scrolls the overlay by the offsets provided. ">Ogre::Overlay::scroll</a>, <a class="el" href="class_ogre_1_1_overlay.html#a8282553a59253170ddd0488ab7179792" title="Adds the passed in angle to the rotation applied to this overlay. ">Ogre::Overlay::rotate</a> and <a class="el" href="class_ogre_1_1_overlay.html#adea3b88baa3697a657e7b5785f0ffedb" title="Sets the scaling factor of this overlay. ">Ogre::Overlay::setScale</a> methods for more information.</p>
<p><a class="anchor" id="GUI-systems"></a></p>
<h2>GUI systems</h2>
<p>Overlays are only really designed for non-interactive screen elements, although you can create a simple GUI using the <a class="el" href="trays.html">Trays System</a>. For a far more complete GUI solution, we recommend or <a href="https://github.com/OGRECave/ogre-imgui">Dear ImGui</a>, <a href="http://www.cegui.org.uk">CEGui</a> or <a href="http://mygui.info/">MyGUI</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: The Core Objects</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_the-_core-_objects.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">The Core Objects </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#The-Root-Object">The Root object</a></li>
<li class="level1"><a href="#The-RenderSystem-object">The RenderSystem object</a></li>
<li class="level1"><a href="#The-SceneManager-object">The SceneManager object</a></li>
<li class="level1"><a href="#The-ResourceGroupManager-Object">The ResourceGroupManager Object</a></li>
<li class="level1"><a href="#The-Mesh-Object">The Mesh Object</a></li>
<li class="level1"><a href="#Entities">Entities</a></li>
<li class="level1"><a href="#Materials">Materials</a></li>
<li class="level1"><a href="#Overlays">Overlays</a></li>
</ul>
</div>
<div class="textblock"><p>This tutorial gives you a quick summary of the core objects that you will use in OGRE and what they are used for.</p>
<p><a class="anchor" id="A-Word-About-Namespaces"></a></p>
<h1>A Word About Namespaces</h1>
<p>OGRE uses a C++ feature called namespaces. This lets you put classes, enums, structures, anything really within a ’namespace’ scope which is an easy way to prevent name clashes, i.e. situations where you have 2 things called the same thing. Since OGRE is designed to be used inside other applications, I wanted to be sure that name clashes would not be a problem. Some people prefix their classes/types with a short code because some compilers don’t support namespaces, but I chose to use them because they are the ’right’ way to do it. Sorry if you have a non-compliant compiler, but hey, the C++ standard has been defined for years, so compiler writers really have no excuse anymore. If your compiler doesn’t support namespaces then it’s probably because it’s sh*t - get a better one. ;)</p>
<p>This means every class, type etc should be prefixed with <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, e.g. <a class="el" href="class_ogre_1_1_camera.html" title="A viewpoint from which the scene will be rendered. ">Ogre::Camera</a>, <a class="el" href="class_ogre_1_1_vector3.html" title="Standard 3-dimensional vector. ">Ogre::Vector3</a> etc which means if elsewhere in your application you have used a Vector3 type you won’t get name clashes. To avoid lots of extra typing you can add a ’using namespace <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>;’ statement to your code which means you don’t have to type the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> prefix unless there is ambiguity (in the situation where you have another definition with the same name).</p>
<p><a class="anchor" id="Overview-from-10_002c000-feet"></a></p>
<h1>Overview from 10,000 feet</h1>
<p>Shown below is a diagram of some of the core objects and where they ’sit’ in the grand scheme of things. This is not all the classes by a long shot, just a few examples of the more more significant ones to give you an idea of how it slots together. </p><div class="image">
<object type="image/svg+xml" data="uml-overview.svg">uml-overview.svg</object>
</div>
<p>At the very top of the diagram is the Root object. This is your ’way in’ to the OGRE system, and it’s where you tend to create the top-level objects that you need to deal with, like scene managers, rendering systems and render windows, loading plugins, all the fundamental stuff. If you don’t know where to start, Root is it for almost everything, although often it will just give you another object which will actually do the detail work, since Root itself is more of an organiser and facilitator object.</p>
<p>The majority of rest of OGRE’s classes fall into one of 3 roles:</p>
<dl compact="compact">
<dt>Scene Management </dt>
<dd><p class="startdd"></p>
<p>This is about the contents of your scene, how it’s structured, how it’s viewed from cameras, etc. Objects in this area are responsible for giving you a natural declarative interface to the world you’re building; i.e. you don’t tell OGRE "set these render states and then render 3 polygons", you tell it "I want an object here, here and here, with these materials on them, rendered from this view", and let it get on with it.</p>
<p class="enddd"></p>
</dd>
<dt>Resource Management </dt>
<dd><p class="startdd"></p>
<p>All rendering needs resources, whether it’s geometry, textures, fonts, whatever. It’s important to manage the loading, re-use and unloading of these things carefully, so that’s what classes in this area do.</p>
<p class="enddd"></p>
</dd>
<dt>Rendering </dt>
<dd><p class="startdd"></p>
<p>Finally, there’s getting the visuals on the screen - this is about the lower-level end of the rendering pipeline, the specific rendering system API objects like buffers, render states and the like and pushing it all down the pipeline. Classes in the Scene Management subsystem use this to get their higher-level scene information onto the screen.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You’ll notice that scattered around the edge are a number of plugins. OGRE is designed to be extended, and plugins are the usual way to go about it. Many of the classes in OGRE can be subclassed and extended, whether it’s changing the scene organisation through a custom SceneManager, adding a new render system implementation (e.g. Direct3D or OpenGL), or providing a way to load resources from another source (say from a web location or a database). Again this is just a small smattering of the kinds of things plugins can do, but as you can see they can plug in to almost any aspect of the system. This way, OGRE isn’t just a solution for one narrowly defined problem, it can extend to pretty much anything you need it to do.</p>
<h1><a class="anchor" id="The-Root-Object"></a>
The Root object</h1>
<p>The <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object is the entry point to the OGRE system. This object MUST be the first one to be created, and the last one to be destroyed. In the example applications I chose to make an instance of Root a member of my application object which ensured that it was created as soon as my application object was, and deleted when the application object was deleted.</p>
<p>The root object lets you configure the system, for example through the <a class="el" href="class_ogre_1_1_root.html#aedc76ef3bdc6514bd4655be0d70d6bcd" title="Displays a dialog asking the user to choose system settings. ">Ogre::Root::showConfigDialog</a> method which is an extremely handy method which performs all render system options detection and shows a dialog for the user to customise resolution, colour depth, full screen options etc. It also sets the options the user selects so that you can initialise the system directly afterwards.</p>
<p>The root object is also your method for obtaining pointers to other objects in the system, such as the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> and various other resource managers. See below for details.</p>
<p>Finally, if you run OGRE in continuous rendering mode, i.e. you want to always refresh all the rendering targets as fast as possible (the norm for games and demos, but not for windowed utilities), the root object has a method called <a class="el" href="class_ogre_1_1_root.html#a8eda253befda1255cbfd3b298f62449e" title="Starts / restarts the automatic rendering cycle. ">Ogre::Root::startRendering</a>, which when called will enter a continuous rendering loop which will only end when all rendering windows are closed, or any <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects indicate that they want to stop the cycle (see below for details of <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects).</p>
<h1><a class="anchor" id="The-RenderSystem-object"></a>
The RenderSystem object</h1>
<p>The <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object is actually an abstract class which defines the interface to the underlying 3D API. It is responsible for sending rendering operations to the API and setting all the various rendering options. This class is abstract because all the implementation is rendering API specific - there are API-specific subclasses for each rendering API (e.g. D3DRenderSystem for Direct3D). After the system has been initialised through <a class="el" href="class_ogre_1_1_root.html#a6ab8f204bbfb150ad180eb6e78e4639c" title="Initialises the renderer. ">Ogre::Root::initialise</a>, the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object for the selected rendering API is available via the <a class="el" href="class_ogre_1_1_root.html#ab8ca3d5abebba7a14650db5f618286d0" title="Retrieve a pointer to the currently selected render system. ">Ogre::Root::getRenderSystem()</a> method.</p>
<p>However, a typical application should not normally need to manipulate the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object directly - everything you need for rendering objects and customising settings should be available on the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, Material and other scene-oriented classes. It’s only if you want to create multiple rendering windows (completely separate windows in this case, not multiple viewports like a split-screen effect which is done via the RenderWindow class) or access other advanced features that you need access to the RenderSystem object.</p>
<p>For this reason I will not discuss the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object further in these tutorials. You can assume the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> handles the calls to the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> at the appropriate times.</p>
<h1><a class="anchor" id="The-SceneManager-object"></a>
The SceneManager object</h1>
<p>Apart from the <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object, this is probably the most critical part of the system from the application’s point of view. Certainly it will be the object which is most used by the application. The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> is in charge of the contents of the scene which is to be rendered by the engine. It is responsible for organising the contents using whatever technique it deems best, for creating and managing all the cameras, movable objects (entities), lights and materials (surface properties of objects), and for managing the ’world geometry’ which is the sprawling static geometry usually used to represent the immovable parts of a scene.</p>
<p>It is to the SceneManager that you go when you want to create a camera for the scene. It’s also where you go to retrieve or to remove a light from the scene. There is no need for your application to keep lists of objects, the SceneManager keeps a named set of all of the scene objects for you to access, should you need them. Look in the main documentation under the getCamera, getLight, getEntity etc methods.</p>
<p>The SceneManager also sends the scene to the RenderSystem object when it is time to render the scene. You never have to call the <a class="el" href="class_ogre_1_1_scene_manager.html#a29ba8a2cad151d307d04781f749e7e84" title="Prompts the class to send its contents to the renderer. ">Ogre::SceneManager::_renderScene</a> method directly though - it is called automatically whenever a rendering target is asked to update.</p>
<p>So most of your interaction with the SceneManager is during scene setup. You’re likely to call a great number of methods (perhaps driven by some input file containing the scene data) in order to set up your scene. You can also modify the contents of the scene dynamically during the rendering cycle if you create your own FrameListener object (see later).</p>
<p>Because different scene types require very different algorithmic approaches to deciding which objects get sent to the RenderSystem in order to attain good rendering performance, the SceneManager class is designed to be subclassed for different scene types. The default SceneManager object will render a scene, but it does little or no scene organisation and you should not expect the results to be high performance in the case of large scenes. The intention is that specialisations will be created for each type of scene such that under the surface the subclass will optimise the scene organisation for best performance given assumptions which can be made for that scene type. An example is the BspSceneManager which optimises rendering for large indoor levels based on a Binary Space Partition (BSP) tree.</p>
<p>The application using OGRE does not have to know which subclasses are available. The application simply calls <a class="el" href="class_ogre_1_1_root.html#a8bb8655f823a98007775f69278c1b0f9" title="create a default scene manager ">Ogre::Root::createSceneManager</a>(..) passing as a parameter one of a number of scene types (e.g. <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a0af8f844019e52fe112e9a946a919fcd">Ogre::ST_GENERIC</a>, <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a93f2170ad3a8132f29264dc3642a2dcf">Ogre::ST_INTERIOR</a> etc). OGRE will automatically use the best SceneManager subclass available for that scene type, or default to the basic SceneManager if a specialist one is not available. This allows the developers of OGRE to add new scene specialisations later and thus optimise previously unoptimised scene types without the user applications having to change any code.</p>
<h1><a class="anchor" id="The-ResourceGroupManager-Object"></a>
The ResourceGroupManager Object</h1>
<p>The <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a> class is actually a ’hub’ for loading of reusable resources like textures and meshes. It is the place that you define groups for your resources, so they may be unloaded and reloaded when you want. Servicing it are a number of ResourceManagers which manage the individual types of resource, like <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> or <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a>. In this context, resources are sets of data which must be loaded from somewhere to provide OGRE with the data it needs.</p>
<p>ResourceManagers ensure that resources are only loaded once and shared throughout the OGRE engine. They also manage the memory requirements of the resources they look after. They can also search in a number of locations for the resources they need, including multiple search paths and compressed archives (ZIP files).</p>
<p>Most of the time you won’t interact with resource managers directly. Resource managers will be called by other parts of the OGRE system as required, for example when you request for a texture to be added to a Material, the <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> will be called for you. If you like, you can call the appropriate resource manager directly to preload resources (if for example you want to prevent disk access later on) but most of the time it’s ok to let OGRE decide when to do it.</p>
<p>One thing you will want to do is to tell the resource managers where to look for resources. You do this via Root::getSingleton().addResourceLocation, which actually passes the information on to <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a>.</p>
<p>Because there is only ever 1 instance of each resource manager in the engine, if you do want to get a reference to a resource manager use the following syntax:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().someMethod()</div><div class="line"><a class="code" href="class_ogre_1_1_mesh_manager.html#a2efefa307b7987c5fcff7e8c47f79758">Ogre::MeshManager::getSingleton</a>().someMethod()</div></div><!-- fragment --> <h1><a class="anchor" id="The-Mesh-Object"></a>
The Mesh Object</h1>
<p>A <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object represents a discrete model, a set of geometry which is self-contained and is typically fairly small on a world scale. <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are assumed to represent movable objects and are not used for the sprawling level geometry typically used to create backgrounds.</p>
<p><a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are a type of resource, and are managed by the MeshManager resource manager. They are typically loaded from OGRE’s custom object format, the ’.mesh’ format. Mesh files are typically created by exporting from a modelling tool See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> and can be manipulated through various <a class="el" href="_mesh-_tools.html">Mesh Tools</a></p>
<p>You can also create Mesh objects manually by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html#ad99f5f2e2d045e5a4af301974e45c918" title="Creates a new Mesh specifically for manual definition rather than loading from an object file...">Ogre::MeshManager::createManual</a> method. This way you can define the geometry yourself, but this is outside the scope of this manual.</p>
<p>Mesh objects are the basis for the individual movable objects in the world, which are called <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>.</p>
<p>Mesh objects can also be animated using See <a class="el" href="_animation.html#Skeletal-Animation">Skeletal Animation</a>.</p>
<h1><a class="anchor" id="Entities"></a>
Entities</h1>
<p>An entity is an instance of a movable object in the scene. It could be a car, a person, a dog, a shuriken, whatever. The only assumption is that it does not necessarily have a fixed position in the world.</p>
<p>Entities are based on discrete meshes, i.e. collections of geometry which are self-contained and typically fairly small on a world scale, which are represented by the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object. Multiple entities can be based on the same mesh, since often you want to create multiple copies of the same type of object in a scene.</p>
<p>You create an entity by calling the <a class="el" href="class_ogre_1_1_scene_manager.html#afb393cca49de8b928f7dd60838047185" title="Create an Entity (instance of a discrete mesh). ">Ogre::SceneManager::createEntity</a> method, giving it a name and specifying the name of the mesh object which it will be based on (e.g. ’muscleboundhero.mesh’). The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> will ensure that the mesh is loaded by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a> resource manager for you. Only one copy of the Mesh will be loaded.</p>
<p>Ogre::Entities are not deemed to be a part of the scene until you attach them to a <a class="el" href="class_ogre_1_1_scene_node.html" title="Class representing a node in the scene graph. ">Ogre::SceneNode</a> (see the section below). By attaching entities to SceneNodes, you can create complex hierarchical relationships between the positions and orientations of entities. You then modify the positions of the nodes to indirectly affect the entity positions.</p>
<p>When a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> is loaded, it automatically comes with a number of materials defined. It is possible to have more than one material attached to a mesh - different parts of the mesh may use different materials. Any entity created from the mesh will automatically use the default materials. However, you can change this on a per-entity basis if you like so you can create a number of entities based on the same mesh but with different textures etc.</p>
<p>To understand how this works, you have to know that all Mesh objects are actually composed of <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects, each of which represents a part of the mesh using one Material. If a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> uses only one <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a>, it will only have one <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a>.</p>
<p>When an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> is created based on this Mesh, it is composed of (possibly) multiple <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects, each matching 1 for 1 with the <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects from the original Mesh. You can access the <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects using the <a class="el" href="class_ogre_1_1_entity.html#a23fc34c5e2eff03aae4cbf800c142724" title="Gets a pointer to a SubEntity, ie a part of an Entity. ">Ogre::Entity::getSubEntity</a> method. Once you have a reference to a <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a>, you can change the material it uses by calling it’s setMaterialName method. In this way you can make an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> deviate from the default materials and thus create an individual looking version of it.</p>
<h1><a class="anchor" id="Materials"></a>
Materials</h1>
<p>The <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a> object controls how objects in the scene are rendered. It specifies what basic surface properties objects have such as reflectance of colours, shininess etc, how many texture layers are present, what images are on them and how they are blended together, what special effects are applied such as environment mapping, what culling mode is used, how the textures are filtered etc.</p>
<p>Materials can either be set up programmatically, by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a> and tweaking the settings, or by specifying it in a ’script’ which is loaded at runtime. See <a class="el" href="_material-_scripts.html">Material Scripts</a> for more info.</p>
<p>Basically everything about the appearance of an object apart from it’s shape is controlled by the Material class.</p>
<p>The SceneManager class manages the master list of materials available to the scene. The list can be added to by the application by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a>, or by loading a Mesh (which will in turn load material properties). Whenever materials are added to the SceneManager, they start off with a default set of properties; these are defined by OGRE as the following:</p>
<ul>
<li>ambient reflectance = ColourValue::White (full)</li>
<li>diffuse reflectance = ColourValue::White (full)</li>
<li>specular reflectance = ColourValue::Black (none)</li>
<li>emissive = ColourValue::Black (none)</li>
<li>shininess = 0 (not shiny)</li>
<li>No texture layers (&amp; hence no textures)</li>
<li>SourceBlendFactor = SBF_ONE, DestBlendFactor = SBF_ZERO (opaque)</li>
<li>Depth buffer checking on</li>
<li>Depth buffer writing on</li>
<li>Depth buffer comparison function = CMPF_LESS_EQUAL</li>
<li>Culling mode = CULL_CLOCKWISE</li>
<li>Ambient lighting in scene = ColourValue(0.5, 0.5, 0.5) (mid-grey)</li>
<li>Dynamic lighting enabled</li>
<li>Gourad shading mode</li>
<li>Solid polygon mode</li>
<li>Bilinear texture filtering</li>
</ul>
<p>You can alter these settings by calling <a class="el" href="class_ogre_1_1_material_manager.html#ac70230fa9f123976d7f2b1257bd94bd4" title="Returns a pointer to the default Material settings. ">Ogre::MaterialManager::getDefaultSettings()</a> and making the required changes to the Material which is returned.</p>
<p>Entities automatically have Material’s associated with them if they use a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object, since the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object typically sets up it’s required materials on loading. You can also customise the material used by an entity as described in <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>. Just create a new Material, set it up how you like (you can copy an existing material into it if you like using a standard assignment statement) and point the SubEntity entries at it using <a class="el" href="class_ogre_1_1_sub_entity.html#ab8af6685128c68a5d7863bb5238ebc0f" title="Sets the name of the Material to be used. ">Ogre::SubEntity::setMaterialName()</a>.</p>
<h1><a class="anchor" id="Overlays"></a>
Overlays</h1>
<p>Overlays allow you to render 2D and 3D elements on top of the normal scene contents to create effects like heads-up displays (HUDs), menu systems, status panels etc. The frame rate statistics panel which comes as standard with OGRE is an example of an overlay. Overlays can contain 2D or 3D elements. 2D elements are used for HUDs, and 3D elements can be used to create cockpits or any other 3D object which you wish to be rendered on top of the rest of the scene.</p>
<p>You can create overlays either through the <a class="el" href="class_ogre_1_1_overlay_manager.html#ab6eb3dcc484ba5b06904ab2fb6b1aa23" title="Create a new Overlay. ">Ogre::OverlayManager::create</a> method, or you can define them in an .overlay script. See <a class="el" href="_overlay-_scripts.html">Overlay Scripts</a> for more info. In reality the latter is likely to be the most practical because it is easier to tweak (without the need to recompile the code). Note that you can define as many overlays as you like: they all start off life hidden, and you display them by calling <a class="el" href="class_ogre_1_1_overlay.html#a3c565f3e5cf61f43fa33542d713e8ccf" title="Shows the overlay if it was hidden. ">Ogre::Overlay::show</a>. You can also show multiple overlays at once, and their Z order is determined by the <a class="el" href="class_ogre_1_1_overlay.html#a5d8481d5fa0990846e53345f1a1cc705" title="Alters the Z-order of this overlay. ">Ogre::Overlay::setZOrder()</a> method.</p>
<p><a class="anchor" id="Notes-on-Integration"></a></p>
<h2>Notes on Integration</h2>
<p>The OverlaySystem is its own component, you need to manually initialize it, with the following two lines of code (mSceneMgr is a pointer to your current <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>):</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>* pOverlaySystem = <span class="keyword">new</span> <a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>();</div><div class="line">mSceneMgr-&gt;addRenderQueueListener(pOverlaySystem);</div></div><!-- fragment --><p>One <a class="el" href="class_ogre_1_1_overlay_system.html" title="This class simplify initialization / finalization of the overlay system. ">Ogre::OverlaySystem</a> per application is enough but you need to call addRenderQueueListener once per SceneManager.</p>
<p><a class="anchor" id="Creating-2D-Elements"></a></p>
<h2>Creating 2D Elements</h2>
<p>The <a class="el" href="class_ogre_1_1_overlay_element.html" title="Abstract definition of a 2D element to be displayed in an Overlay. ">Ogre::OverlayElement</a> class abstracts the details of 2D elements which are added to overlays. All items which can be added to overlays are derived from this class. It is possible (and encouraged) for users of OGRE to define their own custom subclasses of OverlayElement in order to provide their own user controls. The key common features of all OverlayElements are things like size, position, basic material name etc. Subclasses extend this behaviour to include more complex properties and behaviour.</p>
<p>An important built-in subclass of OverlayElement is <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a>. OverlayContainer is the same as a OverlayElement, except that it can contain other OverlayElements, grouping them together (allowing them to be moved together for example) and providing them with a local coordinate origin for easier lineup.</p>
<p>The third important class is <a class="el" href="class_ogre_1_1_overlay_manager.html" title="Manages Overlay objects, parsing them from .overlay files and storing a lookup library of them...">Ogre::OverlayManager</a>. Whenever an application wishes to create a 2D element to add to an overlay (or a container), it should call <a class="el" href="class_ogre_1_1_overlay_manager.html#a58166a773d27c22437323e8ef710406b" title="Creates a new OverlayElement of the type requested. ">Ogre::OverlayManager::createOverlayElement</a>. The type of element you wish to create is identified by a string, the reason being that it allows plugins to register new types of OverlayElement for you to create without you having to link specifically to those libraries. For example, to create a panel (a plain rectangular area which can contain other OverlayElements) you would call <code>OverlayManager::getSingleton().createOverlayElement("Panel", "myNewPanel");</code>.</p>
<p><a class="anchor" id="Adding-2D-Elements-to-the-Overlay"></a></p>
<h2>Adding 2D Elements to the Overlay</h2>
<p>Only OverlayContainers can be added direct to an overlay. The reason is that each level of container establishes the Zorder of the elements contained within it, so if you nest several containers, inner containers have a higher Z-order than outer ones to ensure they are displayed correctly. To add a container (such as a Panel) to the overlay, simply call <a class="el" href="class_ogre_1_1_overlay.html#a702b15ebaa6d3f8dbb2918500b76c9f0" title="Adds a 2D &#39;container&#39; to the overlay. ">Ogre::Overlay::add2D</a>.</p>
<p>If you wish to add child elements to that container, call <a class="el" href="class_ogre_1_1_overlay_container.html#aa9eec8f6dca98ac61766fc4785b84661" title="Adds another OverlayElement to this container. ">Ogre::OverlayContainer::addChild</a>. Child elements can be Ogre::OverlayElements or <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a> instances themselves. Remember that the position of a child element is relative to the top-left corner of it’s parent.</p>
<p><a class="anchor" id="A-word-about-2D-coordinates"></a></p>
<h2>A word about 2D coordinates</h2>
<p>OGRE allows you to place and size elements based on 2 coordinate systems: <b>relative</b> and <b>pixel</b> based.</p>
<dl compact="compact">
<dt>Pixel Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want to specify an exact size for your overlay items, and you don’t mind if those items get smaller on the screen if you increase the screen resolution (in fact you might want this). In this mode the only way to put something in the middle or at the right or bottom of the screen reliably in any resolution is to use the aligning options, whilst in relative mode you can do it just by using the right relative coordinates. This mode is very simple, the top-left of the screen is (0,0) and the bottom-right of the screen depends on the resolution. As mentioned above, you can use the aligning options to make the horizontal and vertical coordinate origins the right, bottom or center of the screen if you want to place pixel items in these locations without knowing the resolution.</p>
<p class="enddd"></p>
</dd>
<dt>Relative Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want items in the overlay to be the same size on the screen no matter what the resolution. In relative mode, the top-left of the screen is (0,0) and the bottom-right is (1,1). So if you place an element at (0.5, 0.5), it’s top-left corner is placed exactly in the center of the screen, no matter what resolution the application is running in. The same principle applies to sizes; if you set the width of an element to 0.5, it covers half the width of the screen. Note that because the aspect ratio of the screen is typically 1.3333 : 1 (width : height), an element with dimensions (0.25, 0.25) will not be square, but it will take up exactly 1/16th of the screen in area terms. If you want square-looking areas you will have to compensate using the typical aspect ratio e.g. use (0.1875, 0.25) instead.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Transforming-Overlays"></a> </p><h2>Transforming Overlays</h2>
<p>Another nice feature of overlays is being able to rotate, scroll and scale them as a whole. You can use this for zooming in / out menu systems, dropping them in from off screen and other nice effects. See the <a class="el" href="class_ogre_1_1_overlay.html#ae07b1a57498aad0299f0781b09ca9b39" title="Scrolls the overlay by the offsets provided. ">Ogre::Overlay::scroll</a>, <a class="el" href="class_ogre_1_1_overlay.html#a8282553a59253170ddd0488ab7179792" title="Adds the passed in angle to the rotation applied to this overlay. ">Ogre::Overlay::rotate</a> and <a class="el" href="class_ogre_1_1_overlay.html#adea3b88baa3697a657e7b5785f0ffedb" title="Sets the scaling factor of this overlay. ">Ogre::Overlay::setScale</a> methods for more information.</p>
<p><a class="anchor" id="GUI-systems"></a></p>
<h2>GUI systems</h2>
<p>Overlays are only really designed for non-interactive screen elements, although you can create a simple GUI using the <a class="el" href="trays.html">Trays System</a>. For a far more complete GUI solution, we recommend or <a href="https://github.com/OGRECave/ogre-imgui">Dear ImGui</a>, <a href="http://www.cegui.org.uk">CEGui</a> or <a href="http://mygui.info/">MyGUI</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: The Core Objects</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_the-_core-_objects.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">The Core Objects </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#The-Root-Object">The Root object</a></li>
<li class="level1"><a href="#The-RenderSystem-object">The RenderSystem object</a></li>
<li class="level1"><a href="#The-SceneManager-object">The SceneManager object</a></li>
<li class="level1"><a href="#The-ResourceGroupManager-Object">The ResourceGroupManager Object</a></li>
<li class="level1"><a href="#The-Mesh-Object">The Mesh Object</a></li>
<li class="level1"><a href="#Entities">Entities</a></li>
<li class="level1"><a href="#Materials">Materials</a></li>
<li class="level1"><a href="#Overlays">Overlays</a></li>
</ul>
</div>
<div class="textblock"><p>This tutorial gives you a quick summary of the core objects that you will use in OGRE and what they are used for.</p>
<p><a class="anchor" id="A-Word-About-Namespaces"></a></p>
<h1>A Word About Namespaces</h1>
<p>OGRE uses a C++ feature called namespaces. This lets you put classes, enums, structures, anything really within a ’namespace’ scope which is an easy way to prevent name clashes, i.e. situations where you have 2 things called the same thing. Since OGRE is designed to be used inside other applications, I wanted to be sure that name clashes would not be a problem. Some people prefix their classes/types with a short code because some compilers don’t support namespaces, but I chose to use them because they are the ’right’ way to do it. Sorry if you have a non-compliant compiler, but hey, the C++ standard has been defined for years, so compiler writers really have no excuse anymore. If your compiler doesn’t support namespaces then it’s probably because it’s sh*t - get a better one. ;)</p>
<p>This means every class, type etc should be prefixed with <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, e.g. <a class="el" href="class_ogre_1_1_camera.html" title="A viewpoint from which the scene will be rendered. ">Ogre::Camera</a>, <a class="el" href="class_ogre_1_1_vector3.html" title="Standard 3-dimensional vector. ">Ogre::Vector3</a> etc which means if elsewhere in your application you have used a Vector3 type you won’t get name clashes. To avoid lots of extra typing you can add a ’using namespace <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>;’ statement to your code which means you don’t have to type the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> prefix unless there is ambiguity (in the situation where you have another definition with the same name).</p>
<p><a class="anchor" id="Overview-from-10_002c000-feet"></a></p>
<h1>Overview from 10,000 feet</h1>
<p>Shown below is a diagram of some of the core objects and where they ’sit’ in the grand scheme of things. This is not all the classes by a long shot, just a few examples of the more more significant ones to give you an idea of how it slots together. </p><div class="image">
<object type="image/svg+xml" data="uml-overview.svg">uml-overview.svg</object>
</div>
<p>At the very top of the diagram is the Root object. This is your ’way in’ to the OGRE system, and it’s where you tend to create the top-level objects that you need to deal with, like scene managers, rendering systems and render windows, loading plugins, all the fundamental stuff. If you don’t know where to start, Root is it for almost everything, although often it will just give you another object which will actually do the detail work, since Root itself is more of an organiser and facilitator object.</p>
<p>The majority of rest of OGRE’s classes fall into one of 3 roles:</p>
<dl compact="compact">
<dt>Scene Management </dt>
<dd><p class="startdd"></p>
<p>This is about the contents of your scene, how it’s structured, how it’s viewed from cameras, etc. Objects in this area are responsible for giving you a natural declarative interface to the world you’re building; i.e. you don’t tell OGRE "set these render states and then render 3 polygons", you tell it "I want an object here, here and here, with these materials on them, rendered from this view", and let it get on with it.</p>
<p class="enddd"></p>
</dd>
<dt>Resource Management </dt>
<dd><p class="startdd"></p>
<p>All rendering needs resources, whether it’s geometry, textures, fonts, whatever. It’s important to manage the loading, re-use and unloading of these things carefully, so that’s what classes in this area do.</p>
<p class="enddd"></p>
</dd>
<dt>Rendering </dt>
<dd><p class="startdd"></p>
<p>Finally, there’s getting the visuals on the screen - this is about the lower-level end of the rendering pipeline, the specific rendering system API objects like buffers, render states and the like and pushing it all down the pipeline. Classes in the Scene Management subsystem use this to get their higher-level scene information onto the screen.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You’ll notice that scattered around the edge are a number of plugins. OGRE is designed to be extended, and plugins are the usual way to go about it. Many of the classes in OGRE can be subclassed and extended, whether it’s changing the scene organisation through a custom SceneManager, adding a new render system implementation (e.g. Direct3D or OpenGL), or providing a way to load resources from another source (say from a web location or a database). Again this is just a small smattering of the kinds of things plugins can do, but as you can see they can plug in to almost any aspect of the system. This way, OGRE isn’t just a solution for one narrowly defined problem, it can extend to pretty much anything you need it to do.</p>
<h1><a class="anchor" id="The-Root-Object"></a>
The Root object</h1>
<p>The <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object is the entry point to the OGRE system. This object MUST be the first one to be created, and the last one to be destroyed. In the example applications I chose to make an instance of Root a member of my application object which ensured that it was created as soon as my application object was, and deleted when the application object was deleted.</p>
<p>The root object lets you configure the system, for example through the <a class="el" href="class_ogre_1_1_root.html#aedc76ef3bdc6514bd4655be0d70d6bcd" title="Displays a dialog asking the user to choose system settings. ">Ogre::Root::showConfigDialog</a> method which is an extremely handy method which performs all render system options detection and shows a dialog for the user to customise resolution, colour depth, full screen options etc. It also sets the options the user selects so that you can initialise the system directly afterwards.</p>
<p>The root object is also your method for obtaining pointers to other objects in the system, such as the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> and various other resource managers. See below for details.</p>
<p>Finally, if you run OGRE in continuous rendering mode, i.e. you want to always refresh all the rendering targets as fast as possible (the norm for games and demos, but not for windowed utilities), the root object has a method called <a class="el" href="class_ogre_1_1_root.html#a8eda253befda1255cbfd3b298f62449e" title="Starts / restarts the automatic rendering cycle. ">Ogre::Root::startRendering</a>, which when called will enter a continuous rendering loop which will only end when all rendering windows are closed, or any <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects indicate that they want to stop the cycle (see below for details of <a class="el" href="class_ogre_1_1_frame_listener.html" title="A interface class defining a listener which can be used to receive notifications of frame events...">Ogre::FrameListener</a> objects).</p>
<h1><a class="anchor" id="The-RenderSystem-object"></a>
The RenderSystem object</h1>
<p>The <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object is actually an abstract class which defines the interface to the underlying 3D API. It is responsible for sending rendering operations to the API and setting all the various rendering options. This class is abstract because all the implementation is rendering API specific - there are API-specific subclasses for each rendering API (e.g. D3DRenderSystem for Direct3D). After the system has been initialised through <a class="el" href="class_ogre_1_1_root.html#a6ab8f204bbfb150ad180eb6e78e4639c" title="Initialises the renderer. ">Ogre::Root::initialise</a>, the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object for the selected rendering API is available via the <a class="el" href="class_ogre_1_1_root.html#ab8ca3d5abebba7a14650db5f618286d0" title="Retrieve a pointer to the currently selected render system. ">Ogre::Root::getRenderSystem()</a> method.</p>
<p>However, a typical application should not normally need to manipulate the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object directly - everything you need for rendering objects and customising settings should be available on the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>, Material and other scene-oriented classes. It’s only if you want to create multiple rendering windows (completely separate windows in this case, not multiple viewports like a split-screen effect which is done via the RenderWindow class) or access other advanced features that you need access to the RenderSystem object.</p>
<p>For this reason I will not discuss the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> object further in these tutorials. You can assume the <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> handles the calls to the <a class="el" href="class_ogre_1_1_render_system.html" title="Defines the functionality of a 3D API. ">Ogre::RenderSystem</a> at the appropriate times.</p>
<h1><a class="anchor" id="The-SceneManager-object"></a>
The SceneManager object</h1>
<p>Apart from the <a class="el" href="class_ogre_1_1_root.html" title="The root class of the Ogre system. ">Ogre::Root</a> object, this is probably the most critical part of the system from the application’s point of view. Certainly it will be the object which is most used by the application. The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> is in charge of the contents of the scene which is to be rendered by the engine. It is responsible for organising the contents using whatever technique it deems best, for creating and managing all the cameras, movable objects (entities), lights and materials (surface properties of objects), and for managing the ’world geometry’ which is the sprawling static geometry usually used to represent the immovable parts of a scene.</p>
<p>It is to the SceneManager that you go when you want to create a camera for the scene. It’s also where you go to retrieve or to remove a light from the scene. There is no need for your application to keep lists of objects, the SceneManager keeps a named set of all of the scene objects for you to access, should you need them. Look in the main documentation under the getCamera, getLight, getEntity etc methods.</p>
<p>The SceneManager also sends the scene to the RenderSystem object when it is time to render the scene. You never have to call the <a class="el" href="class_ogre_1_1_scene_manager.html#a29ba8a2cad151d307d04781f749e7e84" title="Prompts the class to send its contents to the renderer. ">Ogre::SceneManager::_renderScene</a> method directly though - it is called automatically whenever a rendering target is asked to update.</p>
<p>So most of your interaction with the SceneManager is during scene setup. You’re likely to call a great number of methods (perhaps driven by some input file containing the scene data) in order to set up your scene. You can also modify the contents of the scene dynamically during the rendering cycle if you create your own FrameListener object (see later).</p>
<p>Because different scene types require very different algorithmic approaches to deciding which objects get sent to the RenderSystem in order to attain good rendering performance, the SceneManager class is designed to be subclassed for different scene types. The default SceneManager object will render a scene, but it does little or no scene organisation and you should not expect the results to be high performance in the case of large scenes. The intention is that specialisations will be created for each type of scene such that under the surface the subclass will optimise the scene organisation for best performance given assumptions which can be made for that scene type. An example is the BspSceneManager which optimises rendering for large indoor levels based on a Binary Space Partition (BSP) tree.</p>
<p>The application using OGRE does not have to know which subclasses are available. The application simply calls <a class="el" href="class_ogre_1_1_root.html#a8bb8655f823a98007775f69278c1b0f9" title="create a default scene manager ">Ogre::Root::createSceneManager</a>(..) passing as a parameter one of a number of scene types (e.g. <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a0af8f844019e52fe112e9a946a919fcd">Ogre::ST_GENERIC</a>, <a class="el" href="group___scene.html#gga7383602bd480d43b80c626969b9af914a93f2170ad3a8132f29264dc3642a2dcf">Ogre::ST_INTERIOR</a> etc). OGRE will automatically use the best SceneManager subclass available for that scene type, or default to the basic SceneManager if a specialist one is not available. This allows the developers of OGRE to add new scene specialisations later and thus optimise previously unoptimised scene types without the user applications having to change any code.</p>
<h1><a class="anchor" id="The-ResourceGroupManager-Object"></a>
The ResourceGroupManager Object</h1>
<p>The <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a> class is actually a ’hub’ for loading of reusable resources like textures and meshes. It is the place that you define groups for your resources, so they may be unloaded and reloaded when you want. Servicing it are a number of ResourceManagers which manage the individual types of resource, like <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> or <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a>. In this context, resources are sets of data which must be loaded from somewhere to provide OGRE with the data it needs.</p>
<p>ResourceManagers ensure that resources are only loaded once and shared throughout the OGRE engine. They also manage the memory requirements of the resources they look after. They can also search in a number of locations for the resources they need, including multiple search paths and compressed archives (ZIP files).</p>
<p>Most of the time you won’t interact with resource managers directly. Resource managers will be called by other parts of the OGRE system as required, for example when you request for a texture to be added to a Material, the <a class="el" href="class_ogre_1_1_texture_manager.html" title="Class for loading &amp; managing textures. ">Ogre::TextureManager</a> will be called for you. If you like, you can call the appropriate resource manager directly to preload resources (if for example you want to prevent disk access later on) but most of the time it’s ok to let OGRE decide when to do it.</p>
<p>One thing you will want to do is to tell the resource managers where to look for resources. You do this via Root::getSingleton().addResourceLocation, which actually passes the information on to <a class="el" href="class_ogre_1_1_resource_group_manager.html" title="This singleton class manages the list of resource groups, and notifying the various resource managers...">Ogre::ResourceGroupManager</a>.</p>
<p>Because there is only ever 1 instance of each resource manager in the engine, if you do want to get a reference to a resource manager use the following syntax:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().someMethod()</div><div class="line"><a class="code" href="class_ogre_1_1_mesh_manager.html#a2efefa307b7987c5fcff7e8c47f79758">Ogre::MeshManager::getSingleton</a>().someMethod()</div></div><!-- fragment --> <h1><a class="anchor" id="The-Mesh-Object"></a>
The Mesh Object</h1>
<p>A <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object represents a discrete model, a set of geometry which is self-contained and is typically fairly small on a world scale. <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are assumed to represent movable objects and are not used for the sprawling level geometry typically used to create backgrounds.</p>
<p><a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> objects are a type of resource, and are managed by the MeshManager resource manager. They are typically loaded from OGRE’s custom object format, the ’.mesh’ format. Mesh files are typically created by exporting from a modelling tool See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> and can be manipulated through various <a class="el" href="_mesh-_tools.html">Mesh Tools</a></p>
<p>You can also create Mesh objects manually by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html#ad99f5f2e2d045e5a4af301974e45c918" title="Creates a new Mesh specifically for manual definition rather than loading from an object file...">Ogre::MeshManager::createManual</a> method. This way you can define the geometry yourself, but this is outside the scope of this manual.</p>
<p>Mesh objects are the basis for the individual movable objects in the world, which are called <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>.</p>
<p>Mesh objects can also be animated using See <a class="el" href="_animation.html#Skeletal-Animation">Skeletal Animation</a>.</p>
<h1><a class="anchor" id="Entities"></a>
Entities</h1>
<p>An entity is an instance of a movable object in the scene. It could be a car, a person, a dog, a shuriken, whatever. The only assumption is that it does not necessarily have a fixed position in the world.</p>
<p>Entities are based on discrete meshes, i.e. collections of geometry which are self-contained and typically fairly small on a world scale, which are represented by the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object. Multiple entities can be based on the same mesh, since often you want to create multiple copies of the same type of object in a scene.</p>
<p>You create an entity by calling the <a class="el" href="class_ogre_1_1_scene_manager.html#afb393cca49de8b928f7dd60838047185" title="Create an Entity (instance of a discrete mesh). ">Ogre::SceneManager::createEntity</a> method, giving it a name and specifying the name of the mesh object which it will be based on (e.g. ’muscleboundhero.mesh’). The <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a> will ensure that the mesh is loaded by calling the <a class="el" href="class_ogre_1_1_mesh_manager.html" title="Handles the management of mesh resources. ">Ogre::MeshManager</a> resource manager for you. Only one copy of the Mesh will be loaded.</p>
<p>Ogre::Entities are not deemed to be a part of the scene until you attach them to a <a class="el" href="class_ogre_1_1_scene_node.html" title="Class representing a node in the scene graph. ">Ogre::SceneNode</a> (see the section below). By attaching entities to SceneNodes, you can create complex hierarchical relationships between the positions and orientations of entities. You then modify the positions of the nodes to indirectly affect the entity positions.</p>
<p>When a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> is loaded, it automatically comes with a number of materials defined. It is possible to have more than one material attached to a mesh - different parts of the mesh may use different materials. Any entity created from the mesh will automatically use the default materials. However, you can change this on a per-entity basis if you like so you can create a number of entities based on the same mesh but with different textures etc.</p>
<p>To understand how this works, you have to know that all Mesh objects are actually composed of <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects, each of which represents a part of the mesh using one Material. If a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> uses only one <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a>, it will only have one <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a>.</p>
<p>When an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> is created based on this Mesh, it is composed of (possibly) multiple <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects, each matching 1 for 1 with the <a class="el" href="class_ogre_1_1_sub_mesh.html" title="Defines a part of a complete mesh. ">Ogre::SubMesh</a> objects from the original Mesh. You can access the <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a> objects using the <a class="el" href="class_ogre_1_1_entity.html#a23fc34c5e2eff03aae4cbf800c142724" title="Gets a pointer to a SubEntity, ie a part of an Entity. ">Ogre::Entity::getSubEntity</a> method. Once you have a reference to a <a class="el" href="class_ogre_1_1_sub_entity.html" title="Utility class which defines the sub-parts of an Entity. ">Ogre::SubEntity</a>, you can change the material it uses by calling it’s setMaterialName method. In this way you can make an <a class="el" href="class_ogre_1_1_entity.html" title="Defines an instance of a discrete, movable object based on a Mesh. ">Ogre::Entity</a> deviate from the default materials and thus create an individual looking version of it.</p>
<h1><a class="anchor" id="Materials"></a>
Materials</h1>
<p>The <a class="el" href="class_ogre_1_1_material.html" title="Class encapsulates rendering properties of an object. ">Ogre::Material</a> object controls how objects in the scene are rendered. It specifies what basic surface properties objects have such as reflectance of colours, shininess etc, how many texture layers are present, what images are on them and how they are blended together, what special effects are applied such as environment mapping, what culling mode is used, how the textures are filtered etc.</p>
<p>Materials can either be set up programmatically, by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a> and tweaking the settings, or by specifying it in a ’script’ which is loaded at runtime. See <a class="el" href="_material-_scripts.html">Material Scripts</a> for more info.</p>
<p>Basically everything about the appearance of an object apart from it’s shape is controlled by the Material class.</p>
<p>The SceneManager class manages the master list of materials available to the scene. The list can be added to by the application by calling <a class="el" href="class_ogre_1_1_material_manager.html#a3068599a5b10421e6203d568006f5f93" title="Create a new material. ">Ogre::MaterialManager::create</a>, or by loading a Mesh (which will in turn load material properties). Whenever materials are added to the SceneManager, they start off with a default set of properties; these are defined by OGRE as the following:</p>
<ul>
<li>ambient reflectance = ColourValue::White (full)</li>
<li>diffuse reflectance = ColourValue::White (full)</li>
<li>specular reflectance = ColourValue::Black (none)</li>
<li>emissive = ColourValue::Black (none)</li>
<li>shininess = 0 (not shiny)</li>
<li>No texture layers (&amp; hence no textures)</li>
<li>SourceBlendFactor = SBF_ONE, DestBlendFactor = SBF_ZERO (opaque)</li>
<li>Depth buffer checking on</li>
<li>Depth buffer writing on</li>
<li>Depth buffer comparison function = CMPF_LESS_EQUAL</li>
<li>Culling mode = CULL_CLOCKWISE</li>
<li>Ambient lighting in scene = ColourValue(0.5, 0.5, 0.5) (mid-grey)</li>
<li>Dynamic lighting enabled</li>
<li>Gourad shading mode</li>
<li>Solid polygon mode</li>
<li>Bilinear texture filtering</li>
</ul>
<p>You can alter these settings by calling <a class="el" href="class_ogre_1_1_material_manager.html#ac70230fa9f123976d7f2b1257bd94bd4" title="Returns a pointer to the default Material settings. ">Ogre::MaterialManager::getDefaultSettings()</a> and making the required changes to the Material which is returned.</p>
<p>Entities automatically have Material’s associated with them if they use a <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object, since the <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a> object typically sets up it’s required materials on loading. You can also customise the material used by an entity as described in <a class="el" href="_the-_core-_objects.html#Entities">Entities</a>. Just create a new Material, set it up how you like (you can copy an existing material into it if you like using a standard assignment statement) and point the SubEntity entries at it using <a class="el" href="class_ogre_1_1_sub_entity.html#ab8af6685128c68a5d7863bb5238ebc0f" title="Sets the name of the Material to be used. ">Ogre::SubEntity::setMaterialName()</a>.</p>
<h1><a class="anchor" id="Overlays"></a>
Overlays</h1>
<p>Overlays allow you to render 2D and 3D elements on top of the normal scene contents to create effects like heads-up displays (HUDs), menu systems, status panels etc. The frame rate statistics panel which comes as standard with OGRE is an example of an overlay. Overlays can contain 2D or 3D elements. 2D elements are used for HUDs, and 3D elements can be used to create cockpits or any other 3D object which you wish to be rendered on top of the rest of the scene.</p>
<p>You can create overlays either through the <a class="el" href="class_ogre_1_1_overlay_manager.html#ab6eb3dcc484ba5b06904ab2fb6b1aa23" title="Create a new Overlay. ">Ogre::OverlayManager::create</a> method, or you can define them in an .overlay script. See <a class="el" href="_overlay-_scripts.html">Overlay Scripts</a> for more info. In reality the latter is likely to be the most practical because it is easier to tweak (without the need to recompile the code). Note that you can define as many overlays as you like: they all start off life hidden, and you display them by calling <a class="el" href="class_ogre_1_1_overlay.html#a3c565f3e5cf61f43fa33542d713e8ccf" title="Shows the overlay if it was hidden. ">Ogre::Overlay::show</a>. You can also show multiple overlays at once, and their Z order is determined by the <a class="el" href="class_ogre_1_1_overlay.html#a5d8481d5fa0990846e53345f1a1cc705" title="Alters the Z-order of this overlay. ">Ogre::Overlay::setZOrder()</a> method.</p>
<p><a class="anchor" id="Notes-on-Integration"></a></p>
<h2>Notes on Integration</h2>
<p>The OverlaySystem is its own component, you need to manually initialize it, with the following two lines of code (mSceneMgr is a pointer to your current <a class="el" href="class_ogre_1_1_scene_manager.html" title="Manages the organisation and rendering of a &#39;scene&#39; i.e. ">Ogre::SceneManager</a>):</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>* pOverlaySystem = <span class="keyword">new</span> <a class="code" href="class_ogre_1_1_overlay_system.html">Ogre::OverlaySystem</a>();</div><div class="line">mSceneMgr-&gt;addRenderQueueListener(pOverlaySystem);</div></div><!-- fragment --><p>One <a class="el" href="class_ogre_1_1_overlay_system.html" title="This class simplify initialization / finalization of the overlay system. ">Ogre::OverlaySystem</a> per application is enough but you need to call addRenderQueueListener once per SceneManager.</p>
<p><a class="anchor" id="Creating-2D-Elements"></a></p>
<h2>Creating 2D Elements</h2>
<p>The <a class="el" href="class_ogre_1_1_overlay_element.html" title="Abstract definition of a 2D element to be displayed in an Overlay. ">Ogre::OverlayElement</a> class abstracts the details of 2D elements which are added to overlays. All items which can be added to overlays are derived from this class. It is possible (and encouraged) for users of OGRE to define their own custom subclasses of OverlayElement in order to provide their own user controls. The key common features of all OverlayElements are things like size, position, basic material name etc. Subclasses extend this behaviour to include more complex properties and behaviour.</p>
<p>An important built-in subclass of OverlayElement is <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a>. OverlayContainer is the same as a OverlayElement, except that it can contain other OverlayElements, grouping them together (allowing them to be moved together for example) and providing them with a local coordinate origin for easier lineup.</p>
<p>The third important class is <a class="el" href="class_ogre_1_1_overlay_manager.html" title="Manages Overlay objects, parsing them from .overlay files and storing a lookup library of them...">Ogre::OverlayManager</a>. Whenever an application wishes to create a 2D element to add to an overlay (or a container), it should call <a class="el" href="class_ogre_1_1_overlay_manager.html#a58166a773d27c22437323e8ef710406b" title="Creates a new OverlayElement of the type requested. ">Ogre::OverlayManager::createOverlayElement</a>. The type of element you wish to create is identified by a string, the reason being that it allows plugins to register new types of OverlayElement for you to create without you having to link specifically to those libraries. For example, to create a panel (a plain rectangular area which can contain other OverlayElements) you would call <code>OverlayManager::getSingleton().createOverlayElement("Panel", "myNewPanel");</code>.</p>
<p><a class="anchor" id="Adding-2D-Elements-to-the-Overlay"></a></p>
<h2>Adding 2D Elements to the Overlay</h2>
<p>Only OverlayContainers can be added direct to an overlay. The reason is that each level of container establishes the Zorder of the elements contained within it, so if you nest several containers, inner containers have a higher Z-order than outer ones to ensure they are displayed correctly. To add a container (such as a Panel) to the overlay, simply call <a class="el" href="class_ogre_1_1_overlay.html#a702b15ebaa6d3f8dbb2918500b76c9f0" title="Adds a 2D &#39;container&#39; to the overlay. ">Ogre::Overlay::add2D</a>.</p>
<p>If you wish to add child elements to that container, call <a class="el" href="class_ogre_1_1_overlay_container.html#aa9eec8f6dca98ac61766fc4785b84661" title="Adds another OverlayElement to this container. ">Ogre::OverlayContainer::addChild</a>. Child elements can be Ogre::OverlayElements or <a class="el" href="class_ogre_1_1_overlay_container.html" title="A 2D element which contains other OverlayElement instances. ">Ogre::OverlayContainer</a> instances themselves. Remember that the position of a child element is relative to the top-left corner of it’s parent.</p>
<p><a class="anchor" id="A-word-about-2D-coordinates"></a></p>
<h2>A word about 2D coordinates</h2>
<p>OGRE allows you to place and size elements based on 2 coordinate systems: <b>relative</b> and <b>pixel</b> based.</p>
<dl compact="compact">
<dt>Pixel Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want to specify an exact size for your overlay items, and you don’t mind if those items get smaller on the screen if you increase the screen resolution (in fact you might want this). In this mode the only way to put something in the middle or at the right or bottom of the screen reliably in any resolution is to use the aligning options, whilst in relative mode you can do it just by using the right relative coordinates. This mode is very simple, the top-left of the screen is (0,0) and the bottom-right of the screen depends on the resolution. As mentioned above, you can use the aligning options to make the horizontal and vertical coordinate origins the right, bottom or center of the screen if you want to place pixel items in these locations without knowing the resolution.</p>
<p class="enddd"></p>
</dd>
<dt>Relative Mode </dt>
<dd><p class="startdd"></p>
<p>This mode is useful when you want items in the overlay to be the same size on the screen no matter what the resolution. In relative mode, the top-left of the screen is (0,0) and the bottom-right is (1,1). So if you place an element at (0.5, 0.5), it’s top-left corner is placed exactly in the center of the screen, no matter what resolution the application is running in. The same principle applies to sizes; if you set the width of an element to 0.5, it covers half the width of the screen. Note that because the aspect ratio of the screen is typically 1.3333 : 1 (width : height), an element with dimensions (0.25, 0.25) will not be square, but it will take up exactly 1/16th of the screen in area terms. If you want square-looking areas you will have to compensate using the typical aspect ratio e.g. use (0.1875, 0.25) instead.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Transforming-Overlays"></a> </p><h2>Transforming Overlays</h2>
<p>Another nice feature of overlays is being able to rotate, scroll and scale them as a whole. You can use this for zooming in / out menu systems, dropping them in from off screen and other nice effects. See the <a class="el" href="class_ogre_1_1_overlay.html#ae07b1a57498aad0299f0781b09ca9b39" title="Scrolls the overlay by the offsets provided. ">Ogre::Overlay::scroll</a>, <a class="el" href="class_ogre_1_1_overlay.html#a8282553a59253170ddd0488ab7179792" title="Adds the passed in angle to the rotation applied to this overlay. ">Ogre::Overlay::rotate</a> and <a class="el" href="class_ogre_1_1_overlay.html#adea3b88baa3697a657e7b5785f0ffedb" title="Sets the scaling factor of this overlay. ">Ogre::Overlay::setScale</a> methods for more information.</p>
<p><a class="anchor" id="GUI-systems"></a></p>
<h2>GUI systems</h2>
<p>Overlays are only really designed for non-interactive screen elements, although you can create a simple GUI using the <a class="el" href="trays.html">Trays System</a>. For a far more complete GUI solution, we recommend or <a href="https://github.com/OGRECave/ogre-imgui">Dear ImGui</a>, <a href="http://www.cegui.org.uk">CEGui</a> or <a href="http://mygui.info/">MyGUI</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Scripts</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_scripts.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Scripts </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>OGRE drives many of its features through scripts in order to make it easier to set up. The scripts are simply plain text files which can be edited in any standard text editor, and modifying them immediately takes effect on your OGRE-based applications, without any need to recompile. This makes prototyping a lot faster. Here are the items that OGRE lets you script:</p>
<ul>
<li><a class="el" href="_material-_scripts.html">Material Scripts</a></li>
<li><a class="el" href="_compositor-_scripts.html">Compositor Scripts</a></li>
<li><a class="el" href="_particle-_scripts.html">Particle Scripts</a></li>
<li><a class="el" href="_overlay-_scripts.html">Overlay Scripts</a></li>
<li><a class="el" href="_font-_definition-_scripts.html">Font Definition Scripts</a> </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: High-level Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_high-level-_programs.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">High-level Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Cg">Cg programs</a></li>
<li class="level1"><a href="#HLSL">DirectX9 HLSL</a></li>
<li class="level1"><a href="#GLSL">OpenGL GLSL</a><ul><li class="level2"><a href="#GLSL-Texture-Samplers">GLSL Texture Samplers</a></li>
<li class="level2"><a href="#Matrix-parameters">Matrix parameters</a></li>
<li class="level2"><a href="#Accessing-OpenGL-states-in-GLSL">Accessing OpenGL states in GLSL</a></li>
<li class="level2"><a href="#Binding-vertex-attributes">Binding vertex attributes</a></li>
<li class="level2"><a href="#Preprocessor-definitions">Preprocessor definitions</a></li>
<li class="level2"><a href="#GLSL-Geometry-shader-specification">GLSL Geometry shader specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a></li>
<li class="level1"><a href="#Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass">Using GPU Programs in a Pass</a><ul><li class="level2"><a href="#Program-Parameter-Specification">Parameter specification</a></li>
</ul>
</li>
<li class="level1"><a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a></li>
<li class="level1"><a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a></li>
<li class="level1"><a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a></li>
<li class="level1"><a href="#Adjacency-information-in-Geometry-Programs">Adjacency information in Geometry Programs</a></li>
<li class="level1"><a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a></li>
</ul>
</div>
<div class="textblock"><p>Support for high level vertex and fragment programs is provided through plugins; this is to make sure that an application using OGRE can use as little or as much of the high-level program functionality as they like. OGRE currently supports 3 high-level program types, Cg (<a href="#Cg">Cg</a>) (an API- and card-independent, high-level language which lets you write programs for both OpenGL and DirectX for lots of cards), DirectX 9 High-Level Shader Language (<a href="#HLSL">HLSL</a>), and OpenGL Shader Language (<a href="#GLSL">GLSL</a>). HLSL can only be used with the DirectX rendersystem, and GLSL can only be used with the GL rendersystem. Cg can be used with both, although experience has shown that more advanced programs, particularly fragment programs which perform a lot of texture fetches, can produce better code in the rendersystem-specific shader language.</p>
<p>One way to support both HLSL and GLSL is to include separate techniques in the material script, each one referencing separate programs. However, if the programs are basically the same, with the same parameters, and the techniques are complex this can bloat your material scripts with duplication fairly quickly. Instead, if the only difference is the language of the vertex &amp; fragment program you can use OGRE’s <a href="#Unified-High_002dlevel-Programs">Unified High-level Programs</a> to automatically pick a program suitable for your rendersystem whilst using a single technique.</p>
<h1><a class="anchor" id="Cg"></a>
Cg programs</h1>
<p>In order to define Cg programs, you have to have to load Plugin_CgProgramManager.so/.dll at startup, either through plugins.cfg or through your own plugin loading code. They are very easy to define:</p>
<div class="fragment"><div class="line">fragment_program myCgFragmentProgram cg</div><div class="line">{</div><div class="line">    source myCgFragmentProgram.cg</div><div class="line">    entry_point main</div><div class="line">    profiles ps_2_0 arbfp1</div><div class="line">}</div></div><!-- fragment --><p>There are a few differences between this and the assembler program - to begin with, we declare that the fragment program is of type <code>cg</code> rather than <code>asm</code>, which indicates that it’s a high-level program using Cg. The <code>source</code> parameter is the same, except this time it’s referencing a Cg source file instead of a file of assembler. Here is where things start to change. Firstly, we need to define an <code>entry_point</code>, which is the name of a function in the Cg program which will be the first one called as part of the fragment program. Unlike assembler programs, which just run top-to-bottom, Cg programs can include multiple functions and as such you must specify the one which start the ball rolling. Next, instead of a fixed <code>syntax</code> parameter, you specify one or more <code>profiles</code>; profiles are how Cg compiles a program down to the low-level assembler. The profiles have the same names as the assembler syntax codes mentioned above; the main difference is that you can list more than one, thus allowing the program to be compiled down to more low-level syntaxes so you can write a single high-level program which runs on both D3D and GL. You are advised to just enter the simplest profiles under which your programs can be compiled in order to give it the maximum compatibility. The ordering also matters; if a card supports more than one syntax then the one listed first will be used.</p>
<p>Lastly, there is a final option called <code>compile_arguments</code>, where you can specify arguments exactly as you would to the cgc command-line compiler, should you wish to.</p>
<h1><a class="anchor" id="HLSL"></a>
DirectX9 HLSL</h1>
<p>DirectX9 HLSL has a very similar language syntax to Cg but is tied to the DirectX API. The only benefit over Cg is that it only requires the DirectX 9 render system plugin, not any additional plugins. Declaring a DirectX9 HLSL program is very similar to Cg. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myHLSLVertexProgram hlsl</div><div class="line">{</div><div class="line">    source myHLSLVertexProgram.txt</div><div class="line">    entry_point main</div><div class="line">    target vs_2_0</div><div class="line">}</div></div><!-- fragment --><p>As you can see, the main syntax is almost identical, except that instead of <code>profiles</code> with a list of assembler formats, you have a <code>target</code> parameter which allows a single assembler target to be specified - obviously this has to be a DirectX assembler format syntax code.</p>
<p><b>Important Matrix Ordering Note:</b> One thing to bear in mind is that HLSL allows you to use 2 different ways to multiply a vector by a matrix - mul(v,m) or mul(m,v). The only difference between them is that the matrix is effectively transposed. You should use mul(m,v) with the matrices passed in from <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> - this agrees with the shaders produced from tools like RenderMonkey, and is consistent with Cg too, but disagrees with the Dx9 SDK and FX Composer which use mul(v,m) - you will have to switch the parameters to mul() in those shaders.</p>
<p>Note that if you use the float3x4 / matrix3x4 type in your shader, bound to an OGRE auto-definition (such as bone matrices) you should use the <code>column_major_matrices = false</code> option (discussed below) in your program definition. This is because OGRE passes float3x4 as row-major to save constant space (3 float4’s rather than 4 float4’s with only the top 3 values used) and this tells OGRE to pass all matrices like this, so that you can use mul(m,v) consistently for all calculations. OGRE will also to tell the shader to compile in row-major form (you don’t have to set the /Zpr compile option or #pragma pack(row-major) option, OGRE does this for you). Note that passing bones in float4x3 form is not supported by OGRE, but you don’t need it given the above.</p>
<p><b>Advanced options</b><br />
</p>
<dl compact="compact">
<dt>preprocessor_defines &lt;defines&gt; </dt>
<dd><p class="startdd"></p>
<p>This allows you to define symbols which can be used inside the HLSL shader code to alter the behaviour (through #ifdef or #if clauses). Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1.</p>
<p class="enddd"></p>
</dd>
<dt>column_major_matrices &lt;true|false&gt; </dt>
<dd><p class="startdd"></p>
<p>The default for this option is ’true’ so that OGRE passes matrices auto-bound matrices in a form where mul(m,v) works. Setting this option to false does 2 things - it transpose auto-bound 4x4 matrices and also sets the /Zpr (row-major) option on the shader compilation. This means you can still use mul(m,v), but the matrix layout is row-major instead. This is only useful if you need to use bone matrices (float3x4) in a shader since it saves a float4 constant for every bone involved.</p>
<p class="enddd"></p>
</dd>
<dt>optimisation_level &lt;opt&gt; </dt>
<dd><p class="startdd"></p>
<p>Set the optimisation level, which can be one of ’default’, ’none’, ’0’, ’1’, ’2’, or ’3’. This corresponds to the /O parameter of fxc.exe, except that in ’default’ mode, optimisation is disabled in debug mode and set to 1 in release mode (fxc.exe uses 1 all the time). Unsurprisingly the default value is ’default’. You may want to change this if you want to tweak the optimisation, for example if your shader gets so complex that it will not longer compile without some minimum level of optimisation.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="GLSL"></a>
OpenGL GLSL</h1>
<p>OpenGL GLSL has a similar language syntax to HLSL but is tied to the OpenGL API. The are a few benefits over Cg in that it only requires the OpenGL render system plugin, not any additional plugins. Declaring a OpenGL GLSL program is similar to Cg but simpler. Here’s an example:</p>
<div class="fragment"><div class="line">vertex_program myGLSLVertexProgram glsl</div><div class="line">{</div><div class="line">    source myGLSLVertexProgram.txt</div><div class="line">}</div></div><!-- fragment --><p>In GLSL, no entry point needs to be defined since it is always <code>main()</code> and there is no target definition since GLSL source is compiled into native GPU code and not intermediate assembly.</p>
<p>GLSL supports the use of modular shaders. This means you can write GLSL external functions that can be used in multiple shaders.</p>
<div class="fragment"><div class="line">vertex_program myExternalGLSLFunction1 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction1.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myExternalGLSLFunction2 glsl</div><div class="line">{</div><div class="line">    source myExternalGLSLfunction2.txt</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram1 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1 myExternalGLSLFunction2</div><div class="line">}</div><div class="line"></div><div class="line">vertex_program myGLSLVertexProgram2 glsl</div><div class="line">{</div><div class="line">    source myGLSLfunction.txt</div><div class="line">    attach myExternalGLSLFunction1</div><div class="line">}</div></div><!-- fragment --><p>External GLSL functions are attached to the program that needs them by using <code>attach</code> and including the names of all external programs required on the same line separated by spaces. This can be done for both vertex and fragment programs.</p>
<h2><a class="anchor" id="GLSL-Texture-Samplers"></a>
GLSL Texture Samplers</h2>
<p>To pass texture unit index values from the material script to texture samplers in glsl use <code>int</code> type named parameters. See the example below:<br />
</p>
<p>excerpt from GLSL example.frag source:</p>
<div class="fragment"><div class="line">varying vec2 UV;</div><div class="line">uniform sampler2D diffuseMap;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> main(<span class="keywordtype">void</span>)</div><div class="line">{</div><div class="line">    gl_FragColor = texture2D(diffuseMap, UV);</div><div class="line">}</div></div><!-- fragment --><p>In material script:</p>
<div class="fragment"><div class="line">fragment_program myFragmentShader glsl</div><div class="line">{</div><div class="line">  source example.frag</div><div class="line">}</div><div class="line"></div><div class="line">material exampleGLSLTexturing</div><div class="line">{</div><div class="line">  technique</div><div class="line">  {</div><div class="line">    pass</div><div class="line">    {</div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      {</div><div class="line">        param_named diffuseMap <span class="keywordtype">int</span> 0</div><div class="line">      }</div><div class="line"></div><div class="line">      texture_unit </div><div class="line">      {</div><div class="line">        texture myTexture.jpg 2d</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>An index value of 0 refers to the first texture unit in the pass, an index value of 1 refers to the second unit in the pass and so on.</p>
<h2><a class="anchor" id="Matrix-parameters"></a>
Matrix parameters</h2>
<p>Here are some examples of passing matrices to GLSL mat2, mat3, mat4 uniforms:</p>
<div class="fragment"><div class="line">material exampleGLSLmatrixUniforms</div><div class="line">{</div><div class="line">  technique matrix_passing</div><div class="line">  {</div><div class="line">    pass examples</div><div class="line">    {</div><div class="line">      vertex_program_ref myVertexShader</div><div class="line">      {</div><div class="line">        <span class="comment">// mat4 uniform</span></div><div class="line">        param_named OcclusionMatrix matrix4x4 1 0 0 0  0 1 0 0  0 0 1 0  0 0 0 0 </div><div class="line">        <span class="comment">// or</span></div><div class="line">        param_named ViewMatrix float16 0 1 0 0  0 0 1 0  0 0 0 1  0 0 0 0 </div><div class="line"></div><div class="line">        <span class="comment">// mat3</span></div><div class="line">        param_named TextRotMatrix float9 1 0 0  0 1 0  0 0 1  </div><div class="line">      }</div><div class="line"></div><div class="line">      fragment_program_ref myFragmentShader</div><div class="line">      { </div><div class="line">        <span class="comment">// mat2 uniform</span></div><div class="line">        param_named skewMatrix float4 0.5 0 -0.5 1.0</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2><a class="anchor" id="Accessing-OpenGL-states-in-GLSL"></a>
Accessing OpenGL states in GLSL</h2>
<p>GLSL can access most of the GL states directly so you do not need to pass these states through <a href="#param_005fnamed_005fauto">param_named_auto</a> in the material script. This includes lights, material state, and all the matrices used in the openGL state i.e. model view matrix, worldview projection matrix etc.</p>
<dl class="section note"><dt>Note</dt><dd>this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<h2><a class="anchor" id="Binding-vertex-attributes"></a>
Binding vertex attributes</h2>
<p>GLSL natively supports automatic binding of the most common incoming per-vertex attributes (e.g. <code>gl_Vertex</code>, <code>gl_Normal</code>, <code>gl_MultiTexCoord0</code> etc). However, there are some which are not automatically bound, which must be declared in the shader using the <code>attribute &lt;type&gt; &lt;name&gt;</code> syntax, and the vertex data bound to it by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl class="section note"><dt>Note</dt><dd>again this is only possible with OpenGL legacy profiles i.e. <b>not</b> with GL3+.</dd></dl>
<p>In addition to the built in attributes described in section 7.3 of the GLSL manual, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports a number of automatically bound custom vertex attributes. There are some drivers that do not behave correctly when mixing built-in vertex attributes like <code>gl_Normal</code> and custom vertex attributes, so for maximum compatibility you should use all custom attributes</p>
<dl compact="compact">
<dt>vertex </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2" title="Position, 3 reals per vertex. ">Ogre::VES_POSITION</a>, declare as ’attribute vec4 vertex;’.</p>
<p class="enddd"></p>
</dd>
<dt>normal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da1dec7d36a64625348e23af26c9231227" title="Normal, 3 reals per vertex. ">Ogre::VES_NORMAL</a>, declare as ’attribute vec3 normal;’.</p>
<p class="enddd"></p>
</dd>
<dt>colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daabddc327dc0835e75b7622ee81e7f527" title="Diffuse colours. ">Ogre::VES_DIFFUSE</a>, declare as ’attribute vec4 colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>secondary_colour </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00daac8754d6241197a32c19cd72ff1806cc" title="Specular colours. ">Ogre::VES_SPECULAR</a>, declare as ’attribute vec4 secondary_colour;’.</p>
<p class="enddd"></p>
</dd>
<dt>uv0 - uv7 </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da17abb855311f3a67bc31414cad8d6212" title="Texture coordinates. ">Ogre::VES_TEXTURE_COORDINATES</a>, declare as ’attribute vec4 uv0;’. Note that uv6 and uv7 share attributes with tangent and binormal respectively so cannot both be present.</p>
<p class="enddd"></p>
</dd>
<dt>tangent </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da57d43012049b3ec7b4240b207dd029f2" title="Tangent (X axis if normal is Z) ">Ogre::VES_TANGENT</a>, declare as ’attribute vec3 tangent;’.</p>
<p class="enddd"></p>
</dd>
<dt>binormal </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da2b06bd6ce4e2b9115ab122e35d86502f" title="Binormal (Y axis if normal is Z) ">Ogre::VES_BINORMAL</a>, declare as ’attribute vec3 binormal;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendIndices </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dae54f94c9c9cf553d227c2aab2c499bbf" title="Blending indices. ">Ogre::VES_BLEND_INDICES</a>, declare as ’attribute vec4 blendIndices;’.</p>
<p class="enddd"></p>
</dd>
<dt>blendWeights </dt>
<dd><p class="startdd"></p>
<p>Binds <a class="el" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00da63d3494547f373f5c4397a41b39cf6a0" title="Blending weights. ">Ogre::VES_BLEND_WEIGHTS</a>, declare as ’attribute vec4 blendWeights;’.</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Preprocessor-definitions"></a>
Preprocessor definitions</h2>
<p>GLSL supports using preprocessor definitions in your code - some are defined by the implementation, but you can also define your own, say in order to use the same source code for a few different variants of the same technique. In order to use this feature, include preprocessor conditions in your GLSL code, of the kind #ifdef SYMBOL, #if SYMBOL==2 etc. Then in your program definition, use the ’preprocessor_defines’ option, following it with a string if definitions. Definitions are separated by ’;’ or ’,’ and may optionally have a ’=’ operator within them to specify a definition value. Those without an ’=’ will implicitly have a definition of 1. For example:</p>
<div class="fragment"><div class="line"><span class="comment">// in your GLSL</span></div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CLEVERTECHNIQUE</span></div><div class="line">    <span class="comment">// some clever stuff here</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// normal technique</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="preprocessor">#if NUM_THINGS==2</span></div><div class="line">    <span class="comment">// Some specific code</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// something else</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="comment">// in  your program definition</span></div><div class="line">preprocessor_defines CLEVERTECHNIQUE,NUMTHINGS=2</div></div><!-- fragment --><p>This way you can use the same source code but still include small variations, each one defined as a different <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> program name but based on the same source code.</p>
<h2><a class="anchor" id="GLSL-Geometry-shader-specification"></a>
GLSL Geometry shader specification</h2>
<p>GLSL allows the same shader to run on different types of geometry primitives. In order to properly link the shaders together, you have to specify which primitives it will receive as input, which primitives it will emit and how many vertices a single run of the shader can generate. The GLSL geometry_program definition requires three additional parameters :</p>
<dl compact="compact">
<dt>input_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will receive. Can be ’point_list’, ’line_list’, ’line_strip’, ’triangle_list’, ’triangle_strip’ or ’triangle_fan’.</p>
<p class="enddd"></p>
</dd>
<dt>output_operation_type </dt>
<dd><p class="startdd"></p>
<p>The operation type of the geometry that the shader will emit. Can be ’point_list’, ’line_strip’ or ’triangle_strip’.</p>
<p class="enddd"></p>
</dd>
<dt>max_output_vertices </dt>
<dd><p class="startdd"></p>
<p>The maximum number of vertices that the shader can emit. There is an upper limit for this value, it is exposed in the render system capabilities.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For example:</p>
<div class="fragment"><div class="line">geometry_program <a class="code" href="namespace_ogre.html">Ogre</a>/GPTest/Swizzle_GP_GLSL glsl</div><div class="line">{</div><div class="line">    source SwizzleGP.glsl</div><div class="line">    input_operation_type triangle_list</div><div class="line">    output_operation_type line_strip</div><div class="line">    max_output_vertices 6</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="Unified-High_002dlevel-Programs"></a>
Unified High-level Programs</h1>
<p>As mentioned above, it can often be useful to write both HLSL and GLSL programs to specifically target each platform, but if you do this via multiple material techniques this can cause a bloated material definition when the only difference is the program language. Well, there is another option. You can ’wrap’ multiple programs in a ’unified’ program definition, which will automatically choose one of a series of ’delegate’ programs depending on the rendersystem and hardware support.</p>
<div class="fragment"><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate realProgram1</div><div class="line">    delegate realProgram2</div><div class="line">    ... etc</div><div class="line">}</div></div><!-- fragment --><p>This works for both vertex and fragment programs, and you can list as many delegates as you like - the first one to be supported by the current rendersystem &amp; hardware will be used as the real program. This is almost like a mini-technique system, but for a single program and with a much tighter purpose. You can only use this where the programs take all the same inputs, particularly textures and other pass / sampler state. Where the only difference between the programs is the language (or possibly the target in HLSL - you can include multiple HLSL programs with different targets in a single unified program too if you want, or indeed any number of other high-level programs), this can become a very powerful feature. For example, without this feature here’s how you’d have to define a programmable material which supported HLSL and GLSL:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithoutUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="comment">// GLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgramHLSL</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgramHLSL</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>And that’s a really small example. Everything you added to the HLSL technique, you’d have to duplicate in the GLSL technique too. So instead, here’s how you’d do it with unified program definitions:</p>
<div class="fragment"><div class="line">vertex_program myVertexProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_vp</div><div class="line">    target vs_2_0</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramHLSL hlsl</div><div class="line">{</div><div class="line">    source prog.hlsl</div><div class="line">    entry_point main_fp</div><div class="line">    target ps_2_0</div><div class="line">}</div><div class="line">vertex_program myVertexProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.vert</div><div class="line">}</div><div class="line">fragment_program myFragmentProgramGLSL glsl</div><div class="line">{</div><div class="line">    source prog.frag</div><div class="line">    default_params</div><div class="line">    {</div><div class="line">        param_named tex <span class="keywordtype">int</span> 0</div><div class="line">    }</div><div class="line">}</div><div class="line"><span class="comment">// Unified definition</span></div><div class="line">vertex_program myVertexProgram unified</div><div class="line">{</div><div class="line">    delegate myVertexProgramGLSL</div><div class="line">    delegate myVertexProgramHLSL</div><div class="line">}</div><div class="line">fragment_program myFragmentProgram unified</div><div class="line">{</div><div class="line">    delegate myFragmentProgramGLSL</div><div class="line">    delegate myFragmentProgramHLSL</div><div class="line">}</div><div class="line">material SupportHLSLandGLSLwithUnified</div><div class="line">{</div><div class="line">    <span class="comment">// HLSL technique</span></div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            vertex_program_ref myVertexProgram</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj world_view_proj_matrix</div><div class="line">                param_named_auto lightColour light_diffuse_colour 0</div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">                param_named_auto lightAtten light_attenuation 0</div><div class="line">            }</div><div class="line">            fragment_program_ref myFragmentProgram</div><div class="line">            {</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>At runtime, when myVertexProgram or myFragmentProgram are used, OGRE automatically picks a real program to delegate to based on what’s supported on the current hardware / rendersystem. If none of the delegates are supported, the entire technique referencing the unified program is marked as unsupported and the next technique in the material is checked fro fallback, just like normal. As your materials get larger, and you find you need to support HLSL and GLSL specifically (or need to write multiple interface-compatible versions of a program for whatever other reason), unified programs can really help reduce duplication.</p>
<h1><a class="anchor" id="Using-Vertex_002fGeometry_002fFragment-Programs-in-a-Pass"></a>
Using GPU Programs in a Pass</h1>
<p>Within a pass section of a material script, you can reference a vertex, geometry and / or a fragment program which is been defined in a .program script (See <a class="el" href="_material-_scripts.html#Declaring-Vertex_002fGeometry_002fFragment-Programs">Declaring GPU Programs</a>). The programs are defined separately from the usage of them in the pass, since the programs are very likely to be reused between many separate materials, probably across many different .material scripts, so this approach lets you define the program only once and use it many times.</p>
<p>As well as naming the program in question, you can also provide parameters to it. Here’s a simple example:</p>
<div class="fragment"><div class="line">vertex_program_ref myVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed      4 float4  10.0 0 0 0</div><div class="line">}</div></div><!-- fragment --><p>In this example, we bind a vertex program called ’myVertexProgram’ (which will be defined elsewhere) to the pass, and give it 2 parameters, one is an ’auto’ parameter, meaning we do not have to supply a value as such, just a recognised code (in this case it’s the world/view/projection matrix which is kept up to date automatically by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>). The second parameter is a manually specified parameter, a 4-element float. The indexes are described later.</p>
<p>The syntax of the link to a vertex program and a fragment or geometry program are identical, the only difference is that ’fragment_program_ref’ and ’geometry_program_ref’ are used respectively instead of ’vertex_program_ref’.</p>
<p>For many situations vertex, geometry and fragment programs are associated with each other in a pass but this is not cast in stone. You could have a vertex program that can be used by several different fragment programs. Another situation that arises is that you can mix fixed pipeline and programmable pipeline (shaders) together. You could use the non-programmable vertex fixed function pipeline and then provide a fragment_program_ref in a pass i.e. there would be no vertex_program_ref section in the pass. The fragment program referenced in the pass must meet the requirements as defined in the related API in order to read from the outputs of the vertex fixed pipeline. You could also just have a vertex program that outputs to the fragment fixed function pipeline.</p>
<p>The requirements to read from or write to the fixed function pipeline are similar between rendering API’s (DirectX and OpenGL) but how its actually done in each type of shader (vertex, geometry or fragment) depends on the shader language. For HLSL (DirectX API) and associated asm consult MSDN at <a href="http://msdn.microsoft.com/library/">http://msdn.microsoft.com/library/</a>. For GLSL (OpenGL), consult section 7.6 of the GLSL spec 1.1 available at <a href="http://www.opengl.org/registry/">http://www.opengl.org/registry/</a>. The built in varying variables provided in GLSL allow your program to read/write to the fixed function pipeline varyings. For Cg consult the Language Profiles section in CgUsersManual.pdf that comes with the Cg Toolkit available at <a href="https://developer.nvidia.com/cg-toolkit">https://developer.nvidia.com/cg-toolkit</a>. For HLSL and Cg its the varying bindings that allow your shader programs to read/write to the fixed function pipeline varyings.</p>
<h2><a class="anchor" id="Program-Parameter-Specification"></a>
Parameter specification</h2>
<p>Parameters can be specified using one of 4 commands as shown below. The same syntax is used whether you are defining a parameter just for this particular use of the program, or when specifying the <a href="#Default-Program-Parameters">Default Program Parameters</a>. Parameters set in the specific use of the program override the defaults.</p>
<ul>
<li><a href="#param_005findexed">param_indexed</a></li>
<li><a href="#param_005findexed_005fauto">param_indexed_auto</a></li>
<li><a href="#param_005fnamed">param_named</a></li>
<li><a href="#param_005fnamed_005fauto">param_named_auto</a></li>
<li><a href="#shared_005fparams_005fref">shared_params_ref</a></li>
</ul>
<p><a class="anchor" id="param_005findexed"></a><a class="anchor" id="param_005findexed-1"></a></p>
<h2>param_indexed</h2>
<p>This command sets the value of an indexed parameter.</p>
<p>format: param_indexed &lt;index&gt; &lt;type&gt; &lt;value&gt; example: param_indexed 0 float4 10.0 0 0 0</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>simply a number representing the position in the parameter list which the value should be written, and you should derive this from your program definition. The index is relative to the way constants are stored on the card, which is in 4-element blocks. For example if you defined a float4 parameter at index 0, the next index would be 1. If you defined a matrix4x4 at index 0, the next usable index would be 4, since a 4x4 matrix takes up 4 indexes.</td></tr>
    <tr><td class="paramname">type</td><td>can be float4, matrix4x4, float&lt;n&gt;, int4, int&lt;n&gt;. Note that ’int’ parameters are only available on some more advanced program syntaxes, check the D3D or GL vertex / fragment program documentation for full details. Typically the most useful ones will be float4 and matrix4x4. Note that if you use a type which is not a multiple of 4, then the remaining values up to the multiple of 4 will be filled with zeroes for you (since GPUs always use banks of 4 floats per constant even if only one is used).</td></tr>
    <tr><td class="paramname">value</td><td>a space or tab-delimited list of values which can be converted into the type you have specified.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005findexed_005fauto"></a><a class="anchor" id="param_005findexed_005fauto-1"></a></p>
<h2>param_indexed_auto</h2>
<p>This command tells <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to automatically update a given parameter with a derived value. This frees you from writing code to update program parameters every frame when they are always changing.</p>
<p>format: param_indexed_auto &lt;index&gt; &lt;value_code&gt; &lt;extra_params&gt; example: param_indexed_auto 0 worldviewproj_matrix</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">index</td><td>has the same meaning as <a href="#param_005findexed">param_indexed</a>; note this time you do not have to specify the size of the parameter because the engine knows this already. In the example, the world/view/projection matrix is being used so this is implicitly a matrix4x4.</td></tr>
    <tr><td class="paramname">value_code</td><td>is one of <a class="el" href="class_ogre_1_1_gpu_program_parameters.html#a155c886f15e0c10d2c33c224f0d43ce3" title="Defines the types of automatically updated values that may be bound to GpuProgram parameters...">Ogre::GpuProgramParameters::AutoConstantType</a> without the <code>ACT_</code> prefix. E.g. <code>ACT_WORLD_MATRIX</code> becomes <code>world_matrix</code>.</td></tr>
  </table>
  </dd>
</dl>
<p><a class="anchor" id="param_005fnamed"></a><a class="anchor" id="param_005fnamed-1"></a></p>
<h2>param_named</h2>
<p>This is the same as param_indexed, but uses a named parameter instead of an index. This can only be used with high-level programs which include parameter names; if you’re using an assembler program then you have no choice but to use indexes. Note that you can use indexed parameters for high-level programs too, but it is less portable since if you reorder your parameters in the high-level program the indexes will change. format: param_named &lt;name&gt; &lt;type&gt; &lt;value&gt; example: param_named shininess float4 10.0 0 0 0 The type is required because the program is not compiled and loaded when the material script is parsed, so at this stage we have no idea what types the parameters are. Programs are only loaded and compiled when they are used, to save memory.</p>
<p><a class="anchor" id="param_005fnamed_005fauto"></a><a class="anchor" id="param_005fnamed_005fauto-1"></a></p>
<h2>param_named_auto</h2>
<p>This is the named equivalent of param_indexed_auto, for use with high-level programs. Format: param_named_auto &lt;name&gt; &lt;value_code&gt; &lt;extra_params&gt; Example: param_named_auto worldViewProj WORLDVIEWPROJ_MATRIX</p>
<p>The allowed value codes and the meaning of extra_params are detailed in <a href="#param_005findexed_005fauto">param_indexed_auto</a>.</p>
<p><a class="anchor" id="shared_005fparams_005fref"></a><a class="anchor" id="shared_005fparams_005fref-1"></a></p>
<h2>shared_params_ref</h2>
<p>This option allows you to reference shared parameter sets as defined in <a href="#Declaring-Shared-Parameters">Declaring Shared Parameters</a>. Format: shared_params_ref &lt;shared_set_name&gt; Example: shared_params_ref mySharedParams</p>
<p>The only required parameter is a name, which must be the name of an already defined shared parameter set. All named parameters which are present in the program that are also present in the shared parameter set will be linked, and the shared parameters used as if you had defined them locally. This is dependent on the definitions (type and array size) matching between the shared set and the program.</p>
<h1><a class="anchor" id="Shadows-and-Vertex-Programs"></a>
Shadows and Vertex Programs</h1>
<p>When using shadows (See <a class="el" href="_shadows.html">Shadows</a>), the use of vertex programs can add some additional complexities, because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only automatically deal with everything when using the fixed-function pipeline. If you use vertex programs, and you are also using shadows, you may need to make some adjustments.</p>
<p>If you use <b>stencil shadows</b>, then any vertex programs which do vertex deformation can be a problem, because stencil shadows are calculated on the CPU, which does not have access to the modified vertices. If the vertex program is doing standard skeletal animation, this is ok (see section above) because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> knows how to replicate the effect in software, but any other vertex deformation cannot be replicated, and you will either have to accept that the shadow will not reflect this deformation, or you should turn off shadows for that object.</p>
<p>If you use <b>texture shadows</b>, then vertex deformation is acceptable; however, when rendering the object into the shadow texture (the shadow caster pass), the shadow has to be rendered in a solid colour (linked to the ambient colour). You must therefore provide an alternative vertex program, so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides you with a way of specifying one to use when rendering the caster. Basically you link an alternative vertex program, using exactly the same syntax as the original vertex program link:</p>
<div class="fragment"><div class="line">shadow_caster_vertex_program_ref myShadowCasterVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 ambient_light_colour</div><div class="line">}</div></div><!-- fragment --><p>When rendering a shadow caster, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will automatically use the alternate program. You can bind the same or different parameters to the program - the most important thing is that you bind <b>ambient_light_colour</b>, since this determines the colour of the shadow in modulative texture shadows. If you don’t supply an alternate program, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will fall back on a fixed-function material which will not reflect any vertex deformation you do in your vertex program.</p>
<p>In addition, when rendering the shadow receivers with shadow textures, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> needs to project the shadow texture. It does this automatically in fixed function mode, but if the receivers use vertex programs, they need to have a shadow receiver program which does the usual vertex deformation, but also generates projective texture coordinates. The additional program linked into the pass like this:</p>
<div class="fragment"><div class="line">shadow_receiver_vertex_program_ref myShadowReceiverVertexProgram</div><div class="line">{</div><div class="line">    param_indexed_auto 0 worldviewproj_matrix</div><div class="line">    param_indexed_auto 4 texture_viewproj_matrix</div><div class="line">}</div></div><!-- fragment --><p>For the purposes of writing this alternate program, there is an automatic parameter binding of ’texture_viewproj_matrix’ which provides the program with texture projection parameters. The vertex program should do it’s normal vertex processing, and generate texture coordinates using this matrix and place them in texture coord sets 0 and 1, since some shadow techniques use 2 texture units. The colour of the vertices output by this vertex program must always be white, so as not to affect the final colour of the rendered shadow.</p>
<p>When using additive texture shadows, the shadow pass render is actually the lighting render, so if you perform any fragment program lighting you also need to pull in a custom fragment program. You use the shadow_receiver_fragment_program_ref for this:</p>
<div class="fragment"><div class="line">shadow_receiver_fragment_program_ref myShadowReceiverFragmentProgram</div><div class="line">{</div><div class="line">    param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">}</div></div><!-- fragment --><p>You should pass the projected shadow coordinates from the custom vertex program. As for textures, texture unit 0 will always be the shadow texture. Any other textures which you bind in your pass will be carried across too, but will be moved up by 1 unit to make room for the shadow texture. Therefore your shadow receiver fragment program is likely to be the same as the bare lighting pass of your normal material, except that you insert an extra texture sampler at index 0, which you will use to adjust the result by (modulating diffuse and specular components).</p>
<h1><a class="anchor" id="Skeletal-Animation-in-Vertex-Programs"></a>
Skeletal Animation in Vertex Programs</h1>
<p>You can implement skeletal animation in hardware by writing a vertex program which uses the per-vertex blending indices and blending weights, together with an array of world matrices (which will be provided for you by <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> if you bind the automatic parameter ’world_matrix_array_3x4’). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform skeletal animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_skeletal_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual animation blend and will expect the vertex program to do it, for both vertex positions and normals. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a class="el" href="_animation.html">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Morph-Animation-in-Vertex-Programs"></a>
Morph Animation in Vertex Programs</h1>
<p>You can implement morph animation in hardware by writing a vertex program which linearly blends between the first and second position keyframes passed as positions and the first free texture coordinate set, and by binding the animation_parametric value to a parameter (which tells you how far to interpolate between the two). However, you need to communicate this support to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> so it does not perform morph animation in software for you. You do this by adding the following attribute to your vertex_program definition:</p>
<div class="fragment"><div class="line">includes_morph_animation <span class="keyword">true</span></div></div><!-- fragment --><p>When you do this, any skeletally animated entity which uses this material will forgo the usual software morph and will expect the vertex program to do it. Note that if your model includes both skeletal animation and morph animation, they must both be implemented in the vertex program if either is to be hardware acceleration. Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Pose-Animation-in-Vertex-Programs"></a>
Pose Animation in Vertex Programs</h1>
<p>You can implement pose animation (blending between multiple poses based on weight) in a vertex program by pulling in the original vertex data (bound to position), and as many pose offset buffers as you’ve defined in your ’includes_pose_animation’ declaration, which will be in the first free texture unit upwards. You must also use the animation_parametric parameter to define the starting point of the constants which will contain the pose weights; they will start at the parameter you define and fill ’n’ constants, where ’n’ is the max number of poses this shader can blend, i.e. the parameter to includes_pose_animation.</p>
<div class="fragment"><div class="line">includes_pose_animation 4</div></div><!-- fragment --><p>Note that ALL submeshes must be assigned a material which implements this, and that if you combine skeletal animation with vertex animation (See <a href="#Animation">Animation</a>) then all techniques must be hardware accelerated for any to be.</p>
<h1><a class="anchor" id="Vertex-texture-fetching-in-vertex-programs"></a>
Vertex texture fetching in vertex programs</h1>
<p>If your vertex program makes use of <a href="#Vertex-Texture-Fetch">Vertex Texture Fetch</a>, you should declare that with the ’uses_vertex_texture_fetch’ directive. This is enough to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that your program uses this feature and that hardware support for it should be checked.</p>
<div class="fragment"><div class="line">uses_vertex_texture_fetch <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Adjacency-information-in-Geometry-Programs"></a>
Adjacency information in Geometry Programs</h1>
<p>Some geometry programs require adjacency information from the geometry. It means that a geometry shader doesn’t only get the information of the primitive it operates on, it also has access to its neighbours (in the case of lines or triangles). This directive will tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to send the information to the geometry shader.</p>
<div class="fragment"><div class="line">uses_adjacency_information <span class="keyword">true</span></div></div><!-- fragment --><h1><a class="anchor" id="Vertex-Texture-Fetch"></a>
Vertex Texture Fetch</h1>
<p>More recent generations of video card allow you to perform a read from a texture in the vertex program rather than just the fragment program, as is traditional. This allows you to, for example, read the contents of a texture and displace vertices based on the intensity of the colour contained within.</p>
<p><a class="anchor" id="Declaring-the-use-of-vertex-texture-fetching"></a></p>
<h2>Declaring the use of vertex texture fetching</h2>
<p>Since hardware support for vertex texture fetching is not ubiquitous, you should use the uses_vertex_texture_fetch (See <a href="#Vertex-texture-fetching-in-vertex-programs">Vertex texture fetching in vertex programs</a>) directive when declaring your vertex programs which use vertex textures, so that if it is not supported, technique fallback can be enabled. This is not strictly necessary for DirectX-targeted shaders, since vertex texture fetching is only supported in vs_3_0, which can be stated as a required syntax in your shader definition, but for OpenGL (GLSL), there are cards which support GLSL but not vertex textures, so you should be explicit about your need for them.</p>
<p><a class="anchor" id="Render-system-texture-binding-differences"></a></p>
<h2>Render system texture binding differences</h2>
<p>Unfortunately the method for binding textures so that they are available to a vertex program is not well standardised. As at the time of writing, Shader Model 3.0 (SM3.0) hardware under DirectX9 include 4 separate sampler bindings for the purposes of vertex textures. OpenGL, on the other hand, is able to access vertex textures in GLSL (and in assembler through NV_vertex_program_3, although this is less popular), but the textures are shared with the fragment pipeline. I expect DirectX to move to the GL model with the advent of DirectX10, since a unified shader architecture implies sharing of texture resources between the two stages. As it is right now though, we’re stuck with an inconsistent situation.</p>
<p>To reflect this, you should use the <a href="#binding_005ftype">binding_type</a> attribute in a texture unit to indicate which unit you are targeting with your texture - ’fragment’ (the default) or ’vertex’. For render systems that don’t have separate bindings, this actually does nothing. But for those that do, it will ensure your texture gets bound to the right processing unit.</p>
<p>Note that whilst DirectX9 has separate bindings for the vertex and fragment pipelines, binding a texture to the vertex processing unit still uses up a ’slot’ which is then not available for use in the fragment pipeline. I didn’t manage to find this documented anywhere, but the nVidia samples certainly avoid binding a texture to the same index on both vertex and fragment units, and when I tried to do it, the texture did not appear correctly in the fragment unit, whilst it did as soon as I moved it into the next unit.</p>
<p><a class="anchor" id="Texture-format-limitations"></a></p>
<h2>Texture format limitations</h2>
<p>Again as at the time of writing, the types of texture you can use in a vertex program are limited to 1- or 4-component, full precision floating point formats. In code that equates to PF_FLOAT32_R or PF_FLOAT32_RGBA. No other formats are supported. In addition, the textures must be regular 2D textures (no cube or volume maps) and mipmapping and filtering is not supported, although you can perform filtering in your vertex program if you wish by sampling multiple times.</p>
<p><a class="anchor" id="Hardware-limitations"></a></p>
<h2>Hardware limitations</h2>
<p>As at the time of writing (early Q3 2006), ATI do not support texture fetch in their current crop of cards (Radeon X1n00). nVidia do support it in both their 6n00 and 7n00 range. ATI support an alternative called ’Render to Vertex Buffer’, but this is not standardised at this time and is very much different in its implementation, so cannot be considered to be a drop-in replacement. This is the case even though the Radeon X1n00 cards claim to support vs_3_0 (which requires vertex texture fetch). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Mesh Tools</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_mesh-_tools.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Mesh Tools </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>There are a number of mesh tools available with OGRE to help you manipulate your meshes.</p>
<dl compact="compact">
<dt><a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> </dt>
<dd><p class="startdd"></p>
<p>For getting data out of modellers and into OGRE.</p>
<p class="enddd"></p>
</dd>
<dt><a href="#XMLConverter">XMLConverter</a> </dt>
<dd><p class="startdd"></p>
<p>For converting meshes and skeletons to/from XML.</p>
<p class="enddd"></p>
</dd>
<dt><a href="#MeshUpgrader">MeshUpgrader</a> </dt>
<dd><p class="startdd"></p>
<p>For upgrading binary meshes from one version of OGRE to another.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Exporters"></a>
Exporters</h1>
<p>Exporters are plugins to 3D modelling tools which write meshes and skeletal animation to file formats which OGRE can use for realtime rendering. The files the exporters write end in .mesh and .skeleton respectively.</p>
<p>Each exporter has to be written specifically for the modeller in question, although they all use a common set of facilities provided by the classes MeshSerializer and SkeletonSerializer. They also normally require you to own the modelling tool.</p>
<p>All the exporters here can be built from the source code, or you can download precompiled versions from the OGRE web site.</p>
<h1>A Note About Modelling / Animation For OGRE</h1>
<p>There are a few rules when creating an animated model for OGRE:</p>
<ul>
<li>You must have no more than 4 weighted bone assignments per vertex. If you have more, OGRE will eliminate the lowest weighted assignments and re-normalise the other weights. This limit is imposed by hardware blending limitations.</li>
<li>All vertices must be assigned to at least one bone - assign static vertices to the root bone.</li>
<li>At the very least each bone must have a keyframe at the beginning and end of the animation.</li>
</ul>
<p>If you’re creating non-animated meshes, then you do not need to be concerned with the above.</p>
<p>Full documentation for each exporter is provided along with the exporter itself, and there is a list of the currently supported modelling tools in the OGRE Wiki at <a href="http://www.ogre3d.org/tikiwiki/tiki-index.php?page=OGRE+Exporters&structure=Tools">http://www.ogre3d.org/tikiwiki/tiki-index.php?page=OGRE+Exporters&amp;structure=Tools</a>.</p>
<h1><a class="anchor" id="XMLConverter"></a>
XMLConverter</h1>
<p>The OgreXMLConverter tool can converter binary .mesh and .skeleton files to XML and back again - this is a very useful tool for debugging the contents of meshes, or for exchanging mesh data easily - many of the modeller mesh exporters export to XML because it is simpler to do, and OgreXMLConverter can then produce a binary from it. Other than simplicity, the other advantage is that OgreXMLConverter can generate additional information for the mesh, like bounding regions and level-of-detail reduction.</p>
<p>Syntax:</p>
<div class="fragment"><div class="line">Usage: OgreXMLConverter sourcefile [destfile]</div><div class="line">sourcefile = name of file to convert</div><div class="line">destfile   = optional name of file to write to. If you don&#39;t</div><div class="line">             specify this OGRE works it out through the extension</div><div class="line">             and the XML contents if the source is XML. For example</div><div class="line">             test.mesh becomes test.xml, test.xml becomes test.mesh</div><div class="line">             if the XML document root is &lt;mesh&gt; etc.</div></div><!-- fragment --><p>When converting XML to .mesh, you will be prompted to (re)generate level-of-detail(LOD) information for the mesh - you can choose to skip this part if you wish, but doing it will allow you to make your mesh reduce in detail automatically when it is loaded into the engine. The engine uses a complex algorithm to determine the best parts of the mesh to reduce in detail depending on many factors such as the curvature of the surface, the edges of the mesh and seams at the edges of textures and smoothing groups - taking advantage of it is advised to make your meshes more scalable in real scenes.</p>
<h1><a class="anchor" id="MeshUpgrader"></a>
MeshUpgrader</h1>
<p>This tool is provided to allow you to upgrade your meshes when the binary format changes - sometimes we alter it to add new features and as such you need to keep your own assets up to date. This tools has a very simple syntax:</p>
<div class="fragment"><div class="line">OgreMeshUpgrader &lt;oldmesh&gt; &lt;newmesh&gt;</div></div><!-- fragment --><p>The OGRE release notes will notify you when this is necessary with a new release. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Mesh Tools</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_mesh-_tools.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Mesh Tools </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>There are a number of mesh tools available with OGRE to help you manipulate your meshes.</p>
<dl compact="compact">
<dt><a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> </dt>
<dd><p class="startdd"></p>
<p>For getting data out of modellers and into OGRE.</p>
<p class="enddd"></p>
</dd>
<dt><a href="#XMLConverter">XMLConverter</a> </dt>
<dd><p class="startdd"></p>
<p>For converting meshes and skeletons to/from XML.</p>
<p class="enddd"></p>
</dd>
<dt><a href="#MeshUpgrader">MeshUpgrader</a> </dt>
<dd><p class="startdd"></p>
<p>For upgrading binary meshes from one version of OGRE to another.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Exporters"></a>
Exporters</h1>
<p>Exporters are plugins to 3D modelling tools which write meshes and skeletal animation to file formats which OGRE can use for realtime rendering. The files the exporters write end in .mesh and .skeleton respectively.</p>
<p>Each exporter has to be written specifically for the modeller in question, although they all use a common set of facilities provided by the classes MeshSerializer and SkeletonSerializer. They also normally require you to own the modelling tool.</p>
<p>All the exporters here can be built from the source code, or you can download precompiled versions from the OGRE web site.</p>
<h1>A Note About Modelling / Animation For OGRE</h1>
<p>There are a few rules when creating an animated model for OGRE:</p>
<ul>
<li>You must have no more than 4 weighted bone assignments per vertex. If you have more, OGRE will eliminate the lowest weighted assignments and re-normalise the other weights. This limit is imposed by hardware blending limitations.</li>
<li>All vertices must be assigned to at least one bone - assign static vertices to the root bone.</li>
<li>At the very least each bone must have a keyframe at the beginning and end of the animation.</li>
</ul>
<p>If you’re creating non-animated meshes, then you do not need to be concerned with the above.</p>
<p>Full documentation for each exporter is provided along with the exporter itself, and there is a list of the currently supported modelling tools in the OGRE Wiki at <a href="http://www.ogre3d.org/tikiwiki/tiki-index.php?page=OGRE+Exporters&structure=Tools">http://www.ogre3d.org/tikiwiki/tiki-index.php?page=OGRE+Exporters&amp;structure=Tools</a>.</p>
<h1><a class="anchor" id="XMLConverter"></a>
XMLConverter</h1>
<p>The OgreXMLConverter tool can converter binary .mesh and .skeleton files to XML and back again - this is a very useful tool for debugging the contents of meshes, or for exchanging mesh data easily - many of the modeller mesh exporters export to XML because it is simpler to do, and OgreXMLConverter can then produce a binary from it. Other than simplicity, the other advantage is that OgreXMLConverter can generate additional information for the mesh, like bounding regions and level-of-detail reduction.</p>
<p>Syntax:</p>
<div class="fragment"><div class="line">Usage: OgreXMLConverter sourcefile [destfile]</div><div class="line">sourcefile = name of file to convert</div><div class="line">destfile   = optional name of file to write to. If you don&#39;t</div><div class="line">             specify this OGRE works it out through the extension</div><div class="line">             and the XML contents if the source is XML. For example</div><div class="line">             test.mesh becomes test.xml, test.xml becomes test.mesh</div><div class="line">             if the XML document root is &lt;mesh&gt; etc.</div></div><!-- fragment --><p>When converting XML to .mesh, you will be prompted to (re)generate level-of-detail(LOD) information for the mesh - you can choose to skip this part if you wish, but doing it will allow you to make your mesh reduce in detail automatically when it is loaded into the engine. The engine uses a complex algorithm to determine the best parts of the mesh to reduce in detail depending on many factors such as the curvature of the surface, the edges of the mesh and seams at the edges of textures and smoothing groups - taking advantage of it is advised to make your meshes more scalable in real scenes.</p>
<h1><a class="anchor" id="MeshUpgrader"></a>
MeshUpgrader</h1>
<p>This tool is provided to allow you to upgrade your meshes when the binary format changes - sometimes we alter it to add new features and as such you need to keep your own assets up to date. This tools has a very simple syntax:</p>
<div class="fragment"><div class="line">OgreMeshUpgrader &lt;oldmesh&gt; &lt;newmesh&gt;</div></div><!-- fragment --><p>The OGRE release notes will notify you when this is necessary with a new release. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Mesh Tools</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_mesh-_tools.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Mesh Tools </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>There are a number of mesh tools available with OGRE to help you manipulate your meshes.</p>
<dl compact="compact">
<dt><a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> </dt>
<dd><p class="startdd"></p>
<p>For getting data out of modellers and into OGRE.</p>
<p class="enddd"></p>
</dd>
<dt><a href="#XMLConverter">XMLConverter</a> </dt>
<dd><p class="startdd"></p>
<p>For converting meshes and skeletons to/from XML.</p>
<p class="enddd"></p>
</dd>
<dt><a href="#MeshUpgrader">MeshUpgrader</a> </dt>
<dd><p class="startdd"></p>
<p>For upgrading binary meshes from one version of OGRE to another.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Exporters"></a>
Exporters</h1>
<p>Exporters are plugins to 3D modelling tools which write meshes and skeletal animation to file formats which OGRE can use for realtime rendering. The files the exporters write end in .mesh and .skeleton respectively.</p>
<p>Each exporter has to be written specifically for the modeller in question, although they all use a common set of facilities provided by the classes MeshSerializer and SkeletonSerializer. They also normally require you to own the modelling tool.</p>
<p>All the exporters here can be built from the source code, or you can download precompiled versions from the OGRE web site.</p>
<h1>A Note About Modelling / Animation For OGRE</h1>
<p>There are a few rules when creating an animated model for OGRE:</p>
<ul>
<li>You must have no more than 4 weighted bone assignments per vertex. If you have more, OGRE will eliminate the lowest weighted assignments and re-normalise the other weights. This limit is imposed by hardware blending limitations.</li>
<li>All vertices must be assigned to at least one bone - assign static vertices to the root bone.</li>
<li>At the very least each bone must have a keyframe at the beginning and end of the animation.</li>
</ul>
<p>If you’re creating non-animated meshes, then you do not need to be concerned with the above.</p>
<p>Full documentation for each exporter is provided along with the exporter itself, and there is a list of the currently supported modelling tools in the OGRE Wiki at <a href="http://www.ogre3d.org/tikiwiki/tiki-index.php?page=OGRE+Exporters&structure=Tools">http://www.ogre3d.org/tikiwiki/tiki-index.php?page=OGRE+Exporters&amp;structure=Tools</a>.</p>
<h1><a class="anchor" id="XMLConverter"></a>
XMLConverter</h1>
<p>The OgreXMLConverter tool can converter binary .mesh and .skeleton files to XML and back again - this is a very useful tool for debugging the contents of meshes, or for exchanging mesh data easily - many of the modeller mesh exporters export to XML because it is simpler to do, and OgreXMLConverter can then produce a binary from it. Other than simplicity, the other advantage is that OgreXMLConverter can generate additional information for the mesh, like bounding regions and level-of-detail reduction.</p>
<p>Syntax:</p>
<div class="fragment"><div class="line">Usage: OgreXMLConverter sourcefile [destfile]</div><div class="line">sourcefile = name of file to convert</div><div class="line">destfile   = optional name of file to write to. If you don&#39;t</div><div class="line">             specify this OGRE works it out through the extension</div><div class="line">             and the XML contents if the source is XML. For example</div><div class="line">             test.mesh becomes test.xml, test.xml becomes test.mesh</div><div class="line">             if the XML document root is &lt;mesh&gt; etc.</div></div><!-- fragment --><p>When converting XML to .mesh, you will be prompted to (re)generate level-of-detail(LOD) information for the mesh - you can choose to skip this part if you wish, but doing it will allow you to make your mesh reduce in detail automatically when it is loaded into the engine. The engine uses a complex algorithm to determine the best parts of the mesh to reduce in detail depending on many factors such as the curvature of the surface, the edges of the mesh and seams at the edges of textures and smoothing groups - taking advantage of it is advised to make your meshes more scalable in real scenes.</p>
<h1><a class="anchor" id="MeshUpgrader"></a>
MeshUpgrader</h1>
<p>This tool is provided to allow you to upgrade your meshes when the binary format changes - sometimes we alter it to add new features and as such you need to keep your own assets up to date. This tools has a very simple syntax:</p>
<div class="fragment"><div class="line">OgreMeshUpgrader &lt;oldmesh&gt; &lt;newmesh&gt;</div></div><!-- fragment --><p>The OGRE release notes will notify you when this is necessary with a new release. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Mesh Tools</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_mesh-_tools.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Mesh Tools </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>There are a number of mesh tools available with OGRE to help you manipulate your meshes.</p>
<dl compact="compact">
<dt><a class="el" href="_mesh-_tools.html#Exporters">Exporters</a> </dt>
<dd><p class="startdd"></p>
<p>For getting data out of modellers and into OGRE.</p>
<p class="enddd"></p>
</dd>
<dt><a href="#XMLConverter">XMLConverter</a> </dt>
<dd><p class="startdd"></p>
<p>For converting meshes and skeletons to/from XML.</p>
<p class="enddd"></p>
</dd>
<dt><a href="#MeshUpgrader">MeshUpgrader</a> </dt>
<dd><p class="startdd"></p>
<p>For upgrading binary meshes from one version of OGRE to another.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Exporters"></a>
Exporters</h1>
<p>Exporters are plugins to 3D modelling tools which write meshes and skeletal animation to file formats which OGRE can use for realtime rendering. The files the exporters write end in .mesh and .skeleton respectively.</p>
<p>Each exporter has to be written specifically for the modeller in question, although they all use a common set of facilities provided by the classes MeshSerializer and SkeletonSerializer. They also normally require you to own the modelling tool.</p>
<p>All the exporters here can be built from the source code, or you can download precompiled versions from the OGRE web site.</p>
<h1>A Note About Modelling / Animation For OGRE</h1>
<p>There are a few rules when creating an animated model for OGRE:</p>
<ul>
<li>You must have no more than 4 weighted bone assignments per vertex. If you have more, OGRE will eliminate the lowest weighted assignments and re-normalise the other weights. This limit is imposed by hardware blending limitations.</li>
<li>All vertices must be assigned to at least one bone - assign static vertices to the root bone.</li>
<li>At the very least each bone must have a keyframe at the beginning and end of the animation.</li>
</ul>
<p>If you’re creating non-animated meshes, then you do not need to be concerned with the above.</p>
<p>Full documentation for each exporter is provided along with the exporter itself, and there is a list of the currently supported modelling tools in the OGRE Wiki at <a href="http://www.ogre3d.org/tikiwiki/tiki-index.php?page=OGRE+Exporters&structure=Tools">http://www.ogre3d.org/tikiwiki/tiki-index.php?page=OGRE+Exporters&amp;structure=Tools</a>.</p>
<h1><a class="anchor" id="XMLConverter"></a>
XMLConverter</h1>
<p>The OgreXMLConverter tool can converter binary .mesh and .skeleton files to XML and back again - this is a very useful tool for debugging the contents of meshes, or for exchanging mesh data easily - many of the modeller mesh exporters export to XML because it is simpler to do, and OgreXMLConverter can then produce a binary from it. Other than simplicity, the other advantage is that OgreXMLConverter can generate additional information for the mesh, like bounding regions and level-of-detail reduction.</p>
<p>Syntax:</p>
<div class="fragment"><div class="line">Usage: OgreXMLConverter sourcefile [destfile]</div><div class="line">sourcefile = name of file to convert</div><div class="line">destfile   = optional name of file to write to. If you don&#39;t</div><div class="line">             specify this OGRE works it out through the extension</div><div class="line">             and the XML contents if the source is XML. For example</div><div class="line">             test.mesh becomes test.xml, test.xml becomes test.mesh</div><div class="line">             if the XML document root is &lt;mesh&gt; etc.</div></div><!-- fragment --><p>When converting XML to .mesh, you will be prompted to (re)generate level-of-detail(LOD) information for the mesh - you can choose to skip this part if you wish, but doing it will allow you to make your mesh reduce in detail automatically when it is loaded into the engine. The engine uses a complex algorithm to determine the best parts of the mesh to reduce in detail depending on many factors such as the curvature of the surface, the edges of the mesh and seams at the edges of textures and smoothing groups - taking advantage of it is advised to make your meshes more scalable in real scenes.</p>
<h1><a class="anchor" id="MeshUpgrader"></a>
MeshUpgrader</h1>
<p>This tool is provided to allow you to upgrade your meshes when the binary format changes - sometimes we alter it to add new features and as such you need to keep your own assets up to date. This tools has a very simple syntax:</p>
<div class="fragment"><div class="line">OgreMeshUpgrader &lt;oldmesh&gt; &lt;newmesh&gt;</div></div><!-- fragment --><p>The OGRE release notes will notify you when this is necessary with a new release. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Hardware Buffers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_hardware-_buffers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Hardware Buffers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Buffer-Usage">Buffer Usage</a><ul><li class="level2"><a href="#Shadow-Buffers">Shadow Buffers</a></li>
<li class="level2"><a href="#Locking-buffers">Locking buffers</a></li>
<li class="level2"><a href="#Practical-Buffer-Tips">Practical Buffer Tips</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a><ul><li class="level2"><a href="#The-VertexData-class">The VertexData class</a></li>
<li class="level2"><a href="#Vertex-Declarations">Vertex Declarations</a></li>
<li class="level2"><a href="#Important-Considerations">Important Considerations</a></li>
<li class="level2"><a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></li>
<li class="level2"><a href="#Creating-the-Vertex-Buffer">Creating the Vertex Buffer</a></li>
<li class="level2"><a href="#Binding-the-Vertex-Buffer">Binding the Vertex Buffer</a></li>
<li class="level2"><a href="#Updating-Vertex-Buffers">Updating Vertex Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Index-Buffers">Hardware Index Buffers</a><ul><li class="level2"><a href="#The-IndexData-class">The IndexData class</a></li>
<li class="level2"><a href="#Updating-Index-Buffers">Updating Index Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#Hardware-Pixel-Buffers">Hardware Pixel Buffers</a><ul><li class="level2"><a href="#Textures">Textures</a><ul><li class="level3"><a href="#Creating-a-texture">Creating a texture</a></li>
</ul>
</li>
<li class="level2"><a href="#Getting-a-PixelBuffer">Getting a PixelBuffer</a></li>
<li class="level2"><a href="#Updating-Pixel-Buffers">Updating Pixel Buffers</a></li>
<li class="level2"><a href="#blitFromMemory">Blit from memory</a></li>
<li class="level2"><a href="#Direct-memory-locking">Direct memory locking</a></li>
<li class="level2"><a href="#Texture-Types">Texture Types</a></li>
<li class="level2"><a href="#Cube-map-textures">Cube map textures</a></li>
<li class="level2"><a href="#Pixel-Formats">Pixel Formats</a></li>
<li class="level2"><a href="#Colour-channels">Colour channels</a></li>
<li class="level2"><a href="#Complete-list-of-pixel-formats">List of pixel formats</a></li>
<li class="level2"><a href="#Pixel-boxes">Pixel boxes</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>Vertex buffers, index buffers and pixel buffers inherit most of their features from the HardwareBuffer class. The general premise with a hardware buffer is that it is an area of memory with which you can do whatever you like; there is no format (vertex or otherwise) associated with the buffer itself - that is entirely up to interpretation by the methods that use it - in that way, a HardwareBuffer is just like an area of memory you might allocate using ’malloc’ - the difference being that this memory is likely to be located in GPU or AGP memory.</p>
<p><a class="anchor" id="The-Hardware-Buffer-Manager"></a> <a class="anchor" id="The-Hardware-Buffer-Manager-1"></a></p>
<h1>The Hardware Buffer Manager</h1>
<p>The HardwareBufferManager class is the factory hub of all the objects in the new geometry system. You create and destroy the majority of the objects you use to define geometry through this class. It’s a Singleton, so you access it by doing HardwareBufferManager::getSingleton() - however be aware that it is only guaranteed to exist after the RenderSystem has been initialised (after you call Root::initialise); this is because the objects created are invariably API-specific, although you will deal with them through one common interface. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_vertex_declaration.html">Ogre::VertexDeclaration</a>* decl = HardwareBufferManager::getSingleton().createVertexDeclaration();</div></div><!-- fragment --><div class="fragment"><div class="line"><a class="code" href="class_ogre_1_1_shared_ptr.html">Ogre::HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        <a class="code" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630a5ffbb339becd31435c5f616be4c28996">Ogre::HardwareBuffer::HBU_STATIC_WRITE_ONLY</a>, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Don’t worry about the details of the above, we’ll cover that in the later sections. The important thing to remember is to always create objects through the HardwareBufferManager, don’t use ’new’ (it won’t work anyway in most cases).</p>
<h1><a class="anchor" id="Buffer-Usage"></a>
Buffer Usage</h1>
<p>Because the memory in a hardware buffer is likely to be under significant contention during the rendering of a scene, the kind of access you need to the buffer over the time it is used is extremely important; whether you need to update the contents of the buffer regularly, whether you need to be able to read information back from it, these are all important factors to how the graphics card manages the buffer. The method and exact parameters used to create a buffer depends on whether you are creating an index or vertex buffer (See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a>), however one creation parameter is common to them both - the ’usage’. The most optimal type of hardware buffer is one which is not updated often, and is never read from. The usage parameter of createVertexBuffer or createIndexBuffer can be one of <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a>.</p>
<p>Choosing the usage of your buffers carefully is important to getting optimal performance out of your geometry. If you have a situation where you need to update a vertex buffer often, consider whether you actually need to update <b>all</b> the parts of it, or just some. If it’s the latter, consider using more than one buffer, with only the data you need to modify in the HBU_DYNAMIC buffer. Always try to use the _WRITE_ONLY forms. This just means that you cannot read <em>directly</em> from the hardware buffer, which is good practice because reading from hardware buffers is very slow. If you really need to read data back, use a shadow buffer, described in the next section.</p>
<h2><a class="anchor" id="Shadow-Buffers"></a>
Shadow Buffers</h2>
<p>As discussed in the previous section, reading data back from a hardware buffer performs very badly. However, if you have a cast-iron need to read the contents of the vertex buffer, you should set the ’shadowBuffer’ parameter of createVertexBuffer or createIndexBuffer to ’true’. This causes the hardware buffer to be backed with a system memory copy, which you can read from with no more penalty than reading ordinary memory. The catch is that when you write data into this buffer, it will first update the system memory copy, then it will update the hardware buffer, as separate copying process - therefore this technique has an additional overhead when writing data. Don’t use it unless you really need it.</p>
<h2><a class="anchor" id="Locking-buffers"></a>
Locking buffers</h2>
<p>In order to read or update a hardware buffer, you have to ’lock’ it. This performs 2 functions - it tells the card that you want access to the buffer (which can have an effect on its rendering queue), and it returns a pointer which you can manipulate. Note that if you’ve asked to read the buffer (and remember, you really shouldn’t unless you’ve set the buffer up with a shadow buffer), the contents of the hardware buffer will have been copied into system memory somewhere in order for you to get access to it. For the same reason, when you’re finished with the buffer you must unlock it; if you locked the buffer for writing this will trigger the process of uploading the modified information to the graphics hardware.</p>
<p><a class="anchor" id="Lock-parameters"></a></p>
<h3>Lock parameters</h3>
<p>When you lock a buffer, you call one of the following methods:</p>
<div class="fragment"><div class="line"><span class="comment">// Lock the entire buffer</span></div><div class="line">pBuffer-&gt;lock(lockType);</div><div class="line"><span class="comment">// Lock only part of the buffer</span></div><div class="line">pBuffer-&gt;lock(start, length, lockType);</div></div><!-- fragment --><p>The first call locks the entire buffer, the second locks only the section from ’start’ (as a byte offset), for ’length’ bytes. This could be faster than locking the entire buffer since less is transferred, but not if you later update the rest of the buffer too, because doing it in small chunks like this means you cannot use <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0fa176ad39ac15fc0a6028d820a6fe228ae" title="Discards the entire buffer while locking. ">Ogre::HardwareBuffer::HBL_DISCARD</a>. The lockType parameter can have a large effect on the performance of your application, especially if you are not using a shadow buffer. See <a class="el" href="class_ogre_1_1_hardware_buffer.html#aac34930556f611928ec9157c04bd1b0f" title="Locking options. ">Ogre::HardwareBuffer::LockOptions</a>.</p>
<p>Once you have locked a buffer, you can use the pointer returned however you wish (just don’t bother trying to read the data that’s there if you’ve used HBL_DISCARD, or write the data if you’ve used HBL_READ_ONLY). Modifying the contents depends on the type of buffer, See <a href="#Hardware-Vertex-Buffers">Hardware Vertex Buffers</a> and See <a href="#Hardware-Index-Buffers">Hardware Index Buffers</a></p>
<h2><a class="anchor" id="Practical-Buffer-Tips"></a>
Practical Buffer Tips</h2>
<p>The interplay of usage mode on creation, and locking options when reading / updating is important for performance. Here’s some tips:</p>
<ol type="1">
<li>Aim for the ’perfect’ buffer by creating with HBU_STATIC_WRITE_ONLY, with no shadow buffer, and locking all of it once only with HBL_DISCARD to populate it. Never touch it again.</li>
<li>If you need to update a buffer regularly, you will have to compromise. Use HBU_DYNAMIC_WRITE_ONLY when creating (still no shadow buffer), and use HBL_DISCARD to lock the entire buffer, or if you can’t then use HBL_NO_OVERWRITE to lock parts of it.</li>
<li>If you really need to read data from the buffer, create it with a shadow buffer. Make sure you use HBL_READ_ONLY when locking for reading because it will avoid the upload normally associated with unlocking the buffer. You can also combine this with either of the 2 previous points, obviously try for static if you can - remember that the _WRITE_ONLY’ part refers to the hardware buffer so can be safely used with a shadow buffer you read from.</li>
<li>Split your vertex buffers up if you find that your usage patterns for different elements of the vertex are different. No point having one huge updatable buffer with all the vertex data in it, if all you need to update is the texture coordinates. Split that part out into it’s own buffer and make the rest HBU_STATIC_WRITE_ONLY.</li>
</ol>
<h1><a class="anchor" id="Hardware-Vertex-Buffers"></a>
Hardware Vertex Buffers</h1>
<p>This section covers specialised hardware buffers which contain vertex data. For a general discussion of hardware buffers, along with the rules for creating and locking them, see the <a href="#Hardware-Buffers">Hardware Buffers</a> section.</p>
<h2><a class="anchor" id="The-VertexData-class"></a>
The VertexData class</h2>
<p>The <a class="el" href="class_ogre_1_1_vertex_data.html" title="Summary class collecting together vertex source information. ">Ogre::VertexData</a> class collects together all the vertex-related information used to render geometry. The new RenderOperation requires a pointer to a VertexData object, and it is also used in Mesh and SubMesh to store the vertex positions, normals, texture coordinates etc. VertexData can either be used alone (in order to render unindexed geometry, where the stream of vertices defines the triangles), or in combination with IndexData where the triangles are defined by indexes which refer to the entries in VertexData. It’s worth noting that you don’t necessarily have to use VertexData to store your applications geometry; all that is required is that you can build a VertexData structure when it comes to rendering. This is pretty easy since all of VertexData’s members are pointers, so you could maintain your vertex buffers and declarations in alternative structures if you like, so long as you can convert them for rendering. The VertexData class has a number of important members:</p>
<dl compact="compact">
<dt>vertexStart </dt>
<dd><p class="startdd"></p>
<p>The position in the bound buffers to start reading vertex data from. This allows you to use a single buffer for many different renderables.</p>
<p class="enddd"></p>
</dd>
<dt>vertexCount </dt>
<dd><p class="startdd"></p>
<p>The number of vertices to process in this particular rendering group</p>
<p class="enddd"></p>
</dd>
<dt>vertexDeclaration </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexDeclaration object which defines the format of the vertex input; note this is created for you by VertexData. See <a href="#Vertex-Declarations">Vertex Declarations</a></p>
<p class="enddd"></p>
</dd>
<dt>vertexBufferBinding </dt>
<dd><p class="startdd"></p>
<p>A pointer to a VertexBufferBinding object which defines which vertex buffers are bound to which sources - again, this is created for you by VertexData. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Vertex-Declarations"></a>
Vertex Declarations</h2>
<p>Vertex declarations define the vertex inputs used to render the geometry you want to appear on the screen. Basically this means that for each vertex, you want to feed a certain set of data into the graphics pipeline, which (you hope) will affect how it all looks when the triangles are drawn. Vertex declarations let you pull items of data (which we call vertex elements, represented by the VertexElement class) from any number of buffers, both shared and dedicated to that particular element. It’s your job to ensure that the contents of the buffers make sense when interpreted in the way that your VertexDeclaration indicates that they should. To add an element to a VertexDeclaration, you call <a class="el" href="class_ogre_1_1_vertex_declaration.html#a8b1a5f5fe0b7306da4c8b17357d45fac" title="Adds a new VertexElement to this declaration. ">Ogre::VertexDeclaration::addElement</a> method. The parameters to this method are:</p>
<dl compact="compact">
<dt>source </dt>
<dd><p class="startdd"></p>
<p>This tells the declaration which buffer the element is to be pulled from. Note that this is just an index, which may range from 0 to one less than the number of buffers which are being bound as sources of vertex data. See <a href="#Vertex-Buffer-Bindings">Vertex Buffer Bindings</a> for information on how a real buffer is bound to a source index. Storing the source of the vertex element this way (rather than using a buffer pointer) allows you to rebind the source of a vertex very easily, without changing the declaration of the vertex format itself.</p>
<p class="enddd"></p>
</dd>
<dt>offset </dt>
<dd><p class="startdd"></p>
<p>Tells the declaration how far in bytes the element is offset from the start of each whole vertex in this buffer. This will be 0 if this is the only element being sourced from this buffer, but if other elements are there then it may be higher. A good way of thinking of this is the size of all vertex elements which precede this element in the buffer.</p>
<p class="enddd"></p>
</dd>
<dt>type </dt>
<dd><p class="startdd"></p>
<p>This defines the data type of the vertex input, including it’s size. This is an important element because as GPUs become more advanced, we can no longer assume that position input will always require 3 floating point numbers, because programmable vertex pipelines allow full control over the inputs and outputs. This part of the element definition covers the basic type and size, e.g. VET_FLOAT3 is 3 floating point numbers - the meaning of the data is dealt with in the next parameter.</p>
<p class="enddd"></p>
</dd>
<dt>semantic </dt>
<dd><p class="startdd"></p>
<p>This defines the meaning of the element - the GPU will use this to determine what to use this input for, and programmable vertex pipelines will use this to identify which semantic to map the input to. This can identify the element as positional data, normal data, texture coordinate data, etc. See the API reference for full details of all the options.</p>
<p class="enddd"></p>
</dd>
<dt>index </dt>
<dd><p class="startdd"></p>
<p>This parameter is only required when you supply more than one element of the same semantic in one vertex declaration. For example, if you supply more than one set of texture coordinates, you would set first sets index to 0, and the second set to 1.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>You can repeat the call to addElement for as many elements as you have in your vertex input structures. There are also useful methods on VertexDeclaration for locating elements within a declaration - see the API reference for full details.</p>
<h2><a class="anchor" id="Important-Considerations"></a>
Important Considerations</h2>
<p>Whilst in theory you have completely full reign over the format of you vertices, in reality there are some restrictions. Older DirectX hardware imposes a fixed ordering on the elements which are pulled from each buffer; specifically any hardware prior to DirectX 9 may impose the following restrictions:</p>
<ul>
<li>VertexElements should be added in the following order, and the order of the elements within any shared buffer should be as follows:<ol type="1">
<li>Positions</li>
<li>Blending weights</li>
<li>Normals</li>
<li>Diffuse colours</li>
<li>Specular colours</li>
<li>Texture coordinates (starting at 0, listed in order, with no gaps)</li>
</ol>
</li>
<li>You must not have unused gaps in your buffers which are not referenced by any VertexElement</li>
<li>You must not cause the buffer &amp; offset settings of 2 VertexElements to overlap</li>
</ul>
<p>OpenGL and DirectX 9 compatible hardware are not required to follow these strict limitations, so you might find, for example that if you broke these rules your application would run under OpenGL and under DirectX on recent cards, but it is not guaranteed to run on older hardware under DirectX unless you stick to the above rules. For this reason you’re advised to abide by them!</p>
<h2><a class="anchor" id="Vertex-Buffer-Bindings"></a>
Vertex Buffer Bindings</h2>
<p>Vertex buffer bindings are about associating a vertex buffer with a source index used in <a href="#Vertex-Declarations">Vertex Declarations</a>.</p>
<h2><a class="anchor" id="Creating-the-Vertex-Buffer"></a>
Creating the Vertex Buffer</h2>
<p>Firstly, lets look at how you create a vertex buffer:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#a05ef9b65d4ed446b994636e371bf5aab">HardwareVertexBufferSharedPtr</a> vbuf = </div><div class="line">    <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#adee7ce85fcdeefb8b97b37ece2bab8e4">createVertexBuffer</a>(</div><div class="line">        3*<span class="keyword">sizeof</span>(<a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>), <span class="comment">// size of one whole vertex</span></div><div class="line">        numVertices, <span class="comment">// number of vertices</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer</span></div></div><!-- fragment --><p>Notice that we use <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> to create our vertex buffer, and that a class called HardwareVertexBufferSharedPtr is returned from the method, rather than a raw pointer. This is because vertex buffers are reference counted - you are able to use a single vertex buffer as a source for multiple pieces of geometry therefore a standard pointer would not be good enough, because you would not know when all the different users of it had finished with it. The HardwareVertexBufferSharedPtr class manages its own destruction by keeping a reference count of the number of times it is being used - when the last HardwareVertexBufferSharedPtr is destroyed, the buffer itself automatically destroys itself.</p>
<p>The parameters to the creation of a vertex buffer are as follows:</p>
<dl compact="compact">
<dt>vertexSize </dt>
<dd><p class="startdd"></p>
<p>The size in bytes of a whole vertex in this buffer. A vertex may include multiple elements, and in fact the contents of the vertex data may be reinterpreted by different vertex declarations if you wish. Therefore you must tell the buffer manager how large a whole vertex is, but not the internal format of the vertex, since that is down to the declaration to interpret. In the above example, the size is set to the size of 3 floating point values - this would be enough to hold a standard 3D position or normal, or a 3D texture coordinate, per vertex.</p>
<p class="enddd"></p>
</dd>
<dt>numVertices </dt>
<dd><p class="startdd"></p>
<p>The number of vertices in this buffer. Remember, not all the vertices have to be used at once - it can be beneficial to create large buffers which are shared between many chunks of geometry because changing vertex buffer bindings is a render state switch, and those are best minimised.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Binding-the-Vertex-Buffer"></a>
Binding the Vertex Buffer</h2>
<p>The second part of the process is to bind this buffer which you have created to a source index. To do this, you call:</p>
<div class="fragment"><div class="line">vertexBufferBinding-&gt;setBinding(0, vbuf);</div></div><!-- fragment --><p>This results in the vertex buffer you created earlier being bound to source index 0, so any vertex element which is pulling its data from source index 0 will retrieve data from this buffer. There are also methods for retrieving buffers from the binding data - see the API reference for full details.</p>
<h2><a class="anchor" id="Updating-Vertex-Buffers"></a>
Updating Vertex Buffers</h2>
<p>The complexity of updating a vertex buffer entirely depends on how its contents are laid out. You can lock a buffer (See <a href="#Locking-buffers">Locking buffers</a>), but how you write data into it vert much depends on what it contains. Lets start with a vert simple example. Lets say you have a buffer which only contains vertex positions, so it only contains sets of 3 floating point numbers per vertex. In this case, all you need to do to write data into it is:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>... then you just write positions in chunks of 3 reals. If you have other floating point data in there, it’s a little more complex but the principle is largely the same, you just need to write alternate elements. But what if you have elements of different types, or you need to derive how to write the vertex data from the elements themselves? Well, there are some useful methods on the VertexElement class to help you out. Firstly, you lock the buffer but assign the result to a unsigned char* rather than a specific type. Then, for each element which is sourcing from this buffer (which you can find out by calling VertexDeclaration::findElementsBySource) you call VertexElement::baseVertexPointerToElement. This offsets a pointer which points at the base of a vertex in a buffer to the beginning of the element in question, and allows you to use a pointer of the right type to boot. Here’s a full example:</p>
<div class="fragment"><div class="line"><span class="comment">// Get base pointer</span></div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* pVert = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(vbuf-&gt;lock(HardwareBuffer::HBL_READ_ONLY));</div><div class="line"><a class="code" href="namespace_ogre.html#aa3a7b6dfb905e6572d62f0dfa3d4274d">Real</a>* pReal;</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> v = 0; v &lt; vertexCount; ++v)</div><div class="line">{</div><div class="line">    <span class="comment">// Get elements</span></div><div class="line">    VertexDeclaration::VertexElementList elems = decl-&gt;<a class="code" href="class_ogre_1_1_vertex_declaration.html#ada92fbc9ee87b04f8a3c389c71904ce6">findElementsBySource</a>(bufferIdx);</div><div class="line">    VertexDeclaration::VertexElementList::iterator i, iend;</div><div class="line">    <span class="keywordflow">for</span> (i = elems.begin(); i != elems.end(); ++i)</div><div class="line">    {</div><div class="line">        VertexElement&amp; elem = *i;</div><div class="line">        <span class="keywordflow">if</span> (elem.getSemantic() == <a class="code" href="group___render_system.html#gga0a62b3f2ede8992ff365bb013a8bc00dad04d50e91c2ec5377e610b3f5b9b07f2">VES_POSITION</a>)</div><div class="line">        {</div><div class="line">            elem.baseVertexPointerToElement(pVert, &amp;pReal);</div><div class="line">            <span class="comment">// write position using pReal</span></div><div class="line"></div><div class="line">        }</div><div class="line"></div><div class="line">        ...</div><div class="line"></div><div class="line"></div><div class="line">    }</div><div class="line">    pVert += vbuf-&gt;getVertexSize();</div><div class="line">}</div><div class="line">vbuf-&gt;unlock();</div></div><!-- fragment --><p>See the API docs for full details of all the helper methods on VertexDeclaration and VertexElement to assist you in manipulating vertex buffer data pointers.</p>
<h1><a class="anchor" id="Hardware-Index-Buffers"></a>
Hardware Index Buffers</h1>
<p>Index buffers are used to render geometry by building triangles out of vertices indirectly by reference to their position in the buffer, rather than just building triangles by sequentially reading vertices. Index buffers are simpler than vertex buffers, since they are just a list of indexes at the end of the day, however they can be held on the hardware and shared between multiple pieces of geometry in the same way vertex buffers can, so the rules on creation and locking are the same. See <a href="#Hardware-Buffers">Hardware Buffers</a> for information.</p>
<h2><a class="anchor" id="The-IndexData-class"></a>
The IndexData class</h2>
<p>This class summarises the information required to use a set of indexes to render geometry. It’s members are as follows:</p>
<dl compact="compact">
<dt>indexStart </dt>
<dd><p class="startdd"></p>
<p>The first index used by this piece of geometry; this can be useful for sharing a single index buffer among several geometry pieces.</p>
<p class="enddd"></p>
</dd>
<dt>indexCount </dt>
<dd><p class="startdd"></p>
<p>The number of indexes used by this particular renderable.</p>
<p class="enddd"></p>
</dd>
<dt>indexBuffer </dt>
<dd><p class="startdd"></p>
<p>The index buffer which is used to source the indexes.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Creating-an-Index-Buffer"></a></p>
<h2>Creating an Index Buffer</h2>
<p>Index buffers are created using See <a href="#The-Hardware-Buffer-Manager">The Hardware Buffer Manager</a> just like vertex buffers, here’s how:</p>
<div class="fragment"><div class="line"><a class="code" href="namespace_ogre.html#ae906cf8e579ba8ade19879ec05b1a456">HardwareIndexBufferSharedPtr</a> ibuf = <a class="code" href="class_ogre_1_1_hardware_buffer_manager.html#a380063fef4a4563c90d45a03ac196dc7">Ogre::HardwareBufferManager::getSingleton</a>().</div><div class="line">    createIndexBuffer(</div><div class="line">        HardwareIndexBuffer::IT_16BIT, <span class="comment">// type of index</span></div><div class="line">        numIndexes, <span class="comment">// number of indexes</span></div><div class="line">        HardwareBuffer::HBU_STATIC_WRITE_ONLY, <span class="comment">// usage</span></div><div class="line">        <span class="keyword">false</span>); <span class="comment">// no shadow buffer </span></div></div><!-- fragment --><p>Once again, notice that the return type is a class rather than a pointer; this is reference counted so that the buffer is automatically destroyed when no more references are made to it. The parameters to the index buffer creation are:</p>
<dl compact="compact">
<dt>indexType </dt>
<dd><p class="startdd"></p>
<p>There are 2 types of index; 16-bit and 32-bit. They both perform the same way, except that the latter can address larger vertex buffers. If your buffer includes more than 65526 vertices, then you will need to use 32-bit indexes. Note that you should only use 32-bit indexes when you need to, since they incur more overhead than 16-bit vertices, and are not supported on some older hardware.</p>
<p class="enddd"></p>
</dd>
<dt>numIndexes </dt>
<dd><p class="startdd"></p>
<p>The number of indexes in the buffer. As with vertex buffers, you should consider whether you can use a shared index buffer which is used by multiple pieces of geometry, since there can be performance advantages to switching index buffers less often.</p>
<p class="enddd"></p>
</dd>
<dt>usage </dt>
<dd><p class="startdd"></p>
<p>This tells the system how you intend to use the buffer. See <a href="#Buffer-Usage">Buffer Usage</a></p>
<p class="enddd"></p>
</dd>
<dt>useShadowBuffer </dt>
<dd><p class="startdd"></p>
<p>Tells the system whether you want this buffer backed by a system-memory copy. See <a href="#Shadow-Buffers">Shadow Buffers</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Updating-Index-Buffers"></a>
Updating Index Buffers</h2>
<p>Updating index buffers can only be done when you lock the buffer for writing; See <a href="#Locking-buffers">Locking buffers</a> for details. Locking returns a void pointer, which must be cast to the appropriate type; with index buffers this is either an unsigned short (for 16-bit indexes) or an unsigned long (for 32-bit indexes). For example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>* pIdx = <span class="keyword">static_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">short</span>*<span class="keyword">&gt;</span>(ibuf-&gt;lock(HardwareBuffer::HBL_DISCARD));</div></div><!-- fragment --><p>You can then write to the buffer using the usual pointer semantics, just remember to unlock the buffer when you’re finished!</p>
<h1><a class="anchor" id="Hardware-Pixel-Buffers"></a>
Hardware Pixel Buffers</h1>
<p>Hardware Pixel Buffers are a special kind of buffer that stores graphical data in graphics card memory, generally for use as textures. Pixel buffers can represent a one dimensional, two dimensional or three dimensional image. A texture can consist of a multiple of these buffers.</p>
<p>In contrary to vertex and index buffers, pixel buffers are not constructed directly. When creating a texture, the necessary pixel buffers to hold its data are constructed automatically.</p>
<h2><a class="anchor" id="Textures"></a>
Textures</h2>
<p>A texture is an image that can be applied onto the surface of a three dimensional model. In <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>, textures are represented by the Texture resource class.</p>
<h3><a class="anchor" id="Creating-a-texture"></a>
Creating a texture</h3>
<p>Textures are created through the TextureManager. In most cases they are created from image files directly by the <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> resource system. If you are reading this, you most probably want to create a texture manually so that you can provide it with image data yourself. This is done through TextureManager::createManual:</p>
<div class="fragment"><div class="line">ptex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;MyManualTexture&quot;</span>, <span class="comment">// Name of texture</span></div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>, <span class="comment">// Name of resource group in which the texture should be created</span></div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>, <span class="comment">// Texture type</span></div><div class="line">    256, <span class="comment">// Width</span></div><div class="line">    256, <span class="comment">// Height</span></div><div class="line">    1, <span class="comment">// Depth (Must be 1 for two dimensional textures)</span></div><div class="line">    0, <span class="comment">// No mipmaps</span></div><div class="line">    <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca0d5c9bdb0633c1849758fea566a2b754">PF_A8R8G8B8</a>, <span class="comment">// internal Pixel format</span></div><div class="line">    <a class="code" href="group___resources.html#ggaf140ec886884a864abc74a7556f8bf67ac26eda030d21e0491461edea89394493">Ogre::TU_DYNAMIC_WRITE_ONLY</a> <span class="comment">// usage</span></div><div class="line">);</div></div><!-- fragment --><p>This example creates a texture named <em>MyManualTexture</em> in resource group <em>General</em>. It is a square <em>two dimensional</em> texture, with width 256 and height 256.</p>
<p>The different texture types will be discussed in <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>. Pixel formats are summarised in <a href="#Pixel-Formats">Pixel Formats</a>.</p>
<p>In addition to the hardware buffer usages as described in <a class="el" href="class_ogre_1_1_hardware_buffer.html#ae51f5970d879af736e4cf53e1ec46630" title="Enums describing buffer usage; not mutually exclusive. ">Ogre::HardwareBuffer::Usage</a> there are some usage flags specific to textures: <a class="el" href="group___resources.html#gaf140ec886884a864abc74a7556f8bf67" title="Enum identifying the texture usage. ">Ogre::TextureUsage</a>.</p>
<h2><a class="anchor" id="Getting-a-PixelBuffer"></a>
Getting a PixelBuffer</h2>
<p>A Texture can consist of multiple PixelBuffers, one for each combo if mipmap level and face number. To get a PixelBuffer from a Texture object the method Texture::getBuffer(face, mipmap) is used:</p>
<p><em>face</em> should be zero for non-cubemap textures. For cubemap textures it identifies the face to use, which is one of the cube faces described in See <a href="#Texture-Types">Texture Types</a>.</p>
<p><em>mipmap</em> is zero for the zeroth mipmap level, one for the first mipmap level, and so on. On textures that have automatic mipmap generation (TU_AUTOMIPMAP) only level 0 should be accessed, the rest will be taken care of by the rendering API.</p>
<p>A simple example of using getBuffer is</p>
<div class="fragment"><div class="line"><span class="comment">// Get the PixelBuffer for face 0, mipmap 0.</span></div><div class="line"><a class="code" href="namespace_ogre.html#a7786bdc8f60fd946ec3e5ff896276bcf">HardwarePixelBufferSharedPtr</a> ptr = tex-&gt;getBuffer(0,0);</div></div><!-- fragment --><h2><a class="anchor" id="Updating-Pixel-Buffers"></a>
Updating Pixel Buffers</h2>
<p>Pixel Buffers can be updated in two different ways; a simple, convenient way and a more difficult (but in some cases faster) method. Both methods make use of <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> objects to represent image data in memory.</p>
<h2><a class="anchor" id="blitFromMemory"></a>
Blit from memory</h2>
<p>The easy method to get an image into a PixelBuffer is by using HardwarePixelBuffer::blitFromMemory. This takes a PixelBox object and does all necessary pixel format conversion and scaling for you. For example, to create a manual texture and load an image into it, all you have to do is</p>
<div class="fragment"><div class="line"><span class="comment">// Manually loads an image and puts the contents in a manually created texture</span></div><div class="line"><a class="code" href="class_ogre_1_1_image.html">Ogre::Image</a> img;</div><div class="line">img.<a class="code" href="class_ogre_1_1_image.html#a75ced161976ffbb4fc1cf63d4b007993">load</a>(<span class="stringliteral">&quot;elephant.png&quot;</span>, <span class="stringliteral">&quot;General&quot;</span>);</div><div class="line"><span class="comment">// Create RGB texture with 5 mipmaps</span></div><div class="line"><a class="code" href="namespace_ogre.html#aa381890787803da43b614dee8d9c8994">TexturePtr</a> tex = <a class="code" href="class_ogre_1_1_texture_manager.html#ac6fb858ee86f3b7b75de32e3e04ee5c8">Ogre::TextureManager::getSingleton</a>().<a class="code" href="class_ogre_1_1_texture_manager.html#aafa494d2704fdff05708db9db4bac3a0">createManual</a>(</div><div class="line">    <span class="stringliteral">&quot;elephant&quot;</span>,</div><div class="line">    <span class="stringliteral">&quot;General&quot;</span>,</div><div class="line">    <a class="code" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7af1f732018f5a2a7d35233e2ceeb4ee0e">Ogre::TEX_TYPE_2D</a>,</div><div class="line">    img.<a class="code" href="class_ogre_1_1_image.html#a649d36eb8288161540d223b7106b7d67">getWidth</a>(), img.<a class="code" href="class_ogre_1_1_image.html#a064e61e5225c5405d838e2e96a45beed">getHeight</a>(),</div><div class="line">    5, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39ca2542d39f6712cb6f69907330a31a9342">Ogre::PF_X8R8G8B8</a>);</div><div class="line"><span class="comment">// Copy face 0 mipmap 0 of the image to face 0 mipmap 0 of the texture.</span></div><div class="line">tex-&gt;<a class="code" href="class_ogre_1_1_texture.html#a5df1d413ab5aab631adc59c9fac06d22">getBuffer</a>(0,0)-&gt;blitFromMemory(img.<a class="code" href="class_ogre_1_1_image.html#afb8692cfc9e8f50271254722825f6c2b">getPixelBox</a>(0,0));</div></div><!-- fragment --><h2><a class="anchor" id="Direct-memory-locking"></a>
Direct memory locking</h2>
<p>A more advanced method to transfer image data from and to a PixelBuffer is to use locking. By locking a PixelBuffer you can directly access its contents in whatever the internal format of the buffer inside the GPU is.</p>
<div class="fragment"><div class="line">buffer-&gt;lock(HardwareBuffer::HBL_DISCARD);</div><div class="line"><span class="keyword">const</span> <a class="code" href="class_ogre_1_1_pixel_box.html">Ogre::PixelBox</a> &amp;pb = buffer-&gt;getCurrentLock();</div><div class="line"></div><div class="line"><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a> *data = <span class="keyword">static_cast&lt;</span><a class="code" href="namespace_ogre.html#adf2a0cf2e77143b5ee1d38a75e289545">uint32</a>*<span class="keyword">&gt;</span>(pb.<a class="code" href="class_ogre_1_1_pixel_box.html#ac4fe215e13fc38d6a45e0c40ee2f1425">data</a>);</div><div class="line"><span class="keywordtype">size_t</span> height = pb.<a class="code" href="struct_ogre_1_1_box.html#a352d26dea0069f2e7f1414e8caa2328c">getHeight</a>();</div><div class="line"><span class="keywordtype">size_t</span> width = pb.<a class="code" href="struct_ogre_1_1_box.html#a5499449ff454ff98aaa697f8854376fc">getWidth</a>();</div><div class="line"><span class="keywordtype">size_t</span> pitch = pb.<a class="code" href="class_ogre_1_1_pixel_box.html#aacf8f1ebed40c557389d9d1e9dda3b68">rowPitch</a>; <span class="comment">// Skip between rows of image</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> y=0; y&lt;height; ++y)</div><div class="line">{</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">size_t</span> x=0; x&lt;width; ++x)</div><div class="line">    {</div><div class="line">        <span class="comment">// 0xRRGGBB -&gt; fill the buffer with yellow pixels</span></div><div class="line">        data[pitch*y + x] = 0x00FFFF00;</div><div class="line">    }</div><div class="line">}</div><div class="line"></div><div class="line">buffer-&gt;unlock();</div></div><!-- fragment --><h2><a class="anchor" id="Texture-Types"></a>
Texture Types</h2>
<p>There are several types of textures supported by current hardware (see <a class="el" href="group___resources.html#gae50520af6377ff8184cf79125cb9c4d7" title="Enum identifying the texture type. ">Ogre::TextureType</a>), the first three only differ in the amount of dimensions they have (one, two or three).</p>
<h2><a class="anchor" id="Cube-map-textures"></a>
Cube map textures</h2>
<p>The cube map texture type (<a class="el" href="group___resources.html#ggae50520af6377ff8184cf79125cb9c4d7ae295fe0fa26623b66cc023f462962344" title="cube map (six two dimensional textures, one for each cube face), used in combination with 3D texture ...">Ogre::TEX_TYPE_CUBE_MAP</a>) is a different beast from the others; a cube map texture represents a series of six two dimensional images addressed by 3D texture coordinates.</p>
<dl compact="compact">
<dt>+X (face 0) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive x plane (right).</p>
<p class="enddd"></p>
</dd>
<dt>-X (face 1) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative x plane (left).</p>
<p class="enddd"></p>
</dd>
<dt>+Y (face 2) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive y plane (top).</p>
<p class="enddd"></p>
</dd>
<dt>-Y (face 3) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative y plane (bottom).</p>
<p class="enddd"></p>
</dd>
<dt>+Z (face 4) </dt>
<dd><p class="startdd"></p>
<p>Represents the positive z plane (front).</p>
<p class="enddd"></p>
</dd>
<dt>-Z (face 5) </dt>
<dd><p class="startdd"></p>
<p>Represents the negative z plane (back).</p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Pixel-Formats"></a>
Pixel Formats</h2>
<p>A pixel format described the storage format of pixel data. It defines the way pixels are encoded in memory. The following classes of pixel formats (PF_*) are defined:</p>
<dl compact="compact">
<dt>Native endian formats (PF_A8R8G8B8 and other formats with bit counts) </dt>
<dd><p class="startdd"></p>
<p>These are native endian (16, 24 and 32 bit) integers in memory. This means that an image with format PF_A8R8G8B8 can be seen as an array of 32 bit integers, defined as 0xAARRGGBB in hexadecimal. The meaning of the letters is described below.</p>
<p class="enddd"></p>
</dd>
<dt>Byte formats (PF_BYTE_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one byte per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_BYTE_RGBA consists of blocks of four bytes, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Short formats (PF_SHORT_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one unsigned short (16 bit integer) per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_SHORT_RGBA consists of blocks of four 16 bit integers, one for red, one for green, one for blue, one for alpha.</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats (PF_FLOAT16_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 16 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT16_RGBA consists of blocks of four 16 bit floats, one for red, one for green, one for blue, one for alpha. The 16 bit floats, also called half float) are very similar to the IEEE single-precision floating-point standard of the 32 bits floats, except that they have only 5 exponent bits and 10 mantissa. Note that there is no standard C++ data type or CPU support to work with these efficiently, but GPUs can calculate with these much more efficiently than with 32 bit floats.</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats (PF_FLOAT32_*) </dt>
<dd><p class="startdd"></p>
<p>These formats have one 32 bit floating point number per channel, and their channels in memory are organized in the order they are specified in the format name. For example, PF_FLOAT32_RGBA consists of blocks of four 32 bit floats, one for red, one for green, one for blue, one for alpha. The C++ data type for these 32 bits floats is just "float".</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats (PF_DXT[1-5]) </dt>
<dd><p class="startdd"></p>
<p>S3TC compressed texture formats, <a href="http://en.wikipedia.org/wiki/S3TC">a good description can be found at Wikipedia</a></p>
<p class="enddd"></p>
</dd>
</dl>
<h2><a class="anchor" id="Colour-channels"></a>
Colour channels</h2>
<p>The meaning of the channels R,G,B,A,L and X is defined as</p>
<dl compact="compact">
<dt>R </dt>
<dd><p class="startdd"></p>
<p>Red colour component, usually ranging from 0.0 (no red) to 1.0 (full red).</p>
<p class="enddd"></p>
</dd>
<dt>G </dt>
<dd><p class="startdd"></p>
<p>Green colour component, usually ranging from 0.0 (no green) to 1.0 (full green).</p>
<p class="enddd"></p>
</dd>
<dt>B </dt>
<dd><p class="startdd"></p>
<p>Blue colour component, usually ranging from 0.0 (no blue) to 1.0 (full blue).</p>
<p class="enddd"></p>
</dd>
<dt>A </dt>
<dd><p class="startdd"></p>
<p>Alpha component, usually ranging from 0.0 (entire transparent) to 1.0 (opaque).</p>
<p class="enddd"></p>
</dd>
<dt>L </dt>
<dd><p class="startdd"></p>
<p>Luminance component, usually ranging from 0.0 (black) to 1.0 (white). The luminance component is duplicated in the R, G, and B channels to achieve a greyscale image.</p>
<p class="enddd"></p>
</dd>
<dt>X </dt>
<dd><p class="startdd"></p>
<p>This component is completely ignored.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If none of red, green and blue components, or luminance is defined in a format, these default to 0. For the alpha channel this is different; if no alpha is defined, it defaults to 1.</p>
<h2><a class="anchor" id="Complete-list-of-pixel-formats"></a>
List of pixel formats</h2>
<p>This pixel formats supported by the current version of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> are</p>
<dl compact="compact">
<dt>Byte formats </dt>
<dd><p class="startdd"></p>
<p>PF_BYTE_RGB, PF_BYTE_BGR, PF_BYTE_BGRA, PF_BYTE_RGBA, PF_BYTE_L, PF_BYTE_LA, PF_BYTE_A</p>
<p class="enddd"></p>
</dd>
<dt>Short formats </dt>
<dd><p class="startdd"></p>
<p>PF_SHORT_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float16 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT16_R, PF_FLOAT16_RGB, PF_FLOAT16_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>Float32 formats </dt>
<dd><p class="startdd"></p>
<p>PF_FLOAT32_R, PF_FLOAT32_RGB, PF_FLOAT32_RGBA</p>
<p class="enddd"></p>
</dd>
<dt>8 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L8, PF_A8, PF_A4L4, PF_R3G3B2</p>
<p class="enddd"></p>
</dd>
<dt>16 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_L16, PF_R5G6B5, PF_B5G6R5, PF_A4R4G4B4, PF_A1R5G5B5</p>
<p class="enddd"></p>
</dd>
<dt>24 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_R8G8B8, PF_B8G8R8</p>
<p class="enddd"></p>
</dd>
<dt>32 bit native endian formats </dt>
<dd><p class="startdd"></p>
<p>PF_A8R8G8B8, PF_A8B8G8R8, PF_B8G8R8A8, PF_R8G8B8A8, PF_X8R8G8B8, PF_X8B8G8R8, PF_A2R10G10B10 PF_A2B10G10R10</p>
<p class="enddd"></p>
</dd>
<dt>Compressed formats </dt>
<dd><p class="startdd"></p>
<p>PF_DXT1, PF_DXT2, PF_DXT3, PF_DXT4, PF_DXT5</p>
<p class="enddd"></p>
</dd>
</dl>
<p>For a complete list see <a class="el" href="group___image.html#ga7e0353e7d36d4c2e8468641b7303d39c" title="The pixel format used for images, textures, and render surfaces. ">Ogre::PixelFormat</a>.</p>
<h2><a class="anchor" id="Pixel-boxes"></a>
Pixel boxes</h2>
<p>All methods in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> that take or return raw image data return a <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a> object.</p>
<p>A PixelBox is a primitive describing a volume (3D), image (2D) or line (1D) of pixels in CPU memory. It describes the location and data format of a region of memory used for image data, but does not do any memory management in itself.</p>
<p>Inside the memory pointed to by the <em>data</em> member of a pixel box, pixels are stored as a succession of "depth" slices (in Z), each containing "height" rows (Y) of "width" pixels (X).</p>
<p>Dimensions that are not used must be 1. For example, a one dimensional image will have extents (width,1,1). A two dimensional image has extents (width,height,1).</p>
<p>For more information about the members consult the API documentation <a class="el" href="class_ogre_1_1_pixel_box.html" title="A primitive describing a volume (3D), image (2D) or line (1D) of pixels in memory. ">Ogre::PixelBox</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Shadows</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_shadows.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Shadows </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Enabling-shadows">Enabling shadows</a></li>
<li class="level1"><a href="#Opting-out-of-shadows">Opting out of shadows</a></li>
<li class="level1"><a href="#Stencil-Shadows">Stencil Shadows</a></li>
<li class="level1"><a href="#Texture_002dbased-Shadows">Texture-based Shadows</a></li>
<li class="level1"><a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a></li>
<li class="level1"><a href="#Modulative-Shadows">Modulative Shadows</a></li>
<li class="level1"><a href="#Additive-Light-Masking">Additive Light Masking</a></li>
</ul>
</div>
<div class="textblock"><p>Shadows are clearly an important part of rendering a believable scene - they provide a more tangible feel to the objects in the scene, and aid the viewer in understanding the spatial relationship between objects. Unfortunately, shadows are also one of the most challenging aspects of 3D rendering, and they are still very much an active area of research. Whilst there are many techniques to render shadows, none is perfect and they all come with advantages and disadvantages. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides multiple shadow implementations, with plenty of configuration settings, so you can choose which technique is most appropriate for your scene.</p>
<p>Shadow implementations fall into basically 2 broad categories: <a href="#Stencil-Shadows">Stencil Shadows</a> and <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>. This describes the method by which the shape of the shadow is generated. In addition, there is more than one way to render the shadow into the scene: <a href="#Modulative-Shadows">Modulative Shadows</a>, which darkens the scene in areas of shadow, and <a href="#Additive-Light-Masking">Additive Light Masking</a> which by contrast builds up light contribution in areas which are not in shadow. You also have the option of <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> which gives you complete control over texture shadow application, allowing for complex single-pass shadowing shaders. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports all these combinations.</p>
<h1><a class="anchor" id="Enabling-shadows"></a>
Enabling shadows</h1>
<p>Shadows are disabled by default, here’s how you turn them on and configure them in the general sense:</p>
<ol type="1">
<li><p class="startli">Enable a shadow technique on the SceneManager as the <b>first</b> thing you doing your scene setup. It is important that this is done first because the shadow technique can alter the way meshes are loaded. Here’s an example:</p>
<p class="startli"><code>mSceneMgr-&gt;setShadowTechnique(SHADOWTYPE_STENCIL_ADDITIVE);</code></p>
</li>
<li>Create one or more lights. Note that not all light types are necessarily supported by all shadow techniques, you should check the sections about each technique to check. Note that if certain lights should not cast shadows, you can turn that off by calling setCastShadows(false) on the light, the default is true.</li>
<li>Disable shadow casting on objects which should not cast shadows. Call setCastShadows(false) on objects you don’t want to cast shadows, the default for all objects is to cast shadows.</li>
<li>Configure shadow far distance. You can limit the distance at which shadows are considered for performance reasons, by calling SceneManager::setShadowFarDistance.</li>
<li>Turn off the receipt of shadows on materials that should not receive them. You can turn off the receipt of shadows (note, not the casting of shadows - that is done per-object) by calling Material::setReceiveShadows or using the receive_shadows material attribute. This is useful for materials which should be considered self-illuminated for example. Note that transparent materials are typically excluded from receiving and casting shadows, although see the <a href="#transparency_005fcasts_005fshadows">transparency_casts_shadows</a> option for exceptions.</li>
</ol>
<h1><a class="anchor" id="Opting-out-of-shadows"></a>
Opting out of shadows</h1>
<p>By default <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> treats all non-transparent objects as shadow casters and receivers (depending on the shadow technique they may not be able to be both at once, check the docs for your chosen technique first). You can disable shadows in various ways:</p>
<dl compact="compact">
<dt>Turning off shadow casting on the light </dt>
<dd><p class="startdd"></p>
<p>Calling Light::setCastsShadows(false) will mean this light casts no shadows at all.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow receipt on a material </dt>
<dd><p class="startdd"></p>
<p>Calling Material::setReceiveShadows(false) will prevent any objects using this material from receiving shadows.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow casting on individual objects </dt>
<dd><p class="startdd"></p>
<p>Calling MovableObject::setCastsShadows(false) will disable shadow casting for this object.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadows on an entire rendering queue group </dt>
<dd><p class="startdd"></p>
<p>Calling RenderQueueGroup::setShadowsEnabled(false) will turn off both shadow casting and receiving on an entire rendering queue group. This is useful because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has to do light setup tasks per group in order to preserve the inter-group ordering. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically disables shadows on a number of groups automatically, such as RENDER_QUEUE_BACKGROUND, RENDER_QUEUE_OVERLAY, RENDER_QUEUE_SKIES_EARLY and RENDER_QUEUE_SKIES_LATE. If you choose to use more rendering queues (and by default, you won’t be using any more than this plus the ’standard’ queue, so ignore this if you don’t know what it means!), be aware that each one can incur a light setup cost, and you should disable shadows on the additional ones you use if you can.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Stencil-Shadows"></a>
Stencil Shadows</h1>
<p>Stencil shadows are a method by which a ’mask’ is created for the screen using a feature called the stencil buffer. This mask can be used to exclude areas of the screen from subsequent renders, and thus it can be used to either include or exclude areas in shadow. They are enabled by calling SceneManager::setShadowTechnique with a parameter of either <code>SHADOWTYPE_STENCIL_ADDITIVE</code> or <code>SHADOWTYPE_STENCIL_MODULATIVE</code>. Because the stencil can only mask areas to be either ’enabled’ or ’disabled’, stencil shadows have ’hard’ edges, that is to say clear dividing lines between light and shadow - it is not possible to soften these edges.</p>
<p>In order to generate the stencil, ’shadow volumes’ are rendered by extruding the silhouette of the shadow caster away from the light. Where these shadow volumes intersect other objects (or the caster, since self-shadowing is supported using this technique), the stencil is updated, allowing subsequent operations to differentiate between light and shadow. How exactly this is used to render the shadows depends on whether <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> is being used. Objects can both cast and receive stencil shadows, so self-shadowing is inbuilt.</p>
<p>The advantage of stencil shadows is that they can do self-shadowing simply on low-end hardware, provided you keep your poly count under control. In contrast doing self-shadowing with texture shadows requires a fairly modern machine (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>). For this reason, you’re likely to pick stencil shadows if you need an accurate shadowing solution for an application aimed at older or lower-spec machines.</p>
<p>The disadvantages of stencil shadows are numerous though, especially on more modern hardware. Because stencil shadows are a geometric technique, they are inherently more costly the higher the number of polygons you use, meaning you are penalized the more detailed you make your meshes. The fillrate cost, which comes from having to render shadow volumes, also escalates the same way. Since more modern applications are likely to use higher polygon counts, stencil shadows can start to become a bottleneck. In addition, the visual aspects of stencil shadows are pretty primitive - your shadows will always be hard-edged, and you have no possibility of doing clever things with shaders since the stencil is not available for manipulation there. Therefore, if your application is aimed at higher-end machines you should definitely consider switching to texture shadows (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>).</p>
<p>There are a number of issues to consider which are specific to stencil shadows:</p>
<ul>
<li><a href="#CPU-Overhead">CPU Overhead</a></li>
<li><a href="#Extrusion-distance">Extrusion distance</a></li>
<li><a href="#Camera-far-plane-positioning">Camera far plane positioning</a></li>
<li><a href="#Mesh-edge-lists">Mesh edge lists</a></li>
<li><a href="#The-Silhouette-Edge">The Silhouette Edge</a></li>
<li><a href="#Be-realistic">Be realistic</a></li>
<li><a href="#Stencil-Optimisations-Performed-By-Ogre">Stencil Optimisations Performed By Ogre</a></li>
</ul>
<p><a class="anchor" id="CPU-Overhead"></a><a class="anchor" id="CPU-Overhead-1"></a></p>
<h2>CPU Overhead</h2>
<p>Calculating the shadow volume for a mesh can be expensive, and it has to be done on the CPU, it is not a hardware accelerated feature. Therefore, you can find that if you overuse this feature, you can create a CPU bottleneck for your application. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> quite aggressively eliminates objects which cannot be casting shadows on the frustum, but there are limits to how much it can do, and large, elongated shadows (e.g. representing a very low sun position) are very difficult to cull efficiently. Try to avoid having too many shadow casters around at once, and avoid long shadows if you can. Also, make use of the ’shadow far distance’ parameter on the SceneManager, this can eliminate distant shadow casters from the shadow volume construction and save you some time, at the expense of only having shadows for closer objects. Lastly, make use of Ogre’s Level-Of-Detail (LOD) features; you can generate automatically calculated LODs for your meshes in code (see the Mesh API docs) or when using the mesh tools such as OgreXMLConverter and OgreMeshUpgrader. Alternatively, you can assign your own manual LODs by providing alternative mesh files at lower detail levels. Both methods will cause the shadow volume complexity to decrease as the object gets further away, which saves you valuable volume calculation time.</p>
<p><a class="anchor" id="Extrusion-distance"></a><a class="anchor" id="Extrusion-distance-1"></a></p>
<h2>Extrusion distance</h2>
<p>When vertex programs are not available, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only extrude shadow volumes a finite distance from the object. If an object gets too close to a light, any finite extrusion distance will be inadequate to guarantee all objects will be shadowed properly by this object. Therefore, you are advised not to let shadow casters pass too close to light sources if you can avoid it, unless you can guarantee that your target audience will have vertex program capable hardware (in this case, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> extrudes the volume to infinity using a vertex program so the problem does not occur). When infinite extrusion is not possible, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses finite extrusion, either derived from the attenuation range of a light (in the case of a point light or spotlight), or a fixed extrusion distance set in the application in the case of directional lights. To change the directional light extrusion distance, use SceneManager::setShadowDirectionalLightExtrusionDistance.</p>
<p><a class="anchor" id="Camera-far-plane-positioning"></a><a class="anchor" id="Camera-far-plane-positioning-1"></a></p>
<h2>Camera far plane positioning</h2>
<p>Stencil shadow volumes rely very much on not being clipped by the far plane. When you enable stencil shadows, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> internally changes the far plane settings of your cameras such that there is no far plane - i.e. it is placed at infinity (Camera::setFarClipDistance(0)). This avoids artifacts caused by clipping the dark caps on shadow volumes, at the expense of a (very) small amount of depth precision.</p>
<p><a class="anchor" id="Mesh-edge-lists"></a><a class="anchor" id="Mesh-edge-lists-1"></a></p>
<h2>Mesh edge lists</h2>
<p>Stencil shadows can only be calculated when an ’edge list’ has been built for all the geometry in a mesh. The official exporters and tools automatically build this for you (or have an option to do so), but if you create your own meshes, you must remember to build edge lists for them before using them with stencil shadows - you can do that by using OgreMeshUpgrade or OgreXmlConverter, or by calling Mesh::buildEdgeList before you export or use the mesh. If a mesh doesn’t have edge lists, OGRE assumes that it is not supposed to cast stencil shadows.</p>
<p><a class="anchor" id="The-Silhouette-Edge"></a><a class="anchor" id="The-Silhouette-Edge-1"></a></p>
<h2>The Silhouette Edge</h2>
<p>Stencil shadowing is about finding a silhouette of the mesh, and projecting it away to form a volume. What this means is that there is a definite boundary on the shadow caster between light and shadow; a set of edges where where the triangle on one side is facing toward the light, and one is facing away. This produces a sharp edge around the mesh as the transition occurs. Provided there is little or no other light in the scene, and the mesh has smooth normals to produce a gradual light change in its underlying shading, the silhouette edge can be hidden - this works better the higher the tessellation of the mesh. However, if the scene includes ambient light, then the difference is far more marked. This is especially true when using <a href="#Modulative-Shadows">Modulative Shadows</a>, because the light contribution of each shadowed area is not taken into account by this simplified approach, and so using 2 or more lights in a scene using modulative stencil shadows is not advisable; the silhouette edges will be very marked. Additive lights do not suffer from this as badly because each light is masked individually, meaning that it is only ambient light which can show up the silhouette edges.</p>
<p><a class="anchor" id="Be-realistic"></a><a class="anchor" id="Be-realistic-1"></a></p>
<h2>Be realistic</h2>
<p>Don’t expect to be able to throw any scene using any hardware at the stencil shadow algorithm and expect to get perfect, optimum speed results. Shadows are a complex and expensive technique, so you should impose some reasonable limitations on your placing of lights and objects; they’re not really that restricting, but you should be aware that this is not a complete free-for-all.</p>
<ul>
<li>Try to avoid letting objects pass very close (or even through) lights - it might look nice but it’s one of the cases where artifacts can occur on machines not capable of running vertex programs.</li>
<li>Be aware that shadow volumes do not respect the ’solidity’ of the objects they pass through, and if those objects do not themselves cast shadows (which would hide the effect) then the result will be that you can see shadows on the other side of what should be an occluding object.</li>
<li>Make use of SceneManager::setShadowFarDistance to limit the number of shadow volumes constructed</li>
<li>Make use of LOD to reduce shadow volume complexity at distance</li>
<li><p class="startli">Avoid very long (dusk and dawn) shadows - they exacerbate other issues such as volume clipping, fillrate, and cause many more objects at a greater distance to require volume construction.</p>
<p class="startli"><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre"></a><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre-1"></a></p>
</li>
</ul>
<h2>Stencil Optimisations Performed By <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a></h2>
<p>Despite all that, stencil shadows can look very nice (especially with <a href="#Additive-Light-Masking">Additive Light Masking</a>) and can be fast if you respect the rules above. In addition, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> comes pre-packed with a lot of optimisations which help to make this as quick as possible. This section is more for developers or people interested in knowing something about the ’under the hood’ behaviour of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl compact="compact">
<dt>Vertex program extrusion </dt>
<dd><p class="startdd"></p>
<p>As previously mentioned, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> performs the extrusion of shadow volumes in hardware on vertex program-capable hardware (e.g. GeForce3, Radeon 8500 or better). This has 2 major benefits; the obvious one being speed, but secondly that vertex programs can extrude points to infinity, which the fixed-function pipeline cannot, at least not without performing all calculations in software. This leads to more robust volumes, and also eliminates more than half the volume triangles on directional lights since all points are projected to a single point at infinity.</p>
<p class="enddd"></p>
</dd>
<dt>Scissor test optimisation </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a scissor rectangle to limit the effect of point / spot lights when their range does not cover the entire viewport; that means we save fillrate when rendering stencil volumes, especially with distant lights</p>
<p class="enddd"></p>
</dd>
<dt>Z-Pass and Z-Fail algorithms </dt>
<dd><p class="startdd"></p>
<p>The Z-Fail algorithm, often attributed to John Carmack, is used in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to make sure shadows are robust when the camera passes through the shadow volume. However, the Z-Fail algorithm is more expensive than the traditional Z-Pass; so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> detects when Z-Fail is required and only uses it then, Z-Pass is used at all other times.</p>
<p class="enddd"></p>
</dd>
<dt>2-Sided stenciling and stencil wrapping </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports the 2-Sided stenciling / stencil wrapping extensions, which when supported allow volumes to be rendered in a single pass instead of having to do one pass for back facing tris and another for front-facing tris. This doesn’t save fillrate, since the same number of stencil updates are done, but it does save primitive setup and the overhead incurred in the driver every time a render call is made.</p>
<p class="enddd"></p>
</dd>
<dt>Aggressive shadow volume culling </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at detecting which lights could be affecting the frustum, and from that, which objects could be casting a shadow on the frustum. This means we don’t waste time constructing shadow geometry we don’t need. Setting the shadow far distance is another important way you can reduce stencil shadow overhead since it culls far away shadow volumes even if they are visible, which is beneficial in practice since you’re most interested in shadows for close-up objects.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Texture_002dbased-Shadows"></a>
Texture-based Shadows</h1>
<p>Texture shadows involve rendering shadow casters from the point of view of the light into a texture, which is then projected onto shadow receivers. The main advantage of texture shadows as opposed to <a href="#Stencil-Shadows">Stencil Shadows</a> is that the overhead of increasing the geometric detail is far lower, since there is no need to perform per-triangle calculations. Most of the work in rendering texture shadows is done by the graphics card, meaning the technique scales well when taking advantage of the latest cards, which are at present outpacing CPUs in terms of their speed of development. In addition, texture shadows are <b>much</b> more customisable - you can pull them into shaders to apply as you like (particularly with <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a>, you can perform filtering to create softer shadows or perform other special effects on them. Basically, most modern engines use texture shadows as their primary shadow technique simply because they are more powerful, and the increasing speed of GPUs is rapidly amortizing the fillrate / texture access costs of using them.</p>
<p>The main disadvantage to texture shadows is that, because they are simply a texture, they have a fixed resolution which means if stretched, the pixellation of the texture can become obvious. There are ways to combat this though:</p>
<dl compact="compact">
<dt>Choosing a projection basis </dt>
<dd><p class="startdd"></p>
<p>The simplest projection is just to render the shadow casters from the lights perspective using a regular camera setup. This can look bad though, so there are many other projections which can help to improve the quality from the main camera’s perspective. OGRE supports pluggable projection bases via it’s ShadowCameraSetup class, and comes with several existing options - <b>Uniform</b> (which is the simplest), <b>Uniform Focussed</b> (which is still a normal camera projection, except that the camera is focussed into the area that the main viewing camera is looking at), LiSPSM (Light Space Perspective Shadow Mapping - which both focusses and distorts the shadow frustum based on the main view camera) and Plan Optimal (which seeks to optimise the shadow fidelity for a single receiver plane).</p>
<p class="enddd"></p>
</dd>
<dt>Filtering </dt>
<dd><p class="startdd"></p>
<p>You can also sample the shadow texture multiple times rather than once to soften the shadow edges and improve the appearance. Percentage Closest Filtering (PCF) is the most popular approach, although there are multiple variants depending on the number and pattern of the samples you take. Our shadows demo includes a 5-tap PCF example combined with depth shadow mapping.</p>
<p class="enddd"></p>
</dd>
<dt>Using a larger texture </dt>
<dd><p class="startdd"></p>
<p>Again as GPUs get faster and gain more memory, you can scale up to take advantage of this.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If you combine all 3 of these techniques you can get a very high quality shadow solution.</p>
<p>The other issue is with point lights. Because texture shadows require a render to texture in the direction of the light, omnidirectional lights (point lights) would require 6 renders to totally cover all the directions shadows might be cast. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> primarily supports directional lights and spotlights for generating texture shadows; you can use point lights but they will only work if off-camera since they are essentially turned into a spotlight shining into your camera frustum for the purposes of texture shadows.</p>
<p><a class="anchor" id="Directional-Lights"></a></p>
<h2>Directional Lights</h2>
<p>Directional lights in theory shadow the entire scene from an infinitely distant light. Now, since we only have a finite texture which will look very poor quality if stretched over the entire scene, clearly a simplification is required. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> places a shadow texture over the area immediately in front of the camera, and moves it as the camera moves (although it rounds this movement to multiples of texels so that the slight ’swimming shadow’ effect caused by moving the texture is minimised). The range to which this shadow extends, and the offset used to move it in front of the camera, are configurable (See <a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a>). At the far edge of the shadow, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> fades out the shadow based on other configurable parameters so that the termination of the shadow is softened.</p>
<p><a class="anchor" id="Spotlights"></a></p>
<h2>Spotlights</h2>
<p>Spotlights are much easier to represent as renderable shadow textures than directional lights, since they are naturally a frustum. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> represents spotlight directly by rendering the shadow from the light position, in the direction of the light cone; the field-of-view of the texture camera is adjusted based on the spotlight falloff angles. In addition, to hide the fact that the shadow texture is square and has definite edges which could show up outside the spotlight, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a second texture unit when projecting the shadow onto the scene which fades out the shadow gradually in a projected circle around the spotlight.</p>
<p><a class="anchor" id="Point-Lights"></a></p>
<h2>Point Lights</h2>
<p>As mentioned above, to support point lights properly would require multiple renders (either 6 for a cubic render or perhaps 2 for a less precise parabolic mapping), so rather than do that we approximate point lights as spotlights, where the configuration is changed on the fly to make the light shine from its position over the whole of the viewing frustum. This is not an ideal setup since it means it can only really work if the point light’s position is out of view, and in addition the changing parameterisation can cause some ’swimming’ of the texture. Generally we recommend avoiding making point lights cast texture shadows.</p>
<p><a class="anchor" id="Shadow-Casters-and-Shadow-Receivers"></a></p>
<h2>Shadow Casters and Shadow Receivers</h2>
<p>To enable texture shadows, use the shadow technique SHADOWTYPE_TEXTURE_MODULATIVE or SHADOWTYPE_TEXTURE_ADDITIVE; as the name suggests this produces <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> respectively. The cheapest and simplest texture shadow techniques do not use depth information, they merely render casters to a texture and render this onto receivers as plain colour - this means self-shadowing is not possible using these methods. This is the default behaviour if you use the automatic, fixed-function compatible (and thus usable on lower end hardware) texture shadow techniques. You can however use shaders-based techniques through custom shadow materials for casters and receivers to perform more complex shadow algorithms, such as depth shadow mapping which does allow self-shadowing. OGRE comes with an example of this in its shadows demo, although it’s only usable on Shader Model 2 cards or better. Whilst fixed-function depth shadow mapping is available in OpenGL, it was never standardised in Direct3D so using shaders in custom caster &amp; receiver materials is the only portable way to do it. If you use this approach, call SceneManager::setShadowTextureSelfShadow with a parameter of ’true’ to allow texture shadow casters to also be receivers. If you’re not using depth shadow mapping, OGRE divides shadow casters and receivers into 2 disjoint groups. Simply by turning off shadow casting on an object, you automatically make it a shadow receiver (although this can be disabled by setting the ’receive_shadows’ option to ’false’ in a material script. Similarly, if an object is set as a shadow caster, it cannot receive shadows.</p>
<h1><a class="anchor" id="Configuring-Texture-Shadows"></a>
Configuring Texture Shadows</h1>
<p>There are a number of settings which will help you configure your texture-based shadows so that they match your requirements.</p>
<ul>
<li><a href="#Maximum-number-of-shadow-textures">Maximum number of shadow textures</a></li>
<li><a href="#Shadow-texture-size">Shadow texture size</a></li>
<li><a href="#Shadow-far-distance">Shadow far distance</a></li>
<li><a href="#Shadow-texture-offset-Directional-Lights_0029">Shadow texture offset (Directional Lights)</a></li>
<li><a href="#Shadow-fade-settings">Shadow fade settings</a></li>
<li><a href="#Custom-shadow-camera-setups">Custom shadow camera setups</a></li>
<li><a href="#Shadow-texture-Depth-Buffer-sharing">Shadow texture Depth Buffer sharing</a></li>
<li><a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a></li>
</ul>
<p><a class="anchor" id="Maximum-number-of-shadow-textures"></a><a class="anchor" id="Maximum-number-of-shadow-textures-1"></a></p>
<h2>Maximum number of shadow textures</h2>
<p>Shadow textures take up texture memory, and to avoid stalling the rendering pipeline <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> does not reuse the same shadow texture for multiple lights within the same frame. This means that each light which is to cast shadows must have its own shadow texture. In practice, if you have a lot of lights in your scene you would not wish to incur that sort of texture overhead. You can adjust this manually by simply turning off shadow casting for lights you do not wish to cast shadows. In addition, you can set a maximum limit on the number of shadow textures <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is allowed to use by calling SceneManager::setShadowTextureCount. Each frame, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> determines the lights which could be affecting the frustum, and then allocates the number of shadow textures it is allowed to use to the lights on a first-come-first-served basis. Any additional lights will not cast shadows that frame. Note that you can set the number of shadow textures and their size at the same time by using the SceneManager::setShadowTextureSettings method; this is useful because both the individual calls require the potential creation / destruction of texture resources.</p>
<p><a class="anchor" id="Shadow-texture-size"></a><a class="anchor" id="Shadow-texture-size-1"></a></p>
<h2>Shadow texture size</h2>
<p>The size of the textures used for rendering the shadow casters into can be altered; clearly using larger textures will give you better quality shadows, but at the expense of greater memory usage. Changing the texture size is done by calling SceneManager::setShadowTextureSize - textures are assumed to be square and you must specify a texture size that is a power of 2. Be aware that each modulative shadow texture will take size*size*3 bytes of texture memory. <b>Important</b>: if you use the GL render system your shadow texture size can only be larger (in either dimension) than the size of your primary window surface if the hardware supports the Frame Buffer Object (FBO) or Pixel Buffer Object (PBO) extensions. Most modern cards support this now, but be careful of older cards - you can check the ability of the hardware to manage this through ogreRoot-&gt;getRenderSystem()-&gt;getCapabilities()-&gt;hasCapability(RSC_HWRENDER_TO_TEXTURE). If this returns false, if you create a shadow texture larger in any dimension than the primary surface, the rest of the shadow texture will be blank.</p>
<p><a class="anchor" id="Shadow-far-distance"></a><a class="anchor" id="Shadow-far-distance-1"></a></p>
<h2>Shadow far distance</h2>
<p>This determines the distance at which shadows are terminated; it also determines how far into the distance the texture shadows for directional lights are stretched - by reducing this value, or increasing the texture size, you can improve the quality of shadows from directional lights at the expense of closer shadow termination or increased memory usage, respectively.</p>
<p><a class="anchor" id="Shadow-texture-offset-Directional-Lights_0029"></a></p>
<h2>Shadow texture offset (Directional Lights)</h2>
<p>As mentioned above in the directional lights section, the rendering of shadows for directional lights is an approximation that allows us to use a single render to cover a largish area with shadows. This offset parameter affects how far from the camera position the center of the shadow texture is offset, as a proportion of the shadow far distance. The greater this value, the more of the shadow texture is ’useful’ to you since it’s ahead of the camera, but also the further you offset it, the more chance there is of accidentally seeing the edge of the shadow texture at more extreme angles. You change this value by calling SceneManager::setShadowDirLightTextureOffset, the default is 0.6.</p>
<p><a class="anchor" id="Shadow-fade-settings"></a><a class="anchor" id="Shadow-fade-settings-1"></a></p>
<h2>Shadow fade settings</h2>
<p>Shadows fade out before the shadow far distance so that the termination of shadow is not abrupt. You can configure the start and end points of this fade by calling the SceneManager::setShadowTextureFadeStart and SceneManager::setShadowTextureFadeEnd methods, both take distances as a proportion of the shadow far distance. Because of the inaccuracies caused by using a square texture and a radial fade distance, you cannot use 1.0 as the fade end, if you do you’ll see artifacts at the extreme edges. The default values are 0.7 and 0.9, which serve most purposes but you can change them if you like.</p>
<h1>Texture shadows and vertex / fragment programs</h1>
<p>When rendering shadow casters into a modulative shadow texture, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> turns off all textures, and all lighting contributions except for ambient light, which it sets to the colour of the shadow (<a href="#Shadow-Colour">Shadow Colour</a>). For additive shadows, it render the casters into a black &amp; white texture instead. This is enough to render shadow casters for fixed-function material techniques, however where a vertex program is used <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> doesn’t have so much control. If you use a vertex program in the <b>first pass</b> of your technique, then you must also tell ogre which vertex program you want it to use when rendering the shadow caster; see <a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a> for full details.</p>
<p><a class="anchor" id="Custom-shadow-camera-setups"></a><a class="anchor" id="Custom-shadow-camera-setups-1"></a></p>
<h2>Custom shadow camera setups</h2>
<p>As previously mentioned, one of the downsides of texture shadows is that the texture resolution is finite, and it’s possible to get aliasing when the size of the shadow texel is larger than a screen pixel, due to the projection of the texture. In order to address this, you can specify alternative projection bases by using or creating subclasses of the ShadowCameraSetup class. The default version is called DefaultShadowCameraSetup and this sets up a simple regular frustum for point and spotlights, and an orthographic frustum for directional lights. There is also a PlaneOptimalShadowCameraSetup class which specialises the projection to a plane, thus giving you much better definition provided your shadow receivers exist mostly in a single plane. Other setup classes (e.g. you might create a perspective or trapezoid shadow mapping version) can be created and plugged in at runtime, either on individual lights or on the SceneManager as a whole.</p>
<p><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing"></a><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing-1"></a></p>
<h2>Shadow texture Depth Buffer sharing</h2>
<p>Shadow textures need a depth buffer like many other RTs (Render Textures). Prior to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8, the depth buffer behavior was left undefined leaving a very small possibility of causing inconsistencies across different window resolutions and render systems. Depending on the render window’s resolutions and/or rendersystem being used, the depth buffer might been shared with the render window or a new one could get created to suite the shadow textures. If the application was depending on the depth buffer contents from the previous scene render (that is, no clear was performed) where a shadow texture render pass was in the middle; then the depth buffer would’ve contained garbage (but not consistent on all machines) causing graphical glitches hard to spot.</p>
<p>From <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8 onwards the depth buffer usage &amp; sharing can be flexible controlled through the use of depth pool IDs. These pool IDs are not specifically part of shadow textures, but rather anything involving RTs. All RTs with the same pool ID share the same depth buffers when possible (following RenderSystem API rules, check RenderSystemCapabilities flags to find out what the behavior will be). The default ID for shadow textures is 1; which is the same default value for render windows, and RTTs; thus maintaining the same behavior with older applications while achieving maximum memory saving and performance efficiency because the number of created depth buffers is as lowest as possible.</p>
<p>However there are some reasons to put shadow textures in a separate pool. This holds specially true if the application depends on the previous contents from the depth buffer before the shadow pass, instead of doing a clear:</p>
<ul>
<li>In Direct3D9, the shadow texture is more likely to share the depth buffer with the render window at high resolutions (when the window is bigger than the shadow texture resolution), but at low resolutions it won’t be shared, thus causing two different behaviors. Also probably the shadow texture will share the depth buffers with most other RTTs (i.e. compositors)</li>
<li>In OpenGL 2.1, the shadow texture can’t be shared with the main render window; and most likely will NOT be shared with many other RTTs (i.e. compositors) since OGL 2.1 has a requirement that texture resolutions should exactly match, while D3D9 specifies depth buffers can be shared as long as the resolutions are equal or less.</li>
</ul>
<p>For example, the DeferredShading sample suffers from this problem. If this is a problem for a particular effect you’re trying to achieve, you can specify a custom pool ID so that shadow textures get their own depth buffer(s), ensuring they aren’t shared with other RTs. You can set the poolId parameter from either SceneManager::setShadowTextureSettings or setShadowTextureConfig</p>
<div class="fragment"><div class="line">mSceneMgr-&gt;setShadowTextureSettings( size, count, format, PoolId );</div><div class="line">mSceneMgr-&gt;setShadowTextureConfig( 0, 512, 512, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39cab63dbcddd53d1f766d742fd557c26be4">PF_FLOAT16_R</a>, 50 );</div></div><!-- fragment --><p>Note a poolId of 0 will make the shadow textures not to use a depth buffer, which isn’t usually a desired behavior.</p>
<p><a class="anchor" id="Integrated-Texture-Shadows"></a><a class="anchor" id="Integrated-Texture-Shadows-1"></a></p>
<h2>Integrated Texture Shadows</h2>
<p>Texture shadows have one major advantage over stencil shadows - the data used to represent them can be referenced in regular shaders. Whilst the default texture shadow modes (SHADOWTYPE_TEXTURE_MODULATIVE and SHADOWTYPE_TEXTURE_ADDITIVE) automatically render shadows for you, their disadvantage is that because they are generalised add-ons to your own materials, they tend to take more passes of the scene to use. In addition, you don’t have a lot of control over the composition of the shadows.</p>
<p>Here is where ’integrated’ texture shadows step in. Both of the texture shadow types above have alternative versions called SHADOWTYPE_TEXTURE_MODULATIVE_INTEGRATED and SHADOWTYPE_TEXTURE_ADDITIVE_INTEGRATED, where instead of rendering the shadows for you, it just creates the texture shadow and then expects you to use that shadow texture as you see fit when rendering receiver objects in the scene. The downside is that you have to take into account shadow receipt in every one of your materials if you use this option - the upside is that you have total control over how the shadow textures are used. The big advantage here is that you can can perform more complex shading, taking into account shadowing, than is possible using the generalised bolt-on approaches, AND you can probably write them in a smaller number of passes, since you know precisely what you need and can combine passes where possible. When you use one of these shadowing approaches, the only difference between additive and modulative is the colour of the casters in the shadow texture (the shadow colour for modulative, black for additive) - the actual calculation of how the texture affects the receivers is of course up to you. No separate modulative pass will be performed, and no splitting of your materials into ambient / per-light / decal etc will occur - absolutely everything is determined by your original material (which may have modulative passes or per-light iteration if you want of course, but it’s not required).</p>
<p>You reference a shadow texture in a material which implements this approach by using the ’<a href="#content_005ftype">content_type</a> shadow’ directive in your <a href="#Texture-Units">texture_unit</a>. It implicitly references a shadow texture based on the number of times you’ve used this directive in the same pass, and the light_start option or light-based pass iteration, which might start the light index higher than 0.</p>
<h1><a class="anchor" id="Modulative-Shadows"></a>
Modulative Shadows</h1>
<p>Modulative shadows work by darkening an already rendered scene with a fixed colour. First, the scene is rendered normally containing all the objects which will be shadowed, then a modulative pass is done per light, which darkens areas in shadow. Finally, objects which do not receive shadows are rendered.</p>
<p>There are 2 modulative shadow techniques; stencil-based (See <a href="#Stencil-Shadows">Stencil Shadows</a> : SHADOWTYPE_STENCIL_MODULATIVE) and texture-based (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a> : SHADOWTYPE_TEXTURE_MODULATIVE). Modulative shadows are an inaccurate lighting model, since they darken the areas of shadow uniformly, irrespective of the amount of light which would have fallen on the shadow area anyway. However, they can give fairly attractive results for a much lower overhead than more ’correct’ methods like <a href="#Additive-Light-Masking">Additive Light Masking</a>, and they also combine well with pre-baked static lighting (such as pre-calculated lightmaps), which additive lighting does not. The main thing to consider is that using multiple light sources can result in overly dark shadows (where shadows overlap, which intuitively looks right in fact, but it’s not physically correct) and artifacts when using stencil shadows (See <a href="#The-Silhouette-Edge">The Silhouette Edge</a>).</p>
<p><a class="anchor" id="Shadow-Colour"></a><a class="anchor" id="Shadow-Colour-1"></a></p>
<h2>Shadow Colour</h2>
<p>The colour which is used to darken the areas in shadow is set by SceneManager::setShadowColour; it defaults to a dark grey (so that the underlying colour still shows through a bit).</p>
<p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to have a separate pass of the scene to render shadows. In this case the ’modulative’ aspect of the shadow technique just affects the colour of the shadow texture.</p>
<h1><a class="anchor" id="Additive-Light-Masking"></a>
Additive Light Masking</h1>
<p>Additive light masking is about rendering the scene many times, each time representing a single light contribution whose influence is masked out in areas of shadow. Each pass is combined with (added to) the previous one such that when all the passes are complete, all the light contribution has correctly accumulated in the scene, and each light has been prevented from affecting areas which it should not be able to because of shadow casters. This is an effective technique which results in very realistic looking lighting, but it comes at a price: more rendering passes.</p>
<p>As many technical papers (and game marketing) will tell you, rendering realistic lighting like this requires multiple passes. Being a friendly sort of engine, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> frees you from most of the hard work though, and will let you use the exact same material definitions whether you use this lighting technique or not (for the most part, see <a href="#Pass-Classification-and-Vertex-Programs">Pass Classification and Vertex Programs</a>). In order to do this technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically categorises the <a href="#Passes">Passes</a> you define in your materials into 3 types:</p>
<ol type="1">
<li>ambient Passes categorised as ’ambient’ include any base pass which is not lit by any particular light, i.e. it occurs even if there is no ambient light in the scene. The ambient pass always happens first, and sets up the initial depth value of the fragments, and the ambient colour if applicable. It also includes any emissive / self illumination contribution. Only textures which affect ambient light (e.g. ambient occlusion maps) should be rendered in this pass.</li>
<li>diffuse/specular Passes categorised as ’diffuse/specular’ (or ’per-light’) are rendered once per light, and each pass contributes the diffuse and specular colour from that single light as reflected by the diffuse / specular terms in the pass. Areas in shadow from that light are masked and are thus not updated. The resulting masked colour is added to the existing colour in the scene. Again, no textures are used in this pass (except for textures used for lighting calculations such as normal maps).</li>
<li>decal Passes categorised as ’decal’ add the final texture colour to the scene, which is modulated by the accumulated light built up from all the ambient and diffuse/specular passes.</li>
</ol>
<p>In practice, <a href="#Passes">Passes</a> rarely fall nicely into just one of these categories. For each Technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> compiles a list of ’Illumination Passes’, which are derived from the user defined passes, but can be split, to ensure that the divisions between illumination pass categories can be maintained. For example, if we take a very simple material definition:</p>
<div class="fragment"><div class="line">material TestIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will split this into 3 illumination passes, which will be the equivalent of this:</p>
<div class="fragment"><div class="line">material TestIlluminationSplitIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 0 0 0</div><div class="line">            specular 0 0 0</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Diffuse / specular pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend add</div><div class="line">            iteration once_per_light</div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend modulate</div><div class="line">            lighting off</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>So as you can see, even a simple material requires a minimum of 3 passes when using this shadow technique, and in fact it requires (num_lights + 2) passes in the general sense. You can use more passes in your original material and <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will cope with that too, but be aware that each pass may turn into multiple ones if it uses more than one type of light contribution (ambient vs diffuse/specular) and / or has texture units. The main nice thing is that you get the full multipass lighting behaviour even if you don’t define your materials in terms of it, meaning that your material definitions can remain the same no matter what lighting approach you decide to use.</p>
<p><a class="anchor" id="Manually-Categorising-Illumination-Passes"></a><a class="anchor" id="Manually-Categorising-Illumination-Passes-1"></a></p>
<h2>Manually Categorising Illumination Passes</h2>
<p>Alternatively, if you want more direct control over the categorisation of your passes, you can use the <a href="#illumination_005fstage">illumination_stage</a> option in your pass to explicitly assign a pass unchanged to an illumination stage. This way you can make sure you know precisely how your material will be rendered under additive lighting conditions.</p>
<p><a class="anchor" id="Pass-Classification-and-Vertex-Programs"></a><a class="anchor" id="Pass-Classification-and-Vertex-Programs-1"></a></p>
<h2>Pass Classification and Vertex Programs</h2>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at classifying and splitting your passes to ensure that the multipass rendering approach required by additive lighting works correctly without you having to change your material definitions. However, there is one exception; when you use vertex programs, the normal lighting attributes ambient, diffuse, specular etc are not used, because all of that is determined by the vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has no way of knowing what you’re doing inside that vertex program, so you have to tell it.</p>
<p>In practice this is very easy. Even though your vertex program could be doing a lot of complex, highly customised processing, it can still be classified into one of the 3 types listed above. All you need to do to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> what you’re doing is to use the pass attributes ambient, diffuse, specular and self_illumination, just as if you were not using a vertex program. Sure, these attributes do nothing (as far as rendering is concerned) when you’re using vertex programs, but it’s the easiest way to indicate to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> which light components you’re using in your vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will then classify and potentially split your programmable pass based on this information - it will leave the vertex program as-is (so that any split passes will respect any vertex modification that is being done).</p>
<p>Note that when classifying a diffuse/specular programmable pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> checks to see whether you have indicated the pass can be run once per light (iteration once_per_light). If so, the pass is left intact, including it’s vertex and fragment programs. However, if this attribute is not included in the pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> tries to split off the per-light part, and in doing so it will disable the fragment program, since in the absence of the ’iteration once_per_light’ attribute it can only assume that the fragment program is performing decal work and hence must not be used per light.</p>
<p>So clearly, when you use additive light masking as a shadow technique, you need to make sure that programmable passes you use are properly set up so that they can be classified correctly. However, also note that the changes you have to make to ensure the classification is correct does not affect the way the material renders when you choose not to use additive lighting, so the principle that you should be able to use the same material definitions for all lighting scenarios still holds. Here is an example of a programmable material which will be classified correctly by the illumination pass classifier:</p>
<div class="fragment"><div class="line"><span class="comment">// Per-pixel normal mapping Any number of lights, diffuse and specular</span></div><div class="line">material Examples/BumpMapping/MultiLightSpecular</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Base ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// ambient only, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 1 1 1</div><div class="line">            diffuse 0 0 0 </div><div class="line">            specular 0 0 0 0</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named_auto ambient ambient_light_colour</div><div class="line">            }</div><div class="line">        }</div><div class="line">        <span class="comment">// Now do the lighting pass</span></div><div class="line">        <span class="comment">// NB we don&#39;t do decal texture here because this is repeated per light</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// set ambient off, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 0 0 0 </div><div class="line">            <span class="comment">// do this for each light</span></div><div class="line">            iteration once_per_light</div><div class="line">            scene_blend add</div><div class="line"></div><div class="line">            <span class="comment">// Vertex program reference</span></div><div class="line">            vertex_program_ref Examples/BumpMapVPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightPosition light_position_object_space 0</div><div class="line">                param_named_auto eyePosition camera_position_object_space</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Fragment program</span></div><div class="line">            fragment_program_ref Examples/BumpMapFPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Base bump map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture NMBumpsOut.png</div><div class="line">                colour_op replace</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map #2</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            lighting off</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named ambient float4 1 1 1 1</div><div class="line">            }</div><div class="line">            scene_blend dest_colour zero</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture RustedMetal.jpg </div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to use this explicit sequence - allowing you to compress the number of passes into a much smaller number at the expense of defining an upper number of shadow casting lights. In this case the ’additive’ aspect of the shadow technique just affects the colour of the shadow texture and it’s up to you to combine the shadow textures in your receivers however you like.</p>
<p><a class="anchor" id="Static-Lighting"></a></p>
<h2>Static Lighting</h2>
<p>Despite their power, additive lighting techniques have an additional limitation; they do not combine well with pre-calculated static lighting in the scene. This is because they are based on the principle that shadow is an absence of light, but since static lighting in the scene already includes areas of light and shadow, additive lighting cannot remove light to create new shadows. Therefore, if you use the additive lighting technique you must either use it exclusively as your lighting solution (and you can combine it with per-pixel lighting to create a very impressive dynamic lighting solution), or you must use <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> to combine the static lighting according to your chosen approach. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Shadows</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_shadows.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Shadows </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Enabling-shadows">Enabling shadows</a></li>
<li class="level1"><a href="#Opting-out-of-shadows">Opting out of shadows</a></li>
<li class="level1"><a href="#Stencil-Shadows">Stencil Shadows</a></li>
<li class="level1"><a href="#Texture_002dbased-Shadows">Texture-based Shadows</a></li>
<li class="level1"><a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a></li>
<li class="level1"><a href="#Modulative-Shadows">Modulative Shadows</a></li>
<li class="level1"><a href="#Additive-Light-Masking">Additive Light Masking</a></li>
</ul>
</div>
<div class="textblock"><p>Shadows are clearly an important part of rendering a believable scene - they provide a more tangible feel to the objects in the scene, and aid the viewer in understanding the spatial relationship between objects. Unfortunately, shadows are also one of the most challenging aspects of 3D rendering, and they are still very much an active area of research. Whilst there are many techniques to render shadows, none is perfect and they all come with advantages and disadvantages. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides multiple shadow implementations, with plenty of configuration settings, so you can choose which technique is most appropriate for your scene.</p>
<p>Shadow implementations fall into basically 2 broad categories: <a href="#Stencil-Shadows">Stencil Shadows</a> and <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>. This describes the method by which the shape of the shadow is generated. In addition, there is more than one way to render the shadow into the scene: <a href="#Modulative-Shadows">Modulative Shadows</a>, which darkens the scene in areas of shadow, and <a href="#Additive-Light-Masking">Additive Light Masking</a> which by contrast builds up light contribution in areas which are not in shadow. You also have the option of <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> which gives you complete control over texture shadow application, allowing for complex single-pass shadowing shaders. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports all these combinations.</p>
<h1><a class="anchor" id="Enabling-shadows"></a>
Enabling shadows</h1>
<p>Shadows are disabled by default, here’s how you turn them on and configure them in the general sense:</p>
<ol type="1">
<li><p class="startli">Enable a shadow technique on the SceneManager as the <b>first</b> thing you doing your scene setup. It is important that this is done first because the shadow technique can alter the way meshes are loaded. Here’s an example:</p>
<p class="startli"><code>mSceneMgr-&gt;setShadowTechnique(SHADOWTYPE_STENCIL_ADDITIVE);</code></p>
</li>
<li>Create one or more lights. Note that not all light types are necessarily supported by all shadow techniques, you should check the sections about each technique to check. Note that if certain lights should not cast shadows, you can turn that off by calling setCastShadows(false) on the light, the default is true.</li>
<li>Disable shadow casting on objects which should not cast shadows. Call setCastShadows(false) on objects you don’t want to cast shadows, the default for all objects is to cast shadows.</li>
<li>Configure shadow far distance. You can limit the distance at which shadows are considered for performance reasons, by calling SceneManager::setShadowFarDistance.</li>
<li>Turn off the receipt of shadows on materials that should not receive them. You can turn off the receipt of shadows (note, not the casting of shadows - that is done per-object) by calling Material::setReceiveShadows or using the receive_shadows material attribute. This is useful for materials which should be considered self-illuminated for example. Note that transparent materials are typically excluded from receiving and casting shadows, although see the <a href="#transparency_005fcasts_005fshadows">transparency_casts_shadows</a> option for exceptions.</li>
</ol>
<h1><a class="anchor" id="Opting-out-of-shadows"></a>
Opting out of shadows</h1>
<p>By default <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> treats all non-transparent objects as shadow casters and receivers (depending on the shadow technique they may not be able to be both at once, check the docs for your chosen technique first). You can disable shadows in various ways:</p>
<dl compact="compact">
<dt>Turning off shadow casting on the light </dt>
<dd><p class="startdd"></p>
<p>Calling Light::setCastsShadows(false) will mean this light casts no shadows at all.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow receipt on a material </dt>
<dd><p class="startdd"></p>
<p>Calling Material::setReceiveShadows(false) will prevent any objects using this material from receiving shadows.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow casting on individual objects </dt>
<dd><p class="startdd"></p>
<p>Calling MovableObject::setCastsShadows(false) will disable shadow casting for this object.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadows on an entire rendering queue group </dt>
<dd><p class="startdd"></p>
<p>Calling RenderQueueGroup::setShadowsEnabled(false) will turn off both shadow casting and receiving on an entire rendering queue group. This is useful because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has to do light setup tasks per group in order to preserve the inter-group ordering. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically disables shadows on a number of groups automatically, such as RENDER_QUEUE_BACKGROUND, RENDER_QUEUE_OVERLAY, RENDER_QUEUE_SKIES_EARLY and RENDER_QUEUE_SKIES_LATE. If you choose to use more rendering queues (and by default, you won’t be using any more than this plus the ’standard’ queue, so ignore this if you don’t know what it means!), be aware that each one can incur a light setup cost, and you should disable shadows on the additional ones you use if you can.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Stencil-Shadows"></a>
Stencil Shadows</h1>
<p>Stencil shadows are a method by which a ’mask’ is created for the screen using a feature called the stencil buffer. This mask can be used to exclude areas of the screen from subsequent renders, and thus it can be used to either include or exclude areas in shadow. They are enabled by calling SceneManager::setShadowTechnique with a parameter of either <code>SHADOWTYPE_STENCIL_ADDITIVE</code> or <code>SHADOWTYPE_STENCIL_MODULATIVE</code>. Because the stencil can only mask areas to be either ’enabled’ or ’disabled’, stencil shadows have ’hard’ edges, that is to say clear dividing lines between light and shadow - it is not possible to soften these edges.</p>
<p>In order to generate the stencil, ’shadow volumes’ are rendered by extruding the silhouette of the shadow caster away from the light. Where these shadow volumes intersect other objects (or the caster, since self-shadowing is supported using this technique), the stencil is updated, allowing subsequent operations to differentiate between light and shadow. How exactly this is used to render the shadows depends on whether <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> is being used. Objects can both cast and receive stencil shadows, so self-shadowing is inbuilt.</p>
<p>The advantage of stencil shadows is that they can do self-shadowing simply on low-end hardware, provided you keep your poly count under control. In contrast doing self-shadowing with texture shadows requires a fairly modern machine (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>). For this reason, you’re likely to pick stencil shadows if you need an accurate shadowing solution for an application aimed at older or lower-spec machines.</p>
<p>The disadvantages of stencil shadows are numerous though, especially on more modern hardware. Because stencil shadows are a geometric technique, they are inherently more costly the higher the number of polygons you use, meaning you are penalized the more detailed you make your meshes. The fillrate cost, which comes from having to render shadow volumes, also escalates the same way. Since more modern applications are likely to use higher polygon counts, stencil shadows can start to become a bottleneck. In addition, the visual aspects of stencil shadows are pretty primitive - your shadows will always be hard-edged, and you have no possibility of doing clever things with shaders since the stencil is not available for manipulation there. Therefore, if your application is aimed at higher-end machines you should definitely consider switching to texture shadows (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>).</p>
<p>There are a number of issues to consider which are specific to stencil shadows:</p>
<ul>
<li><a href="#CPU-Overhead">CPU Overhead</a></li>
<li><a href="#Extrusion-distance">Extrusion distance</a></li>
<li><a href="#Camera-far-plane-positioning">Camera far plane positioning</a></li>
<li><a href="#Mesh-edge-lists">Mesh edge lists</a></li>
<li><a href="#The-Silhouette-Edge">The Silhouette Edge</a></li>
<li><a href="#Be-realistic">Be realistic</a></li>
<li><a href="#Stencil-Optimisations-Performed-By-Ogre">Stencil Optimisations Performed By Ogre</a></li>
</ul>
<p><a class="anchor" id="CPU-Overhead"></a><a class="anchor" id="CPU-Overhead-1"></a></p>
<h2>CPU Overhead</h2>
<p>Calculating the shadow volume for a mesh can be expensive, and it has to be done on the CPU, it is not a hardware accelerated feature. Therefore, you can find that if you overuse this feature, you can create a CPU bottleneck for your application. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> quite aggressively eliminates objects which cannot be casting shadows on the frustum, but there are limits to how much it can do, and large, elongated shadows (e.g. representing a very low sun position) are very difficult to cull efficiently. Try to avoid having too many shadow casters around at once, and avoid long shadows if you can. Also, make use of the ’shadow far distance’ parameter on the SceneManager, this can eliminate distant shadow casters from the shadow volume construction and save you some time, at the expense of only having shadows for closer objects. Lastly, make use of Ogre’s Level-Of-Detail (LOD) features; you can generate automatically calculated LODs for your meshes in code (see the Mesh API docs) or when using the mesh tools such as OgreXMLConverter and OgreMeshUpgrader. Alternatively, you can assign your own manual LODs by providing alternative mesh files at lower detail levels. Both methods will cause the shadow volume complexity to decrease as the object gets further away, which saves you valuable volume calculation time.</p>
<p><a class="anchor" id="Extrusion-distance"></a><a class="anchor" id="Extrusion-distance-1"></a></p>
<h2>Extrusion distance</h2>
<p>When vertex programs are not available, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only extrude shadow volumes a finite distance from the object. If an object gets too close to a light, any finite extrusion distance will be inadequate to guarantee all objects will be shadowed properly by this object. Therefore, you are advised not to let shadow casters pass too close to light sources if you can avoid it, unless you can guarantee that your target audience will have vertex program capable hardware (in this case, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> extrudes the volume to infinity using a vertex program so the problem does not occur). When infinite extrusion is not possible, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses finite extrusion, either derived from the attenuation range of a light (in the case of a point light or spotlight), or a fixed extrusion distance set in the application in the case of directional lights. To change the directional light extrusion distance, use SceneManager::setShadowDirectionalLightExtrusionDistance.</p>
<p><a class="anchor" id="Camera-far-plane-positioning"></a><a class="anchor" id="Camera-far-plane-positioning-1"></a></p>
<h2>Camera far plane positioning</h2>
<p>Stencil shadow volumes rely very much on not being clipped by the far plane. When you enable stencil shadows, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> internally changes the far plane settings of your cameras such that there is no far plane - i.e. it is placed at infinity (Camera::setFarClipDistance(0)). This avoids artifacts caused by clipping the dark caps on shadow volumes, at the expense of a (very) small amount of depth precision.</p>
<p><a class="anchor" id="Mesh-edge-lists"></a><a class="anchor" id="Mesh-edge-lists-1"></a></p>
<h2>Mesh edge lists</h2>
<p>Stencil shadows can only be calculated when an ’edge list’ has been built for all the geometry in a mesh. The official exporters and tools automatically build this for you (or have an option to do so), but if you create your own meshes, you must remember to build edge lists for them before using them with stencil shadows - you can do that by using OgreMeshUpgrade or OgreXmlConverter, or by calling Mesh::buildEdgeList before you export or use the mesh. If a mesh doesn’t have edge lists, OGRE assumes that it is not supposed to cast stencil shadows.</p>
<p><a class="anchor" id="The-Silhouette-Edge"></a><a class="anchor" id="The-Silhouette-Edge-1"></a></p>
<h2>The Silhouette Edge</h2>
<p>Stencil shadowing is about finding a silhouette of the mesh, and projecting it away to form a volume. What this means is that there is a definite boundary on the shadow caster between light and shadow; a set of edges where where the triangle on one side is facing toward the light, and one is facing away. This produces a sharp edge around the mesh as the transition occurs. Provided there is little or no other light in the scene, and the mesh has smooth normals to produce a gradual light change in its underlying shading, the silhouette edge can be hidden - this works better the higher the tessellation of the mesh. However, if the scene includes ambient light, then the difference is far more marked. This is especially true when using <a href="#Modulative-Shadows">Modulative Shadows</a>, because the light contribution of each shadowed area is not taken into account by this simplified approach, and so using 2 or more lights in a scene using modulative stencil shadows is not advisable; the silhouette edges will be very marked. Additive lights do not suffer from this as badly because each light is masked individually, meaning that it is only ambient light which can show up the silhouette edges.</p>
<p><a class="anchor" id="Be-realistic"></a><a class="anchor" id="Be-realistic-1"></a></p>
<h2>Be realistic</h2>
<p>Don’t expect to be able to throw any scene using any hardware at the stencil shadow algorithm and expect to get perfect, optimum speed results. Shadows are a complex and expensive technique, so you should impose some reasonable limitations on your placing of lights and objects; they’re not really that restricting, but you should be aware that this is not a complete free-for-all.</p>
<ul>
<li>Try to avoid letting objects pass very close (or even through) lights - it might look nice but it’s one of the cases where artifacts can occur on machines not capable of running vertex programs.</li>
<li>Be aware that shadow volumes do not respect the ’solidity’ of the objects they pass through, and if those objects do not themselves cast shadows (which would hide the effect) then the result will be that you can see shadows on the other side of what should be an occluding object.</li>
<li>Make use of SceneManager::setShadowFarDistance to limit the number of shadow volumes constructed</li>
<li>Make use of LOD to reduce shadow volume complexity at distance</li>
<li><p class="startli">Avoid very long (dusk and dawn) shadows - they exacerbate other issues such as volume clipping, fillrate, and cause many more objects at a greater distance to require volume construction.</p>
<p class="startli"><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre"></a><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre-1"></a></p>
</li>
</ul>
<h2>Stencil Optimisations Performed By <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a></h2>
<p>Despite all that, stencil shadows can look very nice (especially with <a href="#Additive-Light-Masking">Additive Light Masking</a>) and can be fast if you respect the rules above. In addition, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> comes pre-packed with a lot of optimisations which help to make this as quick as possible. This section is more for developers or people interested in knowing something about the ’under the hood’ behaviour of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl compact="compact">
<dt>Vertex program extrusion </dt>
<dd><p class="startdd"></p>
<p>As previously mentioned, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> performs the extrusion of shadow volumes in hardware on vertex program-capable hardware (e.g. GeForce3, Radeon 8500 or better). This has 2 major benefits; the obvious one being speed, but secondly that vertex programs can extrude points to infinity, which the fixed-function pipeline cannot, at least not without performing all calculations in software. This leads to more robust volumes, and also eliminates more than half the volume triangles on directional lights since all points are projected to a single point at infinity.</p>
<p class="enddd"></p>
</dd>
<dt>Scissor test optimisation </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a scissor rectangle to limit the effect of point / spot lights when their range does not cover the entire viewport; that means we save fillrate when rendering stencil volumes, especially with distant lights</p>
<p class="enddd"></p>
</dd>
<dt>Z-Pass and Z-Fail algorithms </dt>
<dd><p class="startdd"></p>
<p>The Z-Fail algorithm, often attributed to John Carmack, is used in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to make sure shadows are robust when the camera passes through the shadow volume. However, the Z-Fail algorithm is more expensive than the traditional Z-Pass; so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> detects when Z-Fail is required and only uses it then, Z-Pass is used at all other times.</p>
<p class="enddd"></p>
</dd>
<dt>2-Sided stenciling and stencil wrapping </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports the 2-Sided stenciling / stencil wrapping extensions, which when supported allow volumes to be rendered in a single pass instead of having to do one pass for back facing tris and another for front-facing tris. This doesn’t save fillrate, since the same number of stencil updates are done, but it does save primitive setup and the overhead incurred in the driver every time a render call is made.</p>
<p class="enddd"></p>
</dd>
<dt>Aggressive shadow volume culling </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at detecting which lights could be affecting the frustum, and from that, which objects could be casting a shadow on the frustum. This means we don’t waste time constructing shadow geometry we don’t need. Setting the shadow far distance is another important way you can reduce stencil shadow overhead since it culls far away shadow volumes even if they are visible, which is beneficial in practice since you’re most interested in shadows for close-up objects.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Texture_002dbased-Shadows"></a>
Texture-based Shadows</h1>
<p>Texture shadows involve rendering shadow casters from the point of view of the light into a texture, which is then projected onto shadow receivers. The main advantage of texture shadows as opposed to <a href="#Stencil-Shadows">Stencil Shadows</a> is that the overhead of increasing the geometric detail is far lower, since there is no need to perform per-triangle calculations. Most of the work in rendering texture shadows is done by the graphics card, meaning the technique scales well when taking advantage of the latest cards, which are at present outpacing CPUs in terms of their speed of development. In addition, texture shadows are <b>much</b> more customisable - you can pull them into shaders to apply as you like (particularly with <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a>, you can perform filtering to create softer shadows or perform other special effects on them. Basically, most modern engines use texture shadows as their primary shadow technique simply because they are more powerful, and the increasing speed of GPUs is rapidly amortizing the fillrate / texture access costs of using them.</p>
<p>The main disadvantage to texture shadows is that, because they are simply a texture, they have a fixed resolution which means if stretched, the pixellation of the texture can become obvious. There are ways to combat this though:</p>
<dl compact="compact">
<dt>Choosing a projection basis </dt>
<dd><p class="startdd"></p>
<p>The simplest projection is just to render the shadow casters from the lights perspective using a regular camera setup. This can look bad though, so there are many other projections which can help to improve the quality from the main camera’s perspective. OGRE supports pluggable projection bases via it’s ShadowCameraSetup class, and comes with several existing options - <b>Uniform</b> (which is the simplest), <b>Uniform Focussed</b> (which is still a normal camera projection, except that the camera is focussed into the area that the main viewing camera is looking at), LiSPSM (Light Space Perspective Shadow Mapping - which both focusses and distorts the shadow frustum based on the main view camera) and Plan Optimal (which seeks to optimise the shadow fidelity for a single receiver plane).</p>
<p class="enddd"></p>
</dd>
<dt>Filtering </dt>
<dd><p class="startdd"></p>
<p>You can also sample the shadow texture multiple times rather than once to soften the shadow edges and improve the appearance. Percentage Closest Filtering (PCF) is the most popular approach, although there are multiple variants depending on the number and pattern of the samples you take. Our shadows demo includes a 5-tap PCF example combined with depth shadow mapping.</p>
<p class="enddd"></p>
</dd>
<dt>Using a larger texture </dt>
<dd><p class="startdd"></p>
<p>Again as GPUs get faster and gain more memory, you can scale up to take advantage of this.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If you combine all 3 of these techniques you can get a very high quality shadow solution.</p>
<p>The other issue is with point lights. Because texture shadows require a render to texture in the direction of the light, omnidirectional lights (point lights) would require 6 renders to totally cover all the directions shadows might be cast. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> primarily supports directional lights and spotlights for generating texture shadows; you can use point lights but they will only work if off-camera since they are essentially turned into a spotlight shining into your camera frustum for the purposes of texture shadows.</p>
<p><a class="anchor" id="Directional-Lights"></a></p>
<h2>Directional Lights</h2>
<p>Directional lights in theory shadow the entire scene from an infinitely distant light. Now, since we only have a finite texture which will look very poor quality if stretched over the entire scene, clearly a simplification is required. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> places a shadow texture over the area immediately in front of the camera, and moves it as the camera moves (although it rounds this movement to multiples of texels so that the slight ’swimming shadow’ effect caused by moving the texture is minimised). The range to which this shadow extends, and the offset used to move it in front of the camera, are configurable (See <a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a>). At the far edge of the shadow, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> fades out the shadow based on other configurable parameters so that the termination of the shadow is softened.</p>
<p><a class="anchor" id="Spotlights"></a></p>
<h2>Spotlights</h2>
<p>Spotlights are much easier to represent as renderable shadow textures than directional lights, since they are naturally a frustum. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> represents spotlight directly by rendering the shadow from the light position, in the direction of the light cone; the field-of-view of the texture camera is adjusted based on the spotlight falloff angles. In addition, to hide the fact that the shadow texture is square and has definite edges which could show up outside the spotlight, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a second texture unit when projecting the shadow onto the scene which fades out the shadow gradually in a projected circle around the spotlight.</p>
<p><a class="anchor" id="Point-Lights"></a></p>
<h2>Point Lights</h2>
<p>As mentioned above, to support point lights properly would require multiple renders (either 6 for a cubic render or perhaps 2 for a less precise parabolic mapping), so rather than do that we approximate point lights as spotlights, where the configuration is changed on the fly to make the light shine from its position over the whole of the viewing frustum. This is not an ideal setup since it means it can only really work if the point light’s position is out of view, and in addition the changing parameterisation can cause some ’swimming’ of the texture. Generally we recommend avoiding making point lights cast texture shadows.</p>
<p><a class="anchor" id="Shadow-Casters-and-Shadow-Receivers"></a></p>
<h2>Shadow Casters and Shadow Receivers</h2>
<p>To enable texture shadows, use the shadow technique SHADOWTYPE_TEXTURE_MODULATIVE or SHADOWTYPE_TEXTURE_ADDITIVE; as the name suggests this produces <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> respectively. The cheapest and simplest texture shadow techniques do not use depth information, they merely render casters to a texture and render this onto receivers as plain colour - this means self-shadowing is not possible using these methods. This is the default behaviour if you use the automatic, fixed-function compatible (and thus usable on lower end hardware) texture shadow techniques. You can however use shaders-based techniques through custom shadow materials for casters and receivers to perform more complex shadow algorithms, such as depth shadow mapping which does allow self-shadowing. OGRE comes with an example of this in its shadows demo, although it’s only usable on Shader Model 2 cards or better. Whilst fixed-function depth shadow mapping is available in OpenGL, it was never standardised in Direct3D so using shaders in custom caster &amp; receiver materials is the only portable way to do it. If you use this approach, call SceneManager::setShadowTextureSelfShadow with a parameter of ’true’ to allow texture shadow casters to also be receivers. If you’re not using depth shadow mapping, OGRE divides shadow casters and receivers into 2 disjoint groups. Simply by turning off shadow casting on an object, you automatically make it a shadow receiver (although this can be disabled by setting the ’receive_shadows’ option to ’false’ in a material script. Similarly, if an object is set as a shadow caster, it cannot receive shadows.</p>
<h1><a class="anchor" id="Configuring-Texture-Shadows"></a>
Configuring Texture Shadows</h1>
<p>There are a number of settings which will help you configure your texture-based shadows so that they match your requirements.</p>
<ul>
<li><a href="#Maximum-number-of-shadow-textures">Maximum number of shadow textures</a></li>
<li><a href="#Shadow-texture-size">Shadow texture size</a></li>
<li><a href="#Shadow-far-distance">Shadow far distance</a></li>
<li><a href="#Shadow-texture-offset-Directional-Lights_0029">Shadow texture offset (Directional Lights)</a></li>
<li><a href="#Shadow-fade-settings">Shadow fade settings</a></li>
<li><a href="#Custom-shadow-camera-setups">Custom shadow camera setups</a></li>
<li><a href="#Shadow-texture-Depth-Buffer-sharing">Shadow texture Depth Buffer sharing</a></li>
<li><a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a></li>
</ul>
<p><a class="anchor" id="Maximum-number-of-shadow-textures"></a><a class="anchor" id="Maximum-number-of-shadow-textures-1"></a></p>
<h2>Maximum number of shadow textures</h2>
<p>Shadow textures take up texture memory, and to avoid stalling the rendering pipeline <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> does not reuse the same shadow texture for multiple lights within the same frame. This means that each light which is to cast shadows must have its own shadow texture. In practice, if you have a lot of lights in your scene you would not wish to incur that sort of texture overhead. You can adjust this manually by simply turning off shadow casting for lights you do not wish to cast shadows. In addition, you can set a maximum limit on the number of shadow textures <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is allowed to use by calling SceneManager::setShadowTextureCount. Each frame, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> determines the lights which could be affecting the frustum, and then allocates the number of shadow textures it is allowed to use to the lights on a first-come-first-served basis. Any additional lights will not cast shadows that frame. Note that you can set the number of shadow textures and their size at the same time by using the SceneManager::setShadowTextureSettings method; this is useful because both the individual calls require the potential creation / destruction of texture resources.</p>
<p><a class="anchor" id="Shadow-texture-size"></a><a class="anchor" id="Shadow-texture-size-1"></a></p>
<h2>Shadow texture size</h2>
<p>The size of the textures used for rendering the shadow casters into can be altered; clearly using larger textures will give you better quality shadows, but at the expense of greater memory usage. Changing the texture size is done by calling SceneManager::setShadowTextureSize - textures are assumed to be square and you must specify a texture size that is a power of 2. Be aware that each modulative shadow texture will take size*size*3 bytes of texture memory. <b>Important</b>: if you use the GL render system your shadow texture size can only be larger (in either dimension) than the size of your primary window surface if the hardware supports the Frame Buffer Object (FBO) or Pixel Buffer Object (PBO) extensions. Most modern cards support this now, but be careful of older cards - you can check the ability of the hardware to manage this through ogreRoot-&gt;getRenderSystem()-&gt;getCapabilities()-&gt;hasCapability(RSC_HWRENDER_TO_TEXTURE). If this returns false, if you create a shadow texture larger in any dimension than the primary surface, the rest of the shadow texture will be blank.</p>
<p><a class="anchor" id="Shadow-far-distance"></a><a class="anchor" id="Shadow-far-distance-1"></a></p>
<h2>Shadow far distance</h2>
<p>This determines the distance at which shadows are terminated; it also determines how far into the distance the texture shadows for directional lights are stretched - by reducing this value, or increasing the texture size, you can improve the quality of shadows from directional lights at the expense of closer shadow termination or increased memory usage, respectively.</p>
<p><a class="anchor" id="Shadow-texture-offset-Directional-Lights_0029"></a></p>
<h2>Shadow texture offset (Directional Lights)</h2>
<p>As mentioned above in the directional lights section, the rendering of shadows for directional lights is an approximation that allows us to use a single render to cover a largish area with shadows. This offset parameter affects how far from the camera position the center of the shadow texture is offset, as a proportion of the shadow far distance. The greater this value, the more of the shadow texture is ’useful’ to you since it’s ahead of the camera, but also the further you offset it, the more chance there is of accidentally seeing the edge of the shadow texture at more extreme angles. You change this value by calling SceneManager::setShadowDirLightTextureOffset, the default is 0.6.</p>
<p><a class="anchor" id="Shadow-fade-settings"></a><a class="anchor" id="Shadow-fade-settings-1"></a></p>
<h2>Shadow fade settings</h2>
<p>Shadows fade out before the shadow far distance so that the termination of shadow is not abrupt. You can configure the start and end points of this fade by calling the SceneManager::setShadowTextureFadeStart and SceneManager::setShadowTextureFadeEnd methods, both take distances as a proportion of the shadow far distance. Because of the inaccuracies caused by using a square texture and a radial fade distance, you cannot use 1.0 as the fade end, if you do you’ll see artifacts at the extreme edges. The default values are 0.7 and 0.9, which serve most purposes but you can change them if you like.</p>
<h1>Texture shadows and vertex / fragment programs</h1>
<p>When rendering shadow casters into a modulative shadow texture, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> turns off all textures, and all lighting contributions except for ambient light, which it sets to the colour of the shadow (<a href="#Shadow-Colour">Shadow Colour</a>). For additive shadows, it render the casters into a black &amp; white texture instead. This is enough to render shadow casters for fixed-function material techniques, however where a vertex program is used <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> doesn’t have so much control. If you use a vertex program in the <b>first pass</b> of your technique, then you must also tell ogre which vertex program you want it to use when rendering the shadow caster; see <a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a> for full details.</p>
<p><a class="anchor" id="Custom-shadow-camera-setups"></a><a class="anchor" id="Custom-shadow-camera-setups-1"></a></p>
<h2>Custom shadow camera setups</h2>
<p>As previously mentioned, one of the downsides of texture shadows is that the texture resolution is finite, and it’s possible to get aliasing when the size of the shadow texel is larger than a screen pixel, due to the projection of the texture. In order to address this, you can specify alternative projection bases by using or creating subclasses of the ShadowCameraSetup class. The default version is called DefaultShadowCameraSetup and this sets up a simple regular frustum for point and spotlights, and an orthographic frustum for directional lights. There is also a PlaneOptimalShadowCameraSetup class which specialises the projection to a plane, thus giving you much better definition provided your shadow receivers exist mostly in a single plane. Other setup classes (e.g. you might create a perspective or trapezoid shadow mapping version) can be created and plugged in at runtime, either on individual lights or on the SceneManager as a whole.</p>
<p><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing"></a><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing-1"></a></p>
<h2>Shadow texture Depth Buffer sharing</h2>
<p>Shadow textures need a depth buffer like many other RTs (Render Textures). Prior to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8, the depth buffer behavior was left undefined leaving a very small possibility of causing inconsistencies across different window resolutions and render systems. Depending on the render window’s resolutions and/or rendersystem being used, the depth buffer might been shared with the render window or a new one could get created to suite the shadow textures. If the application was depending on the depth buffer contents from the previous scene render (that is, no clear was performed) where a shadow texture render pass was in the middle; then the depth buffer would’ve contained garbage (but not consistent on all machines) causing graphical glitches hard to spot.</p>
<p>From <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8 onwards the depth buffer usage &amp; sharing can be flexible controlled through the use of depth pool IDs. These pool IDs are not specifically part of shadow textures, but rather anything involving RTs. All RTs with the same pool ID share the same depth buffers when possible (following RenderSystem API rules, check RenderSystemCapabilities flags to find out what the behavior will be). The default ID for shadow textures is 1; which is the same default value for render windows, and RTTs; thus maintaining the same behavior with older applications while achieving maximum memory saving and performance efficiency because the number of created depth buffers is as lowest as possible.</p>
<p>However there are some reasons to put shadow textures in a separate pool. This holds specially true if the application depends on the previous contents from the depth buffer before the shadow pass, instead of doing a clear:</p>
<ul>
<li>In Direct3D9, the shadow texture is more likely to share the depth buffer with the render window at high resolutions (when the window is bigger than the shadow texture resolution), but at low resolutions it won’t be shared, thus causing two different behaviors. Also probably the shadow texture will share the depth buffers with most other RTTs (i.e. compositors)</li>
<li>In OpenGL 2.1, the shadow texture can’t be shared with the main render window; and most likely will NOT be shared with many other RTTs (i.e. compositors) since OGL 2.1 has a requirement that texture resolutions should exactly match, while D3D9 specifies depth buffers can be shared as long as the resolutions are equal or less.</li>
</ul>
<p>For example, the DeferredShading sample suffers from this problem. If this is a problem for a particular effect you’re trying to achieve, you can specify a custom pool ID so that shadow textures get their own depth buffer(s), ensuring they aren’t shared with other RTs. You can set the poolId parameter from either SceneManager::setShadowTextureSettings or setShadowTextureConfig</p>
<div class="fragment"><div class="line">mSceneMgr-&gt;setShadowTextureSettings( size, count, format, PoolId );</div><div class="line">mSceneMgr-&gt;setShadowTextureConfig( 0, 512, 512, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39cab63dbcddd53d1f766d742fd557c26be4">PF_FLOAT16_R</a>, 50 );</div></div><!-- fragment --><p>Note a poolId of 0 will make the shadow textures not to use a depth buffer, which isn’t usually a desired behavior.</p>
<p><a class="anchor" id="Integrated-Texture-Shadows"></a><a class="anchor" id="Integrated-Texture-Shadows-1"></a></p>
<h2>Integrated Texture Shadows</h2>
<p>Texture shadows have one major advantage over stencil shadows - the data used to represent them can be referenced in regular shaders. Whilst the default texture shadow modes (SHADOWTYPE_TEXTURE_MODULATIVE and SHADOWTYPE_TEXTURE_ADDITIVE) automatically render shadows for you, their disadvantage is that because they are generalised add-ons to your own materials, they tend to take more passes of the scene to use. In addition, you don’t have a lot of control over the composition of the shadows.</p>
<p>Here is where ’integrated’ texture shadows step in. Both of the texture shadow types above have alternative versions called SHADOWTYPE_TEXTURE_MODULATIVE_INTEGRATED and SHADOWTYPE_TEXTURE_ADDITIVE_INTEGRATED, where instead of rendering the shadows for you, it just creates the texture shadow and then expects you to use that shadow texture as you see fit when rendering receiver objects in the scene. The downside is that you have to take into account shadow receipt in every one of your materials if you use this option - the upside is that you have total control over how the shadow textures are used. The big advantage here is that you can can perform more complex shading, taking into account shadowing, than is possible using the generalised bolt-on approaches, AND you can probably write them in a smaller number of passes, since you know precisely what you need and can combine passes where possible. When you use one of these shadowing approaches, the only difference between additive and modulative is the colour of the casters in the shadow texture (the shadow colour for modulative, black for additive) - the actual calculation of how the texture affects the receivers is of course up to you. No separate modulative pass will be performed, and no splitting of your materials into ambient / per-light / decal etc will occur - absolutely everything is determined by your original material (which may have modulative passes or per-light iteration if you want of course, but it’s not required).</p>
<p>You reference a shadow texture in a material which implements this approach by using the ’<a href="#content_005ftype">content_type</a> shadow’ directive in your <a href="#Texture-Units">texture_unit</a>. It implicitly references a shadow texture based on the number of times you’ve used this directive in the same pass, and the light_start option or light-based pass iteration, which might start the light index higher than 0.</p>
<h1><a class="anchor" id="Modulative-Shadows"></a>
Modulative Shadows</h1>
<p>Modulative shadows work by darkening an already rendered scene with a fixed colour. First, the scene is rendered normally containing all the objects which will be shadowed, then a modulative pass is done per light, which darkens areas in shadow. Finally, objects which do not receive shadows are rendered.</p>
<p>There are 2 modulative shadow techniques; stencil-based (See <a href="#Stencil-Shadows">Stencil Shadows</a> : SHADOWTYPE_STENCIL_MODULATIVE) and texture-based (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a> : SHADOWTYPE_TEXTURE_MODULATIVE). Modulative shadows are an inaccurate lighting model, since they darken the areas of shadow uniformly, irrespective of the amount of light which would have fallen on the shadow area anyway. However, they can give fairly attractive results for a much lower overhead than more ’correct’ methods like <a href="#Additive-Light-Masking">Additive Light Masking</a>, and they also combine well with pre-baked static lighting (such as pre-calculated lightmaps), which additive lighting does not. The main thing to consider is that using multiple light sources can result in overly dark shadows (where shadows overlap, which intuitively looks right in fact, but it’s not physically correct) and artifacts when using stencil shadows (See <a href="#The-Silhouette-Edge">The Silhouette Edge</a>).</p>
<p><a class="anchor" id="Shadow-Colour"></a><a class="anchor" id="Shadow-Colour-1"></a></p>
<h2>Shadow Colour</h2>
<p>The colour which is used to darken the areas in shadow is set by SceneManager::setShadowColour; it defaults to a dark grey (so that the underlying colour still shows through a bit).</p>
<p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to have a separate pass of the scene to render shadows. In this case the ’modulative’ aspect of the shadow technique just affects the colour of the shadow texture.</p>
<h1><a class="anchor" id="Additive-Light-Masking"></a>
Additive Light Masking</h1>
<p>Additive light masking is about rendering the scene many times, each time representing a single light contribution whose influence is masked out in areas of shadow. Each pass is combined with (added to) the previous one such that when all the passes are complete, all the light contribution has correctly accumulated in the scene, and each light has been prevented from affecting areas which it should not be able to because of shadow casters. This is an effective technique which results in very realistic looking lighting, but it comes at a price: more rendering passes.</p>
<p>As many technical papers (and game marketing) will tell you, rendering realistic lighting like this requires multiple passes. Being a friendly sort of engine, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> frees you from most of the hard work though, and will let you use the exact same material definitions whether you use this lighting technique or not (for the most part, see <a href="#Pass-Classification-and-Vertex-Programs">Pass Classification and Vertex Programs</a>). In order to do this technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically categorises the <a href="#Passes">Passes</a> you define in your materials into 3 types:</p>
<ol type="1">
<li>ambient Passes categorised as ’ambient’ include any base pass which is not lit by any particular light, i.e. it occurs even if there is no ambient light in the scene. The ambient pass always happens first, and sets up the initial depth value of the fragments, and the ambient colour if applicable. It also includes any emissive / self illumination contribution. Only textures which affect ambient light (e.g. ambient occlusion maps) should be rendered in this pass.</li>
<li>diffuse/specular Passes categorised as ’diffuse/specular’ (or ’per-light’) are rendered once per light, and each pass contributes the diffuse and specular colour from that single light as reflected by the diffuse / specular terms in the pass. Areas in shadow from that light are masked and are thus not updated. The resulting masked colour is added to the existing colour in the scene. Again, no textures are used in this pass (except for textures used for lighting calculations such as normal maps).</li>
<li>decal Passes categorised as ’decal’ add the final texture colour to the scene, which is modulated by the accumulated light built up from all the ambient and diffuse/specular passes.</li>
</ol>
<p>In practice, <a href="#Passes">Passes</a> rarely fall nicely into just one of these categories. For each Technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> compiles a list of ’Illumination Passes’, which are derived from the user defined passes, but can be split, to ensure that the divisions between illumination pass categories can be maintained. For example, if we take a very simple material definition:</p>
<div class="fragment"><div class="line">material TestIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will split this into 3 illumination passes, which will be the equivalent of this:</p>
<div class="fragment"><div class="line">material TestIlluminationSplitIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 0 0 0</div><div class="line">            specular 0 0 0</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Diffuse / specular pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend add</div><div class="line">            iteration once_per_light</div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend modulate</div><div class="line">            lighting off</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>So as you can see, even a simple material requires a minimum of 3 passes when using this shadow technique, and in fact it requires (num_lights + 2) passes in the general sense. You can use more passes in your original material and <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will cope with that too, but be aware that each pass may turn into multiple ones if it uses more than one type of light contribution (ambient vs diffuse/specular) and / or has texture units. The main nice thing is that you get the full multipass lighting behaviour even if you don’t define your materials in terms of it, meaning that your material definitions can remain the same no matter what lighting approach you decide to use.</p>
<p><a class="anchor" id="Manually-Categorising-Illumination-Passes"></a><a class="anchor" id="Manually-Categorising-Illumination-Passes-1"></a></p>
<h2>Manually Categorising Illumination Passes</h2>
<p>Alternatively, if you want more direct control over the categorisation of your passes, you can use the <a href="#illumination_005fstage">illumination_stage</a> option in your pass to explicitly assign a pass unchanged to an illumination stage. This way you can make sure you know precisely how your material will be rendered under additive lighting conditions.</p>
<p><a class="anchor" id="Pass-Classification-and-Vertex-Programs"></a><a class="anchor" id="Pass-Classification-and-Vertex-Programs-1"></a></p>
<h2>Pass Classification and Vertex Programs</h2>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at classifying and splitting your passes to ensure that the multipass rendering approach required by additive lighting works correctly without you having to change your material definitions. However, there is one exception; when you use vertex programs, the normal lighting attributes ambient, diffuse, specular etc are not used, because all of that is determined by the vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has no way of knowing what you’re doing inside that vertex program, so you have to tell it.</p>
<p>In practice this is very easy. Even though your vertex program could be doing a lot of complex, highly customised processing, it can still be classified into one of the 3 types listed above. All you need to do to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> what you’re doing is to use the pass attributes ambient, diffuse, specular and self_illumination, just as if you were not using a vertex program. Sure, these attributes do nothing (as far as rendering is concerned) when you’re using vertex programs, but it’s the easiest way to indicate to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> which light components you’re using in your vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will then classify and potentially split your programmable pass based on this information - it will leave the vertex program as-is (so that any split passes will respect any vertex modification that is being done).</p>
<p>Note that when classifying a diffuse/specular programmable pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> checks to see whether you have indicated the pass can be run once per light (iteration once_per_light). If so, the pass is left intact, including it’s vertex and fragment programs. However, if this attribute is not included in the pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> tries to split off the per-light part, and in doing so it will disable the fragment program, since in the absence of the ’iteration once_per_light’ attribute it can only assume that the fragment program is performing decal work and hence must not be used per light.</p>
<p>So clearly, when you use additive light masking as a shadow technique, you need to make sure that programmable passes you use are properly set up so that they can be classified correctly. However, also note that the changes you have to make to ensure the classification is correct does not affect the way the material renders when you choose not to use additive lighting, so the principle that you should be able to use the same material definitions for all lighting scenarios still holds. Here is an example of a programmable material which will be classified correctly by the illumination pass classifier:</p>
<div class="fragment"><div class="line"><span class="comment">// Per-pixel normal mapping Any number of lights, diffuse and specular</span></div><div class="line">material Examples/BumpMapping/MultiLightSpecular</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Base ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// ambient only, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 1 1 1</div><div class="line">            diffuse 0 0 0 </div><div class="line">            specular 0 0 0 0</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named_auto ambient ambient_light_colour</div><div class="line">            }</div><div class="line">        }</div><div class="line">        <span class="comment">// Now do the lighting pass</span></div><div class="line">        <span class="comment">// NB we don&#39;t do decal texture here because this is repeated per light</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// set ambient off, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 0 0 0 </div><div class="line">            <span class="comment">// do this for each light</span></div><div class="line">            iteration once_per_light</div><div class="line">            scene_blend add</div><div class="line"></div><div class="line">            <span class="comment">// Vertex program reference</span></div><div class="line">            vertex_program_ref Examples/BumpMapVPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightPosition light_position_object_space 0</div><div class="line">                param_named_auto eyePosition camera_position_object_space</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Fragment program</span></div><div class="line">            fragment_program_ref Examples/BumpMapFPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Base bump map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture NMBumpsOut.png</div><div class="line">                colour_op replace</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map #2</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            lighting off</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named ambient float4 1 1 1 1</div><div class="line">            }</div><div class="line">            scene_blend dest_colour zero</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture RustedMetal.jpg </div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to use this explicit sequence - allowing you to compress the number of passes into a much smaller number at the expense of defining an upper number of shadow casting lights. In this case the ’additive’ aspect of the shadow technique just affects the colour of the shadow texture and it’s up to you to combine the shadow textures in your receivers however you like.</p>
<p><a class="anchor" id="Static-Lighting"></a></p>
<h2>Static Lighting</h2>
<p>Despite their power, additive lighting techniques have an additional limitation; they do not combine well with pre-calculated static lighting in the scene. This is because they are based on the principle that shadow is an absence of light, but since static lighting in the scene already includes areas of light and shadow, additive lighting cannot remove light to create new shadows. Therefore, if you use the additive lighting technique you must either use it exclusively as your lighting solution (and you can combine it with per-pixel lighting to create a very impressive dynamic lighting solution), or you must use <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> to combine the static lighting according to your chosen approach. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Shadows</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_shadows.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Shadows </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Enabling-shadows">Enabling shadows</a></li>
<li class="level1"><a href="#Opting-out-of-shadows">Opting out of shadows</a></li>
<li class="level1"><a href="#Stencil-Shadows">Stencil Shadows</a></li>
<li class="level1"><a href="#Texture_002dbased-Shadows">Texture-based Shadows</a></li>
<li class="level1"><a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a></li>
<li class="level1"><a href="#Modulative-Shadows">Modulative Shadows</a></li>
<li class="level1"><a href="#Additive-Light-Masking">Additive Light Masking</a></li>
</ul>
</div>
<div class="textblock"><p>Shadows are clearly an important part of rendering a believable scene - they provide a more tangible feel to the objects in the scene, and aid the viewer in understanding the spatial relationship between objects. Unfortunately, shadows are also one of the most challenging aspects of 3D rendering, and they are still very much an active area of research. Whilst there are many techniques to render shadows, none is perfect and they all come with advantages and disadvantages. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides multiple shadow implementations, with plenty of configuration settings, so you can choose which technique is most appropriate for your scene.</p>
<p>Shadow implementations fall into basically 2 broad categories: <a href="#Stencil-Shadows">Stencil Shadows</a> and <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>. This describes the method by which the shape of the shadow is generated. In addition, there is more than one way to render the shadow into the scene: <a href="#Modulative-Shadows">Modulative Shadows</a>, which darkens the scene in areas of shadow, and <a href="#Additive-Light-Masking">Additive Light Masking</a> which by contrast builds up light contribution in areas which are not in shadow. You also have the option of <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> which gives you complete control over texture shadow application, allowing for complex single-pass shadowing shaders. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports all these combinations.</p>
<h1><a class="anchor" id="Enabling-shadows"></a>
Enabling shadows</h1>
<p>Shadows are disabled by default, here’s how you turn them on and configure them in the general sense:</p>
<ol type="1">
<li><p class="startli">Enable a shadow technique on the SceneManager as the <b>first</b> thing you doing your scene setup. It is important that this is done first because the shadow technique can alter the way meshes are loaded. Here’s an example:</p>
<p class="startli"><code>mSceneMgr-&gt;setShadowTechnique(SHADOWTYPE_STENCIL_ADDITIVE);</code></p>
</li>
<li>Create one or more lights. Note that not all light types are necessarily supported by all shadow techniques, you should check the sections about each technique to check. Note that if certain lights should not cast shadows, you can turn that off by calling setCastShadows(false) on the light, the default is true.</li>
<li>Disable shadow casting on objects which should not cast shadows. Call setCastShadows(false) on objects you don’t want to cast shadows, the default for all objects is to cast shadows.</li>
<li>Configure shadow far distance. You can limit the distance at which shadows are considered for performance reasons, by calling SceneManager::setShadowFarDistance.</li>
<li>Turn off the receipt of shadows on materials that should not receive them. You can turn off the receipt of shadows (note, not the casting of shadows - that is done per-object) by calling Material::setReceiveShadows or using the receive_shadows material attribute. This is useful for materials which should be considered self-illuminated for example. Note that transparent materials are typically excluded from receiving and casting shadows, although see the <a href="#transparency_005fcasts_005fshadows">transparency_casts_shadows</a> option for exceptions.</li>
</ol>
<h1><a class="anchor" id="Opting-out-of-shadows"></a>
Opting out of shadows</h1>
<p>By default <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> treats all non-transparent objects as shadow casters and receivers (depending on the shadow technique they may not be able to be both at once, check the docs for your chosen technique first). You can disable shadows in various ways:</p>
<dl compact="compact">
<dt>Turning off shadow casting on the light </dt>
<dd><p class="startdd"></p>
<p>Calling Light::setCastsShadows(false) will mean this light casts no shadows at all.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow receipt on a material </dt>
<dd><p class="startdd"></p>
<p>Calling Material::setReceiveShadows(false) will prevent any objects using this material from receiving shadows.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow casting on individual objects </dt>
<dd><p class="startdd"></p>
<p>Calling MovableObject::setCastsShadows(false) will disable shadow casting for this object.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadows on an entire rendering queue group </dt>
<dd><p class="startdd"></p>
<p>Calling RenderQueueGroup::setShadowsEnabled(false) will turn off both shadow casting and receiving on an entire rendering queue group. This is useful because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has to do light setup tasks per group in order to preserve the inter-group ordering. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically disables shadows on a number of groups automatically, such as RENDER_QUEUE_BACKGROUND, RENDER_QUEUE_OVERLAY, RENDER_QUEUE_SKIES_EARLY and RENDER_QUEUE_SKIES_LATE. If you choose to use more rendering queues (and by default, you won’t be using any more than this plus the ’standard’ queue, so ignore this if you don’t know what it means!), be aware that each one can incur a light setup cost, and you should disable shadows on the additional ones you use if you can.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Stencil-Shadows"></a>
Stencil Shadows</h1>
<p>Stencil shadows are a method by which a ’mask’ is created for the screen using a feature called the stencil buffer. This mask can be used to exclude areas of the screen from subsequent renders, and thus it can be used to either include or exclude areas in shadow. They are enabled by calling SceneManager::setShadowTechnique with a parameter of either <code>SHADOWTYPE_STENCIL_ADDITIVE</code> or <code>SHADOWTYPE_STENCIL_MODULATIVE</code>. Because the stencil can only mask areas to be either ’enabled’ or ’disabled’, stencil shadows have ’hard’ edges, that is to say clear dividing lines between light and shadow - it is not possible to soften these edges.</p>
<p>In order to generate the stencil, ’shadow volumes’ are rendered by extruding the silhouette of the shadow caster away from the light. Where these shadow volumes intersect other objects (or the caster, since self-shadowing is supported using this technique), the stencil is updated, allowing subsequent operations to differentiate between light and shadow. How exactly this is used to render the shadows depends on whether <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> is being used. Objects can both cast and receive stencil shadows, so self-shadowing is inbuilt.</p>
<p>The advantage of stencil shadows is that they can do self-shadowing simply on low-end hardware, provided you keep your poly count under control. In contrast doing self-shadowing with texture shadows requires a fairly modern machine (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>). For this reason, you’re likely to pick stencil shadows if you need an accurate shadowing solution for an application aimed at older or lower-spec machines.</p>
<p>The disadvantages of stencil shadows are numerous though, especially on more modern hardware. Because stencil shadows are a geometric technique, they are inherently more costly the higher the number of polygons you use, meaning you are penalized the more detailed you make your meshes. The fillrate cost, which comes from having to render shadow volumes, also escalates the same way. Since more modern applications are likely to use higher polygon counts, stencil shadows can start to become a bottleneck. In addition, the visual aspects of stencil shadows are pretty primitive - your shadows will always be hard-edged, and you have no possibility of doing clever things with shaders since the stencil is not available for manipulation there. Therefore, if your application is aimed at higher-end machines you should definitely consider switching to texture shadows (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>).</p>
<p>There are a number of issues to consider which are specific to stencil shadows:</p>
<ul>
<li><a href="#CPU-Overhead">CPU Overhead</a></li>
<li><a href="#Extrusion-distance">Extrusion distance</a></li>
<li><a href="#Camera-far-plane-positioning">Camera far plane positioning</a></li>
<li><a href="#Mesh-edge-lists">Mesh edge lists</a></li>
<li><a href="#The-Silhouette-Edge">The Silhouette Edge</a></li>
<li><a href="#Be-realistic">Be realistic</a></li>
<li><a href="#Stencil-Optimisations-Performed-By-Ogre">Stencil Optimisations Performed By Ogre</a></li>
</ul>
<p><a class="anchor" id="CPU-Overhead"></a><a class="anchor" id="CPU-Overhead-1"></a></p>
<h2>CPU Overhead</h2>
<p>Calculating the shadow volume for a mesh can be expensive, and it has to be done on the CPU, it is not a hardware accelerated feature. Therefore, you can find that if you overuse this feature, you can create a CPU bottleneck for your application. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> quite aggressively eliminates objects which cannot be casting shadows on the frustum, but there are limits to how much it can do, and large, elongated shadows (e.g. representing a very low sun position) are very difficult to cull efficiently. Try to avoid having too many shadow casters around at once, and avoid long shadows if you can. Also, make use of the ’shadow far distance’ parameter on the SceneManager, this can eliminate distant shadow casters from the shadow volume construction and save you some time, at the expense of only having shadows for closer objects. Lastly, make use of Ogre’s Level-Of-Detail (LOD) features; you can generate automatically calculated LODs for your meshes in code (see the Mesh API docs) or when using the mesh tools such as OgreXMLConverter and OgreMeshUpgrader. Alternatively, you can assign your own manual LODs by providing alternative mesh files at lower detail levels. Both methods will cause the shadow volume complexity to decrease as the object gets further away, which saves you valuable volume calculation time.</p>
<p><a class="anchor" id="Extrusion-distance"></a><a class="anchor" id="Extrusion-distance-1"></a></p>
<h2>Extrusion distance</h2>
<p>When vertex programs are not available, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only extrude shadow volumes a finite distance from the object. If an object gets too close to a light, any finite extrusion distance will be inadequate to guarantee all objects will be shadowed properly by this object. Therefore, you are advised not to let shadow casters pass too close to light sources if you can avoid it, unless you can guarantee that your target audience will have vertex program capable hardware (in this case, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> extrudes the volume to infinity using a vertex program so the problem does not occur). When infinite extrusion is not possible, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses finite extrusion, either derived from the attenuation range of a light (in the case of a point light or spotlight), or a fixed extrusion distance set in the application in the case of directional lights. To change the directional light extrusion distance, use SceneManager::setShadowDirectionalLightExtrusionDistance.</p>
<p><a class="anchor" id="Camera-far-plane-positioning"></a><a class="anchor" id="Camera-far-plane-positioning-1"></a></p>
<h2>Camera far plane positioning</h2>
<p>Stencil shadow volumes rely very much on not being clipped by the far plane. When you enable stencil shadows, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> internally changes the far plane settings of your cameras such that there is no far plane - i.e. it is placed at infinity (Camera::setFarClipDistance(0)). This avoids artifacts caused by clipping the dark caps on shadow volumes, at the expense of a (very) small amount of depth precision.</p>
<p><a class="anchor" id="Mesh-edge-lists"></a><a class="anchor" id="Mesh-edge-lists-1"></a></p>
<h2>Mesh edge lists</h2>
<p>Stencil shadows can only be calculated when an ’edge list’ has been built for all the geometry in a mesh. The official exporters and tools automatically build this for you (or have an option to do so), but if you create your own meshes, you must remember to build edge lists for them before using them with stencil shadows - you can do that by using OgreMeshUpgrade or OgreXmlConverter, or by calling Mesh::buildEdgeList before you export or use the mesh. If a mesh doesn’t have edge lists, OGRE assumes that it is not supposed to cast stencil shadows.</p>
<p><a class="anchor" id="The-Silhouette-Edge"></a><a class="anchor" id="The-Silhouette-Edge-1"></a></p>
<h2>The Silhouette Edge</h2>
<p>Stencil shadowing is about finding a silhouette of the mesh, and projecting it away to form a volume. What this means is that there is a definite boundary on the shadow caster between light and shadow; a set of edges where where the triangle on one side is facing toward the light, and one is facing away. This produces a sharp edge around the mesh as the transition occurs. Provided there is little or no other light in the scene, and the mesh has smooth normals to produce a gradual light change in its underlying shading, the silhouette edge can be hidden - this works better the higher the tessellation of the mesh. However, if the scene includes ambient light, then the difference is far more marked. This is especially true when using <a href="#Modulative-Shadows">Modulative Shadows</a>, because the light contribution of each shadowed area is not taken into account by this simplified approach, and so using 2 or more lights in a scene using modulative stencil shadows is not advisable; the silhouette edges will be very marked. Additive lights do not suffer from this as badly because each light is masked individually, meaning that it is only ambient light which can show up the silhouette edges.</p>
<p><a class="anchor" id="Be-realistic"></a><a class="anchor" id="Be-realistic-1"></a></p>
<h2>Be realistic</h2>
<p>Don’t expect to be able to throw any scene using any hardware at the stencil shadow algorithm and expect to get perfect, optimum speed results. Shadows are a complex and expensive technique, so you should impose some reasonable limitations on your placing of lights and objects; they’re not really that restricting, but you should be aware that this is not a complete free-for-all.</p>
<ul>
<li>Try to avoid letting objects pass very close (or even through) lights - it might look nice but it’s one of the cases where artifacts can occur on machines not capable of running vertex programs.</li>
<li>Be aware that shadow volumes do not respect the ’solidity’ of the objects they pass through, and if those objects do not themselves cast shadows (which would hide the effect) then the result will be that you can see shadows on the other side of what should be an occluding object.</li>
<li>Make use of SceneManager::setShadowFarDistance to limit the number of shadow volumes constructed</li>
<li>Make use of LOD to reduce shadow volume complexity at distance</li>
<li><p class="startli">Avoid very long (dusk and dawn) shadows - they exacerbate other issues such as volume clipping, fillrate, and cause many more objects at a greater distance to require volume construction.</p>
<p class="startli"><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre"></a><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre-1"></a></p>
</li>
</ul>
<h2>Stencil Optimisations Performed By <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a></h2>
<p>Despite all that, stencil shadows can look very nice (especially with <a href="#Additive-Light-Masking">Additive Light Masking</a>) and can be fast if you respect the rules above. In addition, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> comes pre-packed with a lot of optimisations which help to make this as quick as possible. This section is more for developers or people interested in knowing something about the ’under the hood’ behaviour of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl compact="compact">
<dt>Vertex program extrusion </dt>
<dd><p class="startdd"></p>
<p>As previously mentioned, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> performs the extrusion of shadow volumes in hardware on vertex program-capable hardware (e.g. GeForce3, Radeon 8500 or better). This has 2 major benefits; the obvious one being speed, but secondly that vertex programs can extrude points to infinity, which the fixed-function pipeline cannot, at least not without performing all calculations in software. This leads to more robust volumes, and also eliminates more than half the volume triangles on directional lights since all points are projected to a single point at infinity.</p>
<p class="enddd"></p>
</dd>
<dt>Scissor test optimisation </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a scissor rectangle to limit the effect of point / spot lights when their range does not cover the entire viewport; that means we save fillrate when rendering stencil volumes, especially with distant lights</p>
<p class="enddd"></p>
</dd>
<dt>Z-Pass and Z-Fail algorithms </dt>
<dd><p class="startdd"></p>
<p>The Z-Fail algorithm, often attributed to John Carmack, is used in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to make sure shadows are robust when the camera passes through the shadow volume. However, the Z-Fail algorithm is more expensive than the traditional Z-Pass; so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> detects when Z-Fail is required and only uses it then, Z-Pass is used at all other times.</p>
<p class="enddd"></p>
</dd>
<dt>2-Sided stenciling and stencil wrapping </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports the 2-Sided stenciling / stencil wrapping extensions, which when supported allow volumes to be rendered in a single pass instead of having to do one pass for back facing tris and another for front-facing tris. This doesn’t save fillrate, since the same number of stencil updates are done, but it does save primitive setup and the overhead incurred in the driver every time a render call is made.</p>
<p class="enddd"></p>
</dd>
<dt>Aggressive shadow volume culling </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at detecting which lights could be affecting the frustum, and from that, which objects could be casting a shadow on the frustum. This means we don’t waste time constructing shadow geometry we don’t need. Setting the shadow far distance is another important way you can reduce stencil shadow overhead since it culls far away shadow volumes even if they are visible, which is beneficial in practice since you’re most interested in shadows for close-up objects.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Texture_002dbased-Shadows"></a>
Texture-based Shadows</h1>
<p>Texture shadows involve rendering shadow casters from the point of view of the light into a texture, which is then projected onto shadow receivers. The main advantage of texture shadows as opposed to <a href="#Stencil-Shadows">Stencil Shadows</a> is that the overhead of increasing the geometric detail is far lower, since there is no need to perform per-triangle calculations. Most of the work in rendering texture shadows is done by the graphics card, meaning the technique scales well when taking advantage of the latest cards, which are at present outpacing CPUs in terms of their speed of development. In addition, texture shadows are <b>much</b> more customisable - you can pull them into shaders to apply as you like (particularly with <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a>, you can perform filtering to create softer shadows or perform other special effects on them. Basically, most modern engines use texture shadows as their primary shadow technique simply because they are more powerful, and the increasing speed of GPUs is rapidly amortizing the fillrate / texture access costs of using them.</p>
<p>The main disadvantage to texture shadows is that, because they are simply a texture, they have a fixed resolution which means if stretched, the pixellation of the texture can become obvious. There are ways to combat this though:</p>
<dl compact="compact">
<dt>Choosing a projection basis </dt>
<dd><p class="startdd"></p>
<p>The simplest projection is just to render the shadow casters from the lights perspective using a regular camera setup. This can look bad though, so there are many other projections which can help to improve the quality from the main camera’s perspective. OGRE supports pluggable projection bases via it’s ShadowCameraSetup class, and comes with several existing options - <b>Uniform</b> (which is the simplest), <b>Uniform Focussed</b> (which is still a normal camera projection, except that the camera is focussed into the area that the main viewing camera is looking at), LiSPSM (Light Space Perspective Shadow Mapping - which both focusses and distorts the shadow frustum based on the main view camera) and Plan Optimal (which seeks to optimise the shadow fidelity for a single receiver plane).</p>
<p class="enddd"></p>
</dd>
<dt>Filtering </dt>
<dd><p class="startdd"></p>
<p>You can also sample the shadow texture multiple times rather than once to soften the shadow edges and improve the appearance. Percentage Closest Filtering (PCF) is the most popular approach, although there are multiple variants depending on the number and pattern of the samples you take. Our shadows demo includes a 5-tap PCF example combined with depth shadow mapping.</p>
<p class="enddd"></p>
</dd>
<dt>Using a larger texture </dt>
<dd><p class="startdd"></p>
<p>Again as GPUs get faster and gain more memory, you can scale up to take advantage of this.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If you combine all 3 of these techniques you can get a very high quality shadow solution.</p>
<p>The other issue is with point lights. Because texture shadows require a render to texture in the direction of the light, omnidirectional lights (point lights) would require 6 renders to totally cover all the directions shadows might be cast. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> primarily supports directional lights and spotlights for generating texture shadows; you can use point lights but they will only work if off-camera since they are essentially turned into a spotlight shining into your camera frustum for the purposes of texture shadows.</p>
<p><a class="anchor" id="Directional-Lights"></a></p>
<h2>Directional Lights</h2>
<p>Directional lights in theory shadow the entire scene from an infinitely distant light. Now, since we only have a finite texture which will look very poor quality if stretched over the entire scene, clearly a simplification is required. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> places a shadow texture over the area immediately in front of the camera, and moves it as the camera moves (although it rounds this movement to multiples of texels so that the slight ’swimming shadow’ effect caused by moving the texture is minimised). The range to which this shadow extends, and the offset used to move it in front of the camera, are configurable (See <a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a>). At the far edge of the shadow, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> fades out the shadow based on other configurable parameters so that the termination of the shadow is softened.</p>
<p><a class="anchor" id="Spotlights"></a></p>
<h2>Spotlights</h2>
<p>Spotlights are much easier to represent as renderable shadow textures than directional lights, since they are naturally a frustum. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> represents spotlight directly by rendering the shadow from the light position, in the direction of the light cone; the field-of-view of the texture camera is adjusted based on the spotlight falloff angles. In addition, to hide the fact that the shadow texture is square and has definite edges which could show up outside the spotlight, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a second texture unit when projecting the shadow onto the scene which fades out the shadow gradually in a projected circle around the spotlight.</p>
<p><a class="anchor" id="Point-Lights"></a></p>
<h2>Point Lights</h2>
<p>As mentioned above, to support point lights properly would require multiple renders (either 6 for a cubic render or perhaps 2 for a less precise parabolic mapping), so rather than do that we approximate point lights as spotlights, where the configuration is changed on the fly to make the light shine from its position over the whole of the viewing frustum. This is not an ideal setup since it means it can only really work if the point light’s position is out of view, and in addition the changing parameterisation can cause some ’swimming’ of the texture. Generally we recommend avoiding making point lights cast texture shadows.</p>
<p><a class="anchor" id="Shadow-Casters-and-Shadow-Receivers"></a></p>
<h2>Shadow Casters and Shadow Receivers</h2>
<p>To enable texture shadows, use the shadow technique SHADOWTYPE_TEXTURE_MODULATIVE or SHADOWTYPE_TEXTURE_ADDITIVE; as the name suggests this produces <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> respectively. The cheapest and simplest texture shadow techniques do not use depth information, they merely render casters to a texture and render this onto receivers as plain colour - this means self-shadowing is not possible using these methods. This is the default behaviour if you use the automatic, fixed-function compatible (and thus usable on lower end hardware) texture shadow techniques. You can however use shaders-based techniques through custom shadow materials for casters and receivers to perform more complex shadow algorithms, such as depth shadow mapping which does allow self-shadowing. OGRE comes with an example of this in its shadows demo, although it’s only usable on Shader Model 2 cards or better. Whilst fixed-function depth shadow mapping is available in OpenGL, it was never standardised in Direct3D so using shaders in custom caster &amp; receiver materials is the only portable way to do it. If you use this approach, call SceneManager::setShadowTextureSelfShadow with a parameter of ’true’ to allow texture shadow casters to also be receivers. If you’re not using depth shadow mapping, OGRE divides shadow casters and receivers into 2 disjoint groups. Simply by turning off shadow casting on an object, you automatically make it a shadow receiver (although this can be disabled by setting the ’receive_shadows’ option to ’false’ in a material script. Similarly, if an object is set as a shadow caster, it cannot receive shadows.</p>
<h1><a class="anchor" id="Configuring-Texture-Shadows"></a>
Configuring Texture Shadows</h1>
<p>There are a number of settings which will help you configure your texture-based shadows so that they match your requirements.</p>
<ul>
<li><a href="#Maximum-number-of-shadow-textures">Maximum number of shadow textures</a></li>
<li><a href="#Shadow-texture-size">Shadow texture size</a></li>
<li><a href="#Shadow-far-distance">Shadow far distance</a></li>
<li><a href="#Shadow-texture-offset-Directional-Lights_0029">Shadow texture offset (Directional Lights)</a></li>
<li><a href="#Shadow-fade-settings">Shadow fade settings</a></li>
<li><a href="#Custom-shadow-camera-setups">Custom shadow camera setups</a></li>
<li><a href="#Shadow-texture-Depth-Buffer-sharing">Shadow texture Depth Buffer sharing</a></li>
<li><a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a></li>
</ul>
<p><a class="anchor" id="Maximum-number-of-shadow-textures"></a><a class="anchor" id="Maximum-number-of-shadow-textures-1"></a></p>
<h2>Maximum number of shadow textures</h2>
<p>Shadow textures take up texture memory, and to avoid stalling the rendering pipeline <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> does not reuse the same shadow texture for multiple lights within the same frame. This means that each light which is to cast shadows must have its own shadow texture. In practice, if you have a lot of lights in your scene you would not wish to incur that sort of texture overhead. You can adjust this manually by simply turning off shadow casting for lights you do not wish to cast shadows. In addition, you can set a maximum limit on the number of shadow textures <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is allowed to use by calling SceneManager::setShadowTextureCount. Each frame, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> determines the lights which could be affecting the frustum, and then allocates the number of shadow textures it is allowed to use to the lights on a first-come-first-served basis. Any additional lights will not cast shadows that frame. Note that you can set the number of shadow textures and their size at the same time by using the SceneManager::setShadowTextureSettings method; this is useful because both the individual calls require the potential creation / destruction of texture resources.</p>
<p><a class="anchor" id="Shadow-texture-size"></a><a class="anchor" id="Shadow-texture-size-1"></a></p>
<h2>Shadow texture size</h2>
<p>The size of the textures used for rendering the shadow casters into can be altered; clearly using larger textures will give you better quality shadows, but at the expense of greater memory usage. Changing the texture size is done by calling SceneManager::setShadowTextureSize - textures are assumed to be square and you must specify a texture size that is a power of 2. Be aware that each modulative shadow texture will take size*size*3 bytes of texture memory. <b>Important</b>: if you use the GL render system your shadow texture size can only be larger (in either dimension) than the size of your primary window surface if the hardware supports the Frame Buffer Object (FBO) or Pixel Buffer Object (PBO) extensions. Most modern cards support this now, but be careful of older cards - you can check the ability of the hardware to manage this through ogreRoot-&gt;getRenderSystem()-&gt;getCapabilities()-&gt;hasCapability(RSC_HWRENDER_TO_TEXTURE). If this returns false, if you create a shadow texture larger in any dimension than the primary surface, the rest of the shadow texture will be blank.</p>
<p><a class="anchor" id="Shadow-far-distance"></a><a class="anchor" id="Shadow-far-distance-1"></a></p>
<h2>Shadow far distance</h2>
<p>This determines the distance at which shadows are terminated; it also determines how far into the distance the texture shadows for directional lights are stretched - by reducing this value, or increasing the texture size, you can improve the quality of shadows from directional lights at the expense of closer shadow termination or increased memory usage, respectively.</p>
<p><a class="anchor" id="Shadow-texture-offset-Directional-Lights_0029"></a></p>
<h2>Shadow texture offset (Directional Lights)</h2>
<p>As mentioned above in the directional lights section, the rendering of shadows for directional lights is an approximation that allows us to use a single render to cover a largish area with shadows. This offset parameter affects how far from the camera position the center of the shadow texture is offset, as a proportion of the shadow far distance. The greater this value, the more of the shadow texture is ’useful’ to you since it’s ahead of the camera, but also the further you offset it, the more chance there is of accidentally seeing the edge of the shadow texture at more extreme angles. You change this value by calling SceneManager::setShadowDirLightTextureOffset, the default is 0.6.</p>
<p><a class="anchor" id="Shadow-fade-settings"></a><a class="anchor" id="Shadow-fade-settings-1"></a></p>
<h2>Shadow fade settings</h2>
<p>Shadows fade out before the shadow far distance so that the termination of shadow is not abrupt. You can configure the start and end points of this fade by calling the SceneManager::setShadowTextureFadeStart and SceneManager::setShadowTextureFadeEnd methods, both take distances as a proportion of the shadow far distance. Because of the inaccuracies caused by using a square texture and a radial fade distance, you cannot use 1.0 as the fade end, if you do you’ll see artifacts at the extreme edges. The default values are 0.7 and 0.9, which serve most purposes but you can change them if you like.</p>
<h1>Texture shadows and vertex / fragment programs</h1>
<p>When rendering shadow casters into a modulative shadow texture, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> turns off all textures, and all lighting contributions except for ambient light, which it sets to the colour of the shadow (<a href="#Shadow-Colour">Shadow Colour</a>). For additive shadows, it render the casters into a black &amp; white texture instead. This is enough to render shadow casters for fixed-function material techniques, however where a vertex program is used <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> doesn’t have so much control. If you use a vertex program in the <b>first pass</b> of your technique, then you must also tell ogre which vertex program you want it to use when rendering the shadow caster; see <a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a> for full details.</p>
<p><a class="anchor" id="Custom-shadow-camera-setups"></a><a class="anchor" id="Custom-shadow-camera-setups-1"></a></p>
<h2>Custom shadow camera setups</h2>
<p>As previously mentioned, one of the downsides of texture shadows is that the texture resolution is finite, and it’s possible to get aliasing when the size of the shadow texel is larger than a screen pixel, due to the projection of the texture. In order to address this, you can specify alternative projection bases by using or creating subclasses of the ShadowCameraSetup class. The default version is called DefaultShadowCameraSetup and this sets up a simple regular frustum for point and spotlights, and an orthographic frustum for directional lights. There is also a PlaneOptimalShadowCameraSetup class which specialises the projection to a plane, thus giving you much better definition provided your shadow receivers exist mostly in a single plane. Other setup classes (e.g. you might create a perspective or trapezoid shadow mapping version) can be created and plugged in at runtime, either on individual lights or on the SceneManager as a whole.</p>
<p><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing"></a><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing-1"></a></p>
<h2>Shadow texture Depth Buffer sharing</h2>
<p>Shadow textures need a depth buffer like many other RTs (Render Textures). Prior to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8, the depth buffer behavior was left undefined leaving a very small possibility of causing inconsistencies across different window resolutions and render systems. Depending on the render window’s resolutions and/or rendersystem being used, the depth buffer might been shared with the render window or a new one could get created to suite the shadow textures. If the application was depending on the depth buffer contents from the previous scene render (that is, no clear was performed) where a shadow texture render pass was in the middle; then the depth buffer would’ve contained garbage (but not consistent on all machines) causing graphical glitches hard to spot.</p>
<p>From <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8 onwards the depth buffer usage &amp; sharing can be flexible controlled through the use of depth pool IDs. These pool IDs are not specifically part of shadow textures, but rather anything involving RTs. All RTs with the same pool ID share the same depth buffers when possible (following RenderSystem API rules, check RenderSystemCapabilities flags to find out what the behavior will be). The default ID for shadow textures is 1; which is the same default value for render windows, and RTTs; thus maintaining the same behavior with older applications while achieving maximum memory saving and performance efficiency because the number of created depth buffers is as lowest as possible.</p>
<p>However there are some reasons to put shadow textures in a separate pool. This holds specially true if the application depends on the previous contents from the depth buffer before the shadow pass, instead of doing a clear:</p>
<ul>
<li>In Direct3D9, the shadow texture is more likely to share the depth buffer with the render window at high resolutions (when the window is bigger than the shadow texture resolution), but at low resolutions it won’t be shared, thus causing two different behaviors. Also probably the shadow texture will share the depth buffers with most other RTTs (i.e. compositors)</li>
<li>In OpenGL 2.1, the shadow texture can’t be shared with the main render window; and most likely will NOT be shared with many other RTTs (i.e. compositors) since OGL 2.1 has a requirement that texture resolutions should exactly match, while D3D9 specifies depth buffers can be shared as long as the resolutions are equal or less.</li>
</ul>
<p>For example, the DeferredShading sample suffers from this problem. If this is a problem for a particular effect you’re trying to achieve, you can specify a custom pool ID so that shadow textures get their own depth buffer(s), ensuring they aren’t shared with other RTs. You can set the poolId parameter from either SceneManager::setShadowTextureSettings or setShadowTextureConfig</p>
<div class="fragment"><div class="line">mSceneMgr-&gt;setShadowTextureSettings( size, count, format, PoolId );</div><div class="line">mSceneMgr-&gt;setShadowTextureConfig( 0, 512, 512, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39cab63dbcddd53d1f766d742fd557c26be4">PF_FLOAT16_R</a>, 50 );</div></div><!-- fragment --><p>Note a poolId of 0 will make the shadow textures not to use a depth buffer, which isn’t usually a desired behavior.</p>
<p><a class="anchor" id="Integrated-Texture-Shadows"></a><a class="anchor" id="Integrated-Texture-Shadows-1"></a></p>
<h2>Integrated Texture Shadows</h2>
<p>Texture shadows have one major advantage over stencil shadows - the data used to represent them can be referenced in regular shaders. Whilst the default texture shadow modes (SHADOWTYPE_TEXTURE_MODULATIVE and SHADOWTYPE_TEXTURE_ADDITIVE) automatically render shadows for you, their disadvantage is that because they are generalised add-ons to your own materials, they tend to take more passes of the scene to use. In addition, you don’t have a lot of control over the composition of the shadows.</p>
<p>Here is where ’integrated’ texture shadows step in. Both of the texture shadow types above have alternative versions called SHADOWTYPE_TEXTURE_MODULATIVE_INTEGRATED and SHADOWTYPE_TEXTURE_ADDITIVE_INTEGRATED, where instead of rendering the shadows for you, it just creates the texture shadow and then expects you to use that shadow texture as you see fit when rendering receiver objects in the scene. The downside is that you have to take into account shadow receipt in every one of your materials if you use this option - the upside is that you have total control over how the shadow textures are used. The big advantage here is that you can can perform more complex shading, taking into account shadowing, than is possible using the generalised bolt-on approaches, AND you can probably write them in a smaller number of passes, since you know precisely what you need and can combine passes where possible. When you use one of these shadowing approaches, the only difference between additive and modulative is the colour of the casters in the shadow texture (the shadow colour for modulative, black for additive) - the actual calculation of how the texture affects the receivers is of course up to you. No separate modulative pass will be performed, and no splitting of your materials into ambient / per-light / decal etc will occur - absolutely everything is determined by your original material (which may have modulative passes or per-light iteration if you want of course, but it’s not required).</p>
<p>You reference a shadow texture in a material which implements this approach by using the ’<a href="#content_005ftype">content_type</a> shadow’ directive in your <a href="#Texture-Units">texture_unit</a>. It implicitly references a shadow texture based on the number of times you’ve used this directive in the same pass, and the light_start option or light-based pass iteration, which might start the light index higher than 0.</p>
<h1><a class="anchor" id="Modulative-Shadows"></a>
Modulative Shadows</h1>
<p>Modulative shadows work by darkening an already rendered scene with a fixed colour. First, the scene is rendered normally containing all the objects which will be shadowed, then a modulative pass is done per light, which darkens areas in shadow. Finally, objects which do not receive shadows are rendered.</p>
<p>There are 2 modulative shadow techniques; stencil-based (See <a href="#Stencil-Shadows">Stencil Shadows</a> : SHADOWTYPE_STENCIL_MODULATIVE) and texture-based (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a> : SHADOWTYPE_TEXTURE_MODULATIVE). Modulative shadows are an inaccurate lighting model, since they darken the areas of shadow uniformly, irrespective of the amount of light which would have fallen on the shadow area anyway. However, they can give fairly attractive results for a much lower overhead than more ’correct’ methods like <a href="#Additive-Light-Masking">Additive Light Masking</a>, and they also combine well with pre-baked static lighting (such as pre-calculated lightmaps), which additive lighting does not. The main thing to consider is that using multiple light sources can result in overly dark shadows (where shadows overlap, which intuitively looks right in fact, but it’s not physically correct) and artifacts when using stencil shadows (See <a href="#The-Silhouette-Edge">The Silhouette Edge</a>).</p>
<p><a class="anchor" id="Shadow-Colour"></a><a class="anchor" id="Shadow-Colour-1"></a></p>
<h2>Shadow Colour</h2>
<p>The colour which is used to darken the areas in shadow is set by SceneManager::setShadowColour; it defaults to a dark grey (so that the underlying colour still shows through a bit).</p>
<p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to have a separate pass of the scene to render shadows. In this case the ’modulative’ aspect of the shadow technique just affects the colour of the shadow texture.</p>
<h1><a class="anchor" id="Additive-Light-Masking"></a>
Additive Light Masking</h1>
<p>Additive light masking is about rendering the scene many times, each time representing a single light contribution whose influence is masked out in areas of shadow. Each pass is combined with (added to) the previous one such that when all the passes are complete, all the light contribution has correctly accumulated in the scene, and each light has been prevented from affecting areas which it should not be able to because of shadow casters. This is an effective technique which results in very realistic looking lighting, but it comes at a price: more rendering passes.</p>
<p>As many technical papers (and game marketing) will tell you, rendering realistic lighting like this requires multiple passes. Being a friendly sort of engine, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> frees you from most of the hard work though, and will let you use the exact same material definitions whether you use this lighting technique or not (for the most part, see <a href="#Pass-Classification-and-Vertex-Programs">Pass Classification and Vertex Programs</a>). In order to do this technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically categorises the <a href="#Passes">Passes</a> you define in your materials into 3 types:</p>
<ol type="1">
<li>ambient Passes categorised as ’ambient’ include any base pass which is not lit by any particular light, i.e. it occurs even if there is no ambient light in the scene. The ambient pass always happens first, and sets up the initial depth value of the fragments, and the ambient colour if applicable. It also includes any emissive / self illumination contribution. Only textures which affect ambient light (e.g. ambient occlusion maps) should be rendered in this pass.</li>
<li>diffuse/specular Passes categorised as ’diffuse/specular’ (or ’per-light’) are rendered once per light, and each pass contributes the diffuse and specular colour from that single light as reflected by the diffuse / specular terms in the pass. Areas in shadow from that light are masked and are thus not updated. The resulting masked colour is added to the existing colour in the scene. Again, no textures are used in this pass (except for textures used for lighting calculations such as normal maps).</li>
<li>decal Passes categorised as ’decal’ add the final texture colour to the scene, which is modulated by the accumulated light built up from all the ambient and diffuse/specular passes.</li>
</ol>
<p>In practice, <a href="#Passes">Passes</a> rarely fall nicely into just one of these categories. For each Technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> compiles a list of ’Illumination Passes’, which are derived from the user defined passes, but can be split, to ensure that the divisions between illumination pass categories can be maintained. For example, if we take a very simple material definition:</p>
<div class="fragment"><div class="line">material TestIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will split this into 3 illumination passes, which will be the equivalent of this:</p>
<div class="fragment"><div class="line">material TestIlluminationSplitIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 0 0 0</div><div class="line">            specular 0 0 0</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Diffuse / specular pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend add</div><div class="line">            iteration once_per_light</div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend modulate</div><div class="line">            lighting off</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>So as you can see, even a simple material requires a minimum of 3 passes when using this shadow technique, and in fact it requires (num_lights + 2) passes in the general sense. You can use more passes in your original material and <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will cope with that too, but be aware that each pass may turn into multiple ones if it uses more than one type of light contribution (ambient vs diffuse/specular) and / or has texture units. The main nice thing is that you get the full multipass lighting behaviour even if you don’t define your materials in terms of it, meaning that your material definitions can remain the same no matter what lighting approach you decide to use.</p>
<p><a class="anchor" id="Manually-Categorising-Illumination-Passes"></a><a class="anchor" id="Manually-Categorising-Illumination-Passes-1"></a></p>
<h2>Manually Categorising Illumination Passes</h2>
<p>Alternatively, if you want more direct control over the categorisation of your passes, you can use the <a href="#illumination_005fstage">illumination_stage</a> option in your pass to explicitly assign a pass unchanged to an illumination stage. This way you can make sure you know precisely how your material will be rendered under additive lighting conditions.</p>
<p><a class="anchor" id="Pass-Classification-and-Vertex-Programs"></a><a class="anchor" id="Pass-Classification-and-Vertex-Programs-1"></a></p>
<h2>Pass Classification and Vertex Programs</h2>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at classifying and splitting your passes to ensure that the multipass rendering approach required by additive lighting works correctly without you having to change your material definitions. However, there is one exception; when you use vertex programs, the normal lighting attributes ambient, diffuse, specular etc are not used, because all of that is determined by the vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has no way of knowing what you’re doing inside that vertex program, so you have to tell it.</p>
<p>In practice this is very easy. Even though your vertex program could be doing a lot of complex, highly customised processing, it can still be classified into one of the 3 types listed above. All you need to do to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> what you’re doing is to use the pass attributes ambient, diffuse, specular and self_illumination, just as if you were not using a vertex program. Sure, these attributes do nothing (as far as rendering is concerned) when you’re using vertex programs, but it’s the easiest way to indicate to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> which light components you’re using in your vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will then classify and potentially split your programmable pass based on this information - it will leave the vertex program as-is (so that any split passes will respect any vertex modification that is being done).</p>
<p>Note that when classifying a diffuse/specular programmable pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> checks to see whether you have indicated the pass can be run once per light (iteration once_per_light). If so, the pass is left intact, including it’s vertex and fragment programs. However, if this attribute is not included in the pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> tries to split off the per-light part, and in doing so it will disable the fragment program, since in the absence of the ’iteration once_per_light’ attribute it can only assume that the fragment program is performing decal work and hence must not be used per light.</p>
<p>So clearly, when you use additive light masking as a shadow technique, you need to make sure that programmable passes you use are properly set up so that they can be classified correctly. However, also note that the changes you have to make to ensure the classification is correct does not affect the way the material renders when you choose not to use additive lighting, so the principle that you should be able to use the same material definitions for all lighting scenarios still holds. Here is an example of a programmable material which will be classified correctly by the illumination pass classifier:</p>
<div class="fragment"><div class="line"><span class="comment">// Per-pixel normal mapping Any number of lights, diffuse and specular</span></div><div class="line">material Examples/BumpMapping/MultiLightSpecular</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Base ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// ambient only, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 1 1 1</div><div class="line">            diffuse 0 0 0 </div><div class="line">            specular 0 0 0 0</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named_auto ambient ambient_light_colour</div><div class="line">            }</div><div class="line">        }</div><div class="line">        <span class="comment">// Now do the lighting pass</span></div><div class="line">        <span class="comment">// NB we don&#39;t do decal texture here because this is repeated per light</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// set ambient off, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 0 0 0 </div><div class="line">            <span class="comment">// do this for each light</span></div><div class="line">            iteration once_per_light</div><div class="line">            scene_blend add</div><div class="line"></div><div class="line">            <span class="comment">// Vertex program reference</span></div><div class="line">            vertex_program_ref Examples/BumpMapVPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightPosition light_position_object_space 0</div><div class="line">                param_named_auto eyePosition camera_position_object_space</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Fragment program</span></div><div class="line">            fragment_program_ref Examples/BumpMapFPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Base bump map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture NMBumpsOut.png</div><div class="line">                colour_op replace</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map #2</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            lighting off</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named ambient float4 1 1 1 1</div><div class="line">            }</div><div class="line">            scene_blend dest_colour zero</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture RustedMetal.jpg </div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to use this explicit sequence - allowing you to compress the number of passes into a much smaller number at the expense of defining an upper number of shadow casting lights. In this case the ’additive’ aspect of the shadow technique just affects the colour of the shadow texture and it’s up to you to combine the shadow textures in your receivers however you like.</p>
<p><a class="anchor" id="Static-Lighting"></a></p>
<h2>Static Lighting</h2>
<p>Despite their power, additive lighting techniques have an additional limitation; they do not combine well with pre-calculated static lighting in the scene. This is because they are based on the principle that shadow is an absence of light, but since static lighting in the scene already includes areas of light and shadow, additive lighting cannot remove light to create new shadows. Therefore, if you use the additive lighting technique you must either use it exclusively as your lighting solution (and you can combine it with per-pixel lighting to create a very impressive dynamic lighting solution), or you must use <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> to combine the static lighting according to your chosen approach. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Shadows</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_shadows.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Shadows </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Enabling-shadows">Enabling shadows</a></li>
<li class="level1"><a href="#Opting-out-of-shadows">Opting out of shadows</a></li>
<li class="level1"><a href="#Stencil-Shadows">Stencil Shadows</a></li>
<li class="level1"><a href="#Texture_002dbased-Shadows">Texture-based Shadows</a></li>
<li class="level1"><a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a></li>
<li class="level1"><a href="#Modulative-Shadows">Modulative Shadows</a></li>
<li class="level1"><a href="#Additive-Light-Masking">Additive Light Masking</a></li>
</ul>
</div>
<div class="textblock"><p>Shadows are clearly an important part of rendering a believable scene - they provide a more tangible feel to the objects in the scene, and aid the viewer in understanding the spatial relationship between objects. Unfortunately, shadows are also one of the most challenging aspects of 3D rendering, and they are still very much an active area of research. Whilst there are many techniques to render shadows, none is perfect and they all come with advantages and disadvantages. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides multiple shadow implementations, with plenty of configuration settings, so you can choose which technique is most appropriate for your scene.</p>
<p>Shadow implementations fall into basically 2 broad categories: <a href="#Stencil-Shadows">Stencil Shadows</a> and <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>. This describes the method by which the shape of the shadow is generated. In addition, there is more than one way to render the shadow into the scene: <a href="#Modulative-Shadows">Modulative Shadows</a>, which darkens the scene in areas of shadow, and <a href="#Additive-Light-Masking">Additive Light Masking</a> which by contrast builds up light contribution in areas which are not in shadow. You also have the option of <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> which gives you complete control over texture shadow application, allowing for complex single-pass shadowing shaders. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports all these combinations.</p>
<h1><a class="anchor" id="Enabling-shadows"></a>
Enabling shadows</h1>
<p>Shadows are disabled by default, here’s how you turn them on and configure them in the general sense:</p>
<ol type="1">
<li><p class="startli">Enable a shadow technique on the SceneManager as the <b>first</b> thing you doing your scene setup. It is important that this is done first because the shadow technique can alter the way meshes are loaded. Here’s an example:</p>
<p class="startli"><code>mSceneMgr-&gt;setShadowTechnique(SHADOWTYPE_STENCIL_ADDITIVE);</code></p>
</li>
<li>Create one or more lights. Note that not all light types are necessarily supported by all shadow techniques, you should check the sections about each technique to check. Note that if certain lights should not cast shadows, you can turn that off by calling setCastShadows(false) on the light, the default is true.</li>
<li>Disable shadow casting on objects which should not cast shadows. Call setCastShadows(false) on objects you don’t want to cast shadows, the default for all objects is to cast shadows.</li>
<li>Configure shadow far distance. You can limit the distance at which shadows are considered for performance reasons, by calling SceneManager::setShadowFarDistance.</li>
<li>Turn off the receipt of shadows on materials that should not receive them. You can turn off the receipt of shadows (note, not the casting of shadows - that is done per-object) by calling Material::setReceiveShadows or using the receive_shadows material attribute. This is useful for materials which should be considered self-illuminated for example. Note that transparent materials are typically excluded from receiving and casting shadows, although see the <a href="#transparency_005fcasts_005fshadows">transparency_casts_shadows</a> option for exceptions.</li>
</ol>
<h1><a class="anchor" id="Opting-out-of-shadows"></a>
Opting out of shadows</h1>
<p>By default <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> treats all non-transparent objects as shadow casters and receivers (depending on the shadow technique they may not be able to be both at once, check the docs for your chosen technique first). You can disable shadows in various ways:</p>
<dl compact="compact">
<dt>Turning off shadow casting on the light </dt>
<dd><p class="startdd"></p>
<p>Calling Light::setCastsShadows(false) will mean this light casts no shadows at all.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow receipt on a material </dt>
<dd><p class="startdd"></p>
<p>Calling Material::setReceiveShadows(false) will prevent any objects using this material from receiving shadows.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow casting on individual objects </dt>
<dd><p class="startdd"></p>
<p>Calling MovableObject::setCastsShadows(false) will disable shadow casting for this object.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadows on an entire rendering queue group </dt>
<dd><p class="startdd"></p>
<p>Calling RenderQueueGroup::setShadowsEnabled(false) will turn off both shadow casting and receiving on an entire rendering queue group. This is useful because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has to do light setup tasks per group in order to preserve the inter-group ordering. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically disables shadows on a number of groups automatically, such as RENDER_QUEUE_BACKGROUND, RENDER_QUEUE_OVERLAY, RENDER_QUEUE_SKIES_EARLY and RENDER_QUEUE_SKIES_LATE. If you choose to use more rendering queues (and by default, you won’t be using any more than this plus the ’standard’ queue, so ignore this if you don’t know what it means!), be aware that each one can incur a light setup cost, and you should disable shadows on the additional ones you use if you can.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Stencil-Shadows"></a>
Stencil Shadows</h1>
<p>Stencil shadows are a method by which a ’mask’ is created for the screen using a feature called the stencil buffer. This mask can be used to exclude areas of the screen from subsequent renders, and thus it can be used to either include or exclude areas in shadow. They are enabled by calling SceneManager::setShadowTechnique with a parameter of either <code>SHADOWTYPE_STENCIL_ADDITIVE</code> or <code>SHADOWTYPE_STENCIL_MODULATIVE</code>. Because the stencil can only mask areas to be either ’enabled’ or ’disabled’, stencil shadows have ’hard’ edges, that is to say clear dividing lines between light and shadow - it is not possible to soften these edges.</p>
<p>In order to generate the stencil, ’shadow volumes’ are rendered by extruding the silhouette of the shadow caster away from the light. Where these shadow volumes intersect other objects (or the caster, since self-shadowing is supported using this technique), the stencil is updated, allowing subsequent operations to differentiate between light and shadow. How exactly this is used to render the shadows depends on whether <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> is being used. Objects can both cast and receive stencil shadows, so self-shadowing is inbuilt.</p>
<p>The advantage of stencil shadows is that they can do self-shadowing simply on low-end hardware, provided you keep your poly count under control. In contrast doing self-shadowing with texture shadows requires a fairly modern machine (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>). For this reason, you’re likely to pick stencil shadows if you need an accurate shadowing solution for an application aimed at older or lower-spec machines.</p>
<p>The disadvantages of stencil shadows are numerous though, especially on more modern hardware. Because stencil shadows are a geometric technique, they are inherently more costly the higher the number of polygons you use, meaning you are penalized the more detailed you make your meshes. The fillrate cost, which comes from having to render shadow volumes, also escalates the same way. Since more modern applications are likely to use higher polygon counts, stencil shadows can start to become a bottleneck. In addition, the visual aspects of stencil shadows are pretty primitive - your shadows will always be hard-edged, and you have no possibility of doing clever things with shaders since the stencil is not available for manipulation there. Therefore, if your application is aimed at higher-end machines you should definitely consider switching to texture shadows (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>).</p>
<p>There are a number of issues to consider which are specific to stencil shadows:</p>
<ul>
<li><a href="#CPU-Overhead">CPU Overhead</a></li>
<li><a href="#Extrusion-distance">Extrusion distance</a></li>
<li><a href="#Camera-far-plane-positioning">Camera far plane positioning</a></li>
<li><a href="#Mesh-edge-lists">Mesh edge lists</a></li>
<li><a href="#The-Silhouette-Edge">The Silhouette Edge</a></li>
<li><a href="#Be-realistic">Be realistic</a></li>
<li><a href="#Stencil-Optimisations-Performed-By-Ogre">Stencil Optimisations Performed By Ogre</a></li>
</ul>
<p><a class="anchor" id="CPU-Overhead"></a><a class="anchor" id="CPU-Overhead-1"></a></p>
<h2>CPU Overhead</h2>
<p>Calculating the shadow volume for a mesh can be expensive, and it has to be done on the CPU, it is not a hardware accelerated feature. Therefore, you can find that if you overuse this feature, you can create a CPU bottleneck for your application. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> quite aggressively eliminates objects which cannot be casting shadows on the frustum, but there are limits to how much it can do, and large, elongated shadows (e.g. representing a very low sun position) are very difficult to cull efficiently. Try to avoid having too many shadow casters around at once, and avoid long shadows if you can. Also, make use of the ’shadow far distance’ parameter on the SceneManager, this can eliminate distant shadow casters from the shadow volume construction and save you some time, at the expense of only having shadows for closer objects. Lastly, make use of Ogre’s Level-Of-Detail (LOD) features; you can generate automatically calculated LODs for your meshes in code (see the Mesh API docs) or when using the mesh tools such as OgreXMLConverter and OgreMeshUpgrader. Alternatively, you can assign your own manual LODs by providing alternative mesh files at lower detail levels. Both methods will cause the shadow volume complexity to decrease as the object gets further away, which saves you valuable volume calculation time.</p>
<p><a class="anchor" id="Extrusion-distance"></a><a class="anchor" id="Extrusion-distance-1"></a></p>
<h2>Extrusion distance</h2>
<p>When vertex programs are not available, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only extrude shadow volumes a finite distance from the object. If an object gets too close to a light, any finite extrusion distance will be inadequate to guarantee all objects will be shadowed properly by this object. Therefore, you are advised not to let shadow casters pass too close to light sources if you can avoid it, unless you can guarantee that your target audience will have vertex program capable hardware (in this case, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> extrudes the volume to infinity using a vertex program so the problem does not occur). When infinite extrusion is not possible, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses finite extrusion, either derived from the attenuation range of a light (in the case of a point light or spotlight), or a fixed extrusion distance set in the application in the case of directional lights. To change the directional light extrusion distance, use SceneManager::setShadowDirectionalLightExtrusionDistance.</p>
<p><a class="anchor" id="Camera-far-plane-positioning"></a><a class="anchor" id="Camera-far-plane-positioning-1"></a></p>
<h2>Camera far plane positioning</h2>
<p>Stencil shadow volumes rely very much on not being clipped by the far plane. When you enable stencil shadows, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> internally changes the far plane settings of your cameras such that there is no far plane - i.e. it is placed at infinity (Camera::setFarClipDistance(0)). This avoids artifacts caused by clipping the dark caps on shadow volumes, at the expense of a (very) small amount of depth precision.</p>
<p><a class="anchor" id="Mesh-edge-lists"></a><a class="anchor" id="Mesh-edge-lists-1"></a></p>
<h2>Mesh edge lists</h2>
<p>Stencil shadows can only be calculated when an ’edge list’ has been built for all the geometry in a mesh. The official exporters and tools automatically build this for you (or have an option to do so), but if you create your own meshes, you must remember to build edge lists for them before using them with stencil shadows - you can do that by using OgreMeshUpgrade or OgreXmlConverter, or by calling Mesh::buildEdgeList before you export or use the mesh. If a mesh doesn’t have edge lists, OGRE assumes that it is not supposed to cast stencil shadows.</p>
<p><a class="anchor" id="The-Silhouette-Edge"></a><a class="anchor" id="The-Silhouette-Edge-1"></a></p>
<h2>The Silhouette Edge</h2>
<p>Stencil shadowing is about finding a silhouette of the mesh, and projecting it away to form a volume. What this means is that there is a definite boundary on the shadow caster between light and shadow; a set of edges where where the triangle on one side is facing toward the light, and one is facing away. This produces a sharp edge around the mesh as the transition occurs. Provided there is little or no other light in the scene, and the mesh has smooth normals to produce a gradual light change in its underlying shading, the silhouette edge can be hidden - this works better the higher the tessellation of the mesh. However, if the scene includes ambient light, then the difference is far more marked. This is especially true when using <a href="#Modulative-Shadows">Modulative Shadows</a>, because the light contribution of each shadowed area is not taken into account by this simplified approach, and so using 2 or more lights in a scene using modulative stencil shadows is not advisable; the silhouette edges will be very marked. Additive lights do not suffer from this as badly because each light is masked individually, meaning that it is only ambient light which can show up the silhouette edges.</p>
<p><a class="anchor" id="Be-realistic"></a><a class="anchor" id="Be-realistic-1"></a></p>
<h2>Be realistic</h2>
<p>Don’t expect to be able to throw any scene using any hardware at the stencil shadow algorithm and expect to get perfect, optimum speed results. Shadows are a complex and expensive technique, so you should impose some reasonable limitations on your placing of lights and objects; they’re not really that restricting, but you should be aware that this is not a complete free-for-all.</p>
<ul>
<li>Try to avoid letting objects pass very close (or even through) lights - it might look nice but it’s one of the cases where artifacts can occur on machines not capable of running vertex programs.</li>
<li>Be aware that shadow volumes do not respect the ’solidity’ of the objects they pass through, and if those objects do not themselves cast shadows (which would hide the effect) then the result will be that you can see shadows on the other side of what should be an occluding object.</li>
<li>Make use of SceneManager::setShadowFarDistance to limit the number of shadow volumes constructed</li>
<li>Make use of LOD to reduce shadow volume complexity at distance</li>
<li><p class="startli">Avoid very long (dusk and dawn) shadows - they exacerbate other issues such as volume clipping, fillrate, and cause many more objects at a greater distance to require volume construction.</p>
<p class="startli"><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre"></a><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre-1"></a></p>
</li>
</ul>
<h2>Stencil Optimisations Performed By <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a></h2>
<p>Despite all that, stencil shadows can look very nice (especially with <a href="#Additive-Light-Masking">Additive Light Masking</a>) and can be fast if you respect the rules above. In addition, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> comes pre-packed with a lot of optimisations which help to make this as quick as possible. This section is more for developers or people interested in knowing something about the ’under the hood’ behaviour of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl compact="compact">
<dt>Vertex program extrusion </dt>
<dd><p class="startdd"></p>
<p>As previously mentioned, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> performs the extrusion of shadow volumes in hardware on vertex program-capable hardware (e.g. GeForce3, Radeon 8500 or better). This has 2 major benefits; the obvious one being speed, but secondly that vertex programs can extrude points to infinity, which the fixed-function pipeline cannot, at least not without performing all calculations in software. This leads to more robust volumes, and also eliminates more than half the volume triangles on directional lights since all points are projected to a single point at infinity.</p>
<p class="enddd"></p>
</dd>
<dt>Scissor test optimisation </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a scissor rectangle to limit the effect of point / spot lights when their range does not cover the entire viewport; that means we save fillrate when rendering stencil volumes, especially with distant lights</p>
<p class="enddd"></p>
</dd>
<dt>Z-Pass and Z-Fail algorithms </dt>
<dd><p class="startdd"></p>
<p>The Z-Fail algorithm, often attributed to John Carmack, is used in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to make sure shadows are robust when the camera passes through the shadow volume. However, the Z-Fail algorithm is more expensive than the traditional Z-Pass; so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> detects when Z-Fail is required and only uses it then, Z-Pass is used at all other times.</p>
<p class="enddd"></p>
</dd>
<dt>2-Sided stenciling and stencil wrapping </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports the 2-Sided stenciling / stencil wrapping extensions, which when supported allow volumes to be rendered in a single pass instead of having to do one pass for back facing tris and another for front-facing tris. This doesn’t save fillrate, since the same number of stencil updates are done, but it does save primitive setup and the overhead incurred in the driver every time a render call is made.</p>
<p class="enddd"></p>
</dd>
<dt>Aggressive shadow volume culling </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at detecting which lights could be affecting the frustum, and from that, which objects could be casting a shadow on the frustum. This means we don’t waste time constructing shadow geometry we don’t need. Setting the shadow far distance is another important way you can reduce stencil shadow overhead since it culls far away shadow volumes even if they are visible, which is beneficial in practice since you’re most interested in shadows for close-up objects.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Texture_002dbased-Shadows"></a>
Texture-based Shadows</h1>
<p>Texture shadows involve rendering shadow casters from the point of view of the light into a texture, which is then projected onto shadow receivers. The main advantage of texture shadows as opposed to <a href="#Stencil-Shadows">Stencil Shadows</a> is that the overhead of increasing the geometric detail is far lower, since there is no need to perform per-triangle calculations. Most of the work in rendering texture shadows is done by the graphics card, meaning the technique scales well when taking advantage of the latest cards, which are at present outpacing CPUs in terms of their speed of development. In addition, texture shadows are <b>much</b> more customisable - you can pull them into shaders to apply as you like (particularly with <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a>, you can perform filtering to create softer shadows or perform other special effects on them. Basically, most modern engines use texture shadows as their primary shadow technique simply because they are more powerful, and the increasing speed of GPUs is rapidly amortizing the fillrate / texture access costs of using them.</p>
<p>The main disadvantage to texture shadows is that, because they are simply a texture, they have a fixed resolution which means if stretched, the pixellation of the texture can become obvious. There are ways to combat this though:</p>
<dl compact="compact">
<dt>Choosing a projection basis </dt>
<dd><p class="startdd"></p>
<p>The simplest projection is just to render the shadow casters from the lights perspective using a regular camera setup. This can look bad though, so there are many other projections which can help to improve the quality from the main camera’s perspective. OGRE supports pluggable projection bases via it’s ShadowCameraSetup class, and comes with several existing options - <b>Uniform</b> (which is the simplest), <b>Uniform Focussed</b> (which is still a normal camera projection, except that the camera is focussed into the area that the main viewing camera is looking at), LiSPSM (Light Space Perspective Shadow Mapping - which both focusses and distorts the shadow frustum based on the main view camera) and Plan Optimal (which seeks to optimise the shadow fidelity for a single receiver plane).</p>
<p class="enddd"></p>
</dd>
<dt>Filtering </dt>
<dd><p class="startdd"></p>
<p>You can also sample the shadow texture multiple times rather than once to soften the shadow edges and improve the appearance. Percentage Closest Filtering (PCF) is the most popular approach, although there are multiple variants depending on the number and pattern of the samples you take. Our shadows demo includes a 5-tap PCF example combined with depth shadow mapping.</p>
<p class="enddd"></p>
</dd>
<dt>Using a larger texture </dt>
<dd><p class="startdd"></p>
<p>Again as GPUs get faster and gain more memory, you can scale up to take advantage of this.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If you combine all 3 of these techniques you can get a very high quality shadow solution.</p>
<p>The other issue is with point lights. Because texture shadows require a render to texture in the direction of the light, omnidirectional lights (point lights) would require 6 renders to totally cover all the directions shadows might be cast. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> primarily supports directional lights and spotlights for generating texture shadows; you can use point lights but they will only work if off-camera since they are essentially turned into a spotlight shining into your camera frustum for the purposes of texture shadows.</p>
<p><a class="anchor" id="Directional-Lights"></a></p>
<h2>Directional Lights</h2>
<p>Directional lights in theory shadow the entire scene from an infinitely distant light. Now, since we only have a finite texture which will look very poor quality if stretched over the entire scene, clearly a simplification is required. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> places a shadow texture over the area immediately in front of the camera, and moves it as the camera moves (although it rounds this movement to multiples of texels so that the slight ’swimming shadow’ effect caused by moving the texture is minimised). The range to which this shadow extends, and the offset used to move it in front of the camera, are configurable (See <a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a>). At the far edge of the shadow, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> fades out the shadow based on other configurable parameters so that the termination of the shadow is softened.</p>
<p><a class="anchor" id="Spotlights"></a></p>
<h2>Spotlights</h2>
<p>Spotlights are much easier to represent as renderable shadow textures than directional lights, since they are naturally a frustum. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> represents spotlight directly by rendering the shadow from the light position, in the direction of the light cone; the field-of-view of the texture camera is adjusted based on the spotlight falloff angles. In addition, to hide the fact that the shadow texture is square and has definite edges which could show up outside the spotlight, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a second texture unit when projecting the shadow onto the scene which fades out the shadow gradually in a projected circle around the spotlight.</p>
<p><a class="anchor" id="Point-Lights"></a></p>
<h2>Point Lights</h2>
<p>As mentioned above, to support point lights properly would require multiple renders (either 6 for a cubic render or perhaps 2 for a less precise parabolic mapping), so rather than do that we approximate point lights as spotlights, where the configuration is changed on the fly to make the light shine from its position over the whole of the viewing frustum. This is not an ideal setup since it means it can only really work if the point light’s position is out of view, and in addition the changing parameterisation can cause some ’swimming’ of the texture. Generally we recommend avoiding making point lights cast texture shadows.</p>
<p><a class="anchor" id="Shadow-Casters-and-Shadow-Receivers"></a></p>
<h2>Shadow Casters and Shadow Receivers</h2>
<p>To enable texture shadows, use the shadow technique SHADOWTYPE_TEXTURE_MODULATIVE or SHADOWTYPE_TEXTURE_ADDITIVE; as the name suggests this produces <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> respectively. The cheapest and simplest texture shadow techniques do not use depth information, they merely render casters to a texture and render this onto receivers as plain colour - this means self-shadowing is not possible using these methods. This is the default behaviour if you use the automatic, fixed-function compatible (and thus usable on lower end hardware) texture shadow techniques. You can however use shaders-based techniques through custom shadow materials for casters and receivers to perform more complex shadow algorithms, such as depth shadow mapping which does allow self-shadowing. OGRE comes with an example of this in its shadows demo, although it’s only usable on Shader Model 2 cards or better. Whilst fixed-function depth shadow mapping is available in OpenGL, it was never standardised in Direct3D so using shaders in custom caster &amp; receiver materials is the only portable way to do it. If you use this approach, call SceneManager::setShadowTextureSelfShadow with a parameter of ’true’ to allow texture shadow casters to also be receivers. If you’re not using depth shadow mapping, OGRE divides shadow casters and receivers into 2 disjoint groups. Simply by turning off shadow casting on an object, you automatically make it a shadow receiver (although this can be disabled by setting the ’receive_shadows’ option to ’false’ in a material script. Similarly, if an object is set as a shadow caster, it cannot receive shadows.</p>
<h1><a class="anchor" id="Configuring-Texture-Shadows"></a>
Configuring Texture Shadows</h1>
<p>There are a number of settings which will help you configure your texture-based shadows so that they match your requirements.</p>
<ul>
<li><a href="#Maximum-number-of-shadow-textures">Maximum number of shadow textures</a></li>
<li><a href="#Shadow-texture-size">Shadow texture size</a></li>
<li><a href="#Shadow-far-distance">Shadow far distance</a></li>
<li><a href="#Shadow-texture-offset-Directional-Lights_0029">Shadow texture offset (Directional Lights)</a></li>
<li><a href="#Shadow-fade-settings">Shadow fade settings</a></li>
<li><a href="#Custom-shadow-camera-setups">Custom shadow camera setups</a></li>
<li><a href="#Shadow-texture-Depth-Buffer-sharing">Shadow texture Depth Buffer sharing</a></li>
<li><a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a></li>
</ul>
<p><a class="anchor" id="Maximum-number-of-shadow-textures"></a><a class="anchor" id="Maximum-number-of-shadow-textures-1"></a></p>
<h2>Maximum number of shadow textures</h2>
<p>Shadow textures take up texture memory, and to avoid stalling the rendering pipeline <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> does not reuse the same shadow texture for multiple lights within the same frame. This means that each light which is to cast shadows must have its own shadow texture. In practice, if you have a lot of lights in your scene you would not wish to incur that sort of texture overhead. You can adjust this manually by simply turning off shadow casting for lights you do not wish to cast shadows. In addition, you can set a maximum limit on the number of shadow textures <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is allowed to use by calling SceneManager::setShadowTextureCount. Each frame, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> determines the lights which could be affecting the frustum, and then allocates the number of shadow textures it is allowed to use to the lights on a first-come-first-served basis. Any additional lights will not cast shadows that frame. Note that you can set the number of shadow textures and their size at the same time by using the SceneManager::setShadowTextureSettings method; this is useful because both the individual calls require the potential creation / destruction of texture resources.</p>
<p><a class="anchor" id="Shadow-texture-size"></a><a class="anchor" id="Shadow-texture-size-1"></a></p>
<h2>Shadow texture size</h2>
<p>The size of the textures used for rendering the shadow casters into can be altered; clearly using larger textures will give you better quality shadows, but at the expense of greater memory usage. Changing the texture size is done by calling SceneManager::setShadowTextureSize - textures are assumed to be square and you must specify a texture size that is a power of 2. Be aware that each modulative shadow texture will take size*size*3 bytes of texture memory. <b>Important</b>: if you use the GL render system your shadow texture size can only be larger (in either dimension) than the size of your primary window surface if the hardware supports the Frame Buffer Object (FBO) or Pixel Buffer Object (PBO) extensions. Most modern cards support this now, but be careful of older cards - you can check the ability of the hardware to manage this through ogreRoot-&gt;getRenderSystem()-&gt;getCapabilities()-&gt;hasCapability(RSC_HWRENDER_TO_TEXTURE). If this returns false, if you create a shadow texture larger in any dimension than the primary surface, the rest of the shadow texture will be blank.</p>
<p><a class="anchor" id="Shadow-far-distance"></a><a class="anchor" id="Shadow-far-distance-1"></a></p>
<h2>Shadow far distance</h2>
<p>This determines the distance at which shadows are terminated; it also determines how far into the distance the texture shadows for directional lights are stretched - by reducing this value, or increasing the texture size, you can improve the quality of shadows from directional lights at the expense of closer shadow termination or increased memory usage, respectively.</p>
<p><a class="anchor" id="Shadow-texture-offset-Directional-Lights_0029"></a></p>
<h2>Shadow texture offset (Directional Lights)</h2>
<p>As mentioned above in the directional lights section, the rendering of shadows for directional lights is an approximation that allows us to use a single render to cover a largish area with shadows. This offset parameter affects how far from the camera position the center of the shadow texture is offset, as a proportion of the shadow far distance. The greater this value, the more of the shadow texture is ’useful’ to you since it’s ahead of the camera, but also the further you offset it, the more chance there is of accidentally seeing the edge of the shadow texture at more extreme angles. You change this value by calling SceneManager::setShadowDirLightTextureOffset, the default is 0.6.</p>
<p><a class="anchor" id="Shadow-fade-settings"></a><a class="anchor" id="Shadow-fade-settings-1"></a></p>
<h2>Shadow fade settings</h2>
<p>Shadows fade out before the shadow far distance so that the termination of shadow is not abrupt. You can configure the start and end points of this fade by calling the SceneManager::setShadowTextureFadeStart and SceneManager::setShadowTextureFadeEnd methods, both take distances as a proportion of the shadow far distance. Because of the inaccuracies caused by using a square texture and a radial fade distance, you cannot use 1.0 as the fade end, if you do you’ll see artifacts at the extreme edges. The default values are 0.7 and 0.9, which serve most purposes but you can change them if you like.</p>
<h1>Texture shadows and vertex / fragment programs</h1>
<p>When rendering shadow casters into a modulative shadow texture, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> turns off all textures, and all lighting contributions except for ambient light, which it sets to the colour of the shadow (<a href="#Shadow-Colour">Shadow Colour</a>). For additive shadows, it render the casters into a black &amp; white texture instead. This is enough to render shadow casters for fixed-function material techniques, however where a vertex program is used <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> doesn’t have so much control. If you use a vertex program in the <b>first pass</b> of your technique, then you must also tell ogre which vertex program you want it to use when rendering the shadow caster; see <a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a> for full details.</p>
<p><a class="anchor" id="Custom-shadow-camera-setups"></a><a class="anchor" id="Custom-shadow-camera-setups-1"></a></p>
<h2>Custom shadow camera setups</h2>
<p>As previously mentioned, one of the downsides of texture shadows is that the texture resolution is finite, and it’s possible to get aliasing when the size of the shadow texel is larger than a screen pixel, due to the projection of the texture. In order to address this, you can specify alternative projection bases by using or creating subclasses of the ShadowCameraSetup class. The default version is called DefaultShadowCameraSetup and this sets up a simple regular frustum for point and spotlights, and an orthographic frustum for directional lights. There is also a PlaneOptimalShadowCameraSetup class which specialises the projection to a plane, thus giving you much better definition provided your shadow receivers exist mostly in a single plane. Other setup classes (e.g. you might create a perspective or trapezoid shadow mapping version) can be created and plugged in at runtime, either on individual lights or on the SceneManager as a whole.</p>
<p><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing"></a><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing-1"></a></p>
<h2>Shadow texture Depth Buffer sharing</h2>
<p>Shadow textures need a depth buffer like many other RTs (Render Textures). Prior to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8, the depth buffer behavior was left undefined leaving a very small possibility of causing inconsistencies across different window resolutions and render systems. Depending on the render window’s resolutions and/or rendersystem being used, the depth buffer might been shared with the render window or a new one could get created to suite the shadow textures. If the application was depending on the depth buffer contents from the previous scene render (that is, no clear was performed) where a shadow texture render pass was in the middle; then the depth buffer would’ve contained garbage (but not consistent on all machines) causing graphical glitches hard to spot.</p>
<p>From <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8 onwards the depth buffer usage &amp; sharing can be flexible controlled through the use of depth pool IDs. These pool IDs are not specifically part of shadow textures, but rather anything involving RTs. All RTs with the same pool ID share the same depth buffers when possible (following RenderSystem API rules, check RenderSystemCapabilities flags to find out what the behavior will be). The default ID for shadow textures is 1; which is the same default value for render windows, and RTTs; thus maintaining the same behavior with older applications while achieving maximum memory saving and performance efficiency because the number of created depth buffers is as lowest as possible.</p>
<p>However there are some reasons to put shadow textures in a separate pool. This holds specially true if the application depends on the previous contents from the depth buffer before the shadow pass, instead of doing a clear:</p>
<ul>
<li>In Direct3D9, the shadow texture is more likely to share the depth buffer with the render window at high resolutions (when the window is bigger than the shadow texture resolution), but at low resolutions it won’t be shared, thus causing two different behaviors. Also probably the shadow texture will share the depth buffers with most other RTTs (i.e. compositors)</li>
<li>In OpenGL 2.1, the shadow texture can’t be shared with the main render window; and most likely will NOT be shared with many other RTTs (i.e. compositors) since OGL 2.1 has a requirement that texture resolutions should exactly match, while D3D9 specifies depth buffers can be shared as long as the resolutions are equal or less.</li>
</ul>
<p>For example, the DeferredShading sample suffers from this problem. If this is a problem for a particular effect you’re trying to achieve, you can specify a custom pool ID so that shadow textures get their own depth buffer(s), ensuring they aren’t shared with other RTs. You can set the poolId parameter from either SceneManager::setShadowTextureSettings or setShadowTextureConfig</p>
<div class="fragment"><div class="line">mSceneMgr-&gt;setShadowTextureSettings( size, count, format, PoolId );</div><div class="line">mSceneMgr-&gt;setShadowTextureConfig( 0, 512, 512, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39cab63dbcddd53d1f766d742fd557c26be4">PF_FLOAT16_R</a>, 50 );</div></div><!-- fragment --><p>Note a poolId of 0 will make the shadow textures not to use a depth buffer, which isn’t usually a desired behavior.</p>
<p><a class="anchor" id="Integrated-Texture-Shadows"></a><a class="anchor" id="Integrated-Texture-Shadows-1"></a></p>
<h2>Integrated Texture Shadows</h2>
<p>Texture shadows have one major advantage over stencil shadows - the data used to represent them can be referenced in regular shaders. Whilst the default texture shadow modes (SHADOWTYPE_TEXTURE_MODULATIVE and SHADOWTYPE_TEXTURE_ADDITIVE) automatically render shadows for you, their disadvantage is that because they are generalised add-ons to your own materials, they tend to take more passes of the scene to use. In addition, you don’t have a lot of control over the composition of the shadows.</p>
<p>Here is where ’integrated’ texture shadows step in. Both of the texture shadow types above have alternative versions called SHADOWTYPE_TEXTURE_MODULATIVE_INTEGRATED and SHADOWTYPE_TEXTURE_ADDITIVE_INTEGRATED, where instead of rendering the shadows for you, it just creates the texture shadow and then expects you to use that shadow texture as you see fit when rendering receiver objects in the scene. The downside is that you have to take into account shadow receipt in every one of your materials if you use this option - the upside is that you have total control over how the shadow textures are used. The big advantage here is that you can can perform more complex shading, taking into account shadowing, than is possible using the generalised bolt-on approaches, AND you can probably write them in a smaller number of passes, since you know precisely what you need and can combine passes where possible. When you use one of these shadowing approaches, the only difference between additive and modulative is the colour of the casters in the shadow texture (the shadow colour for modulative, black for additive) - the actual calculation of how the texture affects the receivers is of course up to you. No separate modulative pass will be performed, and no splitting of your materials into ambient / per-light / decal etc will occur - absolutely everything is determined by your original material (which may have modulative passes or per-light iteration if you want of course, but it’s not required).</p>
<p>You reference a shadow texture in a material which implements this approach by using the ’<a href="#content_005ftype">content_type</a> shadow’ directive in your <a href="#Texture-Units">texture_unit</a>. It implicitly references a shadow texture based on the number of times you’ve used this directive in the same pass, and the light_start option or light-based pass iteration, which might start the light index higher than 0.</p>
<h1><a class="anchor" id="Modulative-Shadows"></a>
Modulative Shadows</h1>
<p>Modulative shadows work by darkening an already rendered scene with a fixed colour. First, the scene is rendered normally containing all the objects which will be shadowed, then a modulative pass is done per light, which darkens areas in shadow. Finally, objects which do not receive shadows are rendered.</p>
<p>There are 2 modulative shadow techniques; stencil-based (See <a href="#Stencil-Shadows">Stencil Shadows</a> : SHADOWTYPE_STENCIL_MODULATIVE) and texture-based (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a> : SHADOWTYPE_TEXTURE_MODULATIVE). Modulative shadows are an inaccurate lighting model, since they darken the areas of shadow uniformly, irrespective of the amount of light which would have fallen on the shadow area anyway. However, they can give fairly attractive results for a much lower overhead than more ’correct’ methods like <a href="#Additive-Light-Masking">Additive Light Masking</a>, and they also combine well with pre-baked static lighting (such as pre-calculated lightmaps), which additive lighting does not. The main thing to consider is that using multiple light sources can result in overly dark shadows (where shadows overlap, which intuitively looks right in fact, but it’s not physically correct) and artifacts when using stencil shadows (See <a href="#The-Silhouette-Edge">The Silhouette Edge</a>).</p>
<p><a class="anchor" id="Shadow-Colour"></a><a class="anchor" id="Shadow-Colour-1"></a></p>
<h2>Shadow Colour</h2>
<p>The colour which is used to darken the areas in shadow is set by SceneManager::setShadowColour; it defaults to a dark grey (so that the underlying colour still shows through a bit).</p>
<p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to have a separate pass of the scene to render shadows. In this case the ’modulative’ aspect of the shadow technique just affects the colour of the shadow texture.</p>
<h1><a class="anchor" id="Additive-Light-Masking"></a>
Additive Light Masking</h1>
<p>Additive light masking is about rendering the scene many times, each time representing a single light contribution whose influence is masked out in areas of shadow. Each pass is combined with (added to) the previous one such that when all the passes are complete, all the light contribution has correctly accumulated in the scene, and each light has been prevented from affecting areas which it should not be able to because of shadow casters. This is an effective technique which results in very realistic looking lighting, but it comes at a price: more rendering passes.</p>
<p>As many technical papers (and game marketing) will tell you, rendering realistic lighting like this requires multiple passes. Being a friendly sort of engine, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> frees you from most of the hard work though, and will let you use the exact same material definitions whether you use this lighting technique or not (for the most part, see <a href="#Pass-Classification-and-Vertex-Programs">Pass Classification and Vertex Programs</a>). In order to do this technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically categorises the <a href="#Passes">Passes</a> you define in your materials into 3 types:</p>
<ol type="1">
<li>ambient Passes categorised as ’ambient’ include any base pass which is not lit by any particular light, i.e. it occurs even if there is no ambient light in the scene. The ambient pass always happens first, and sets up the initial depth value of the fragments, and the ambient colour if applicable. It also includes any emissive / self illumination contribution. Only textures which affect ambient light (e.g. ambient occlusion maps) should be rendered in this pass.</li>
<li>diffuse/specular Passes categorised as ’diffuse/specular’ (or ’per-light’) are rendered once per light, and each pass contributes the diffuse and specular colour from that single light as reflected by the diffuse / specular terms in the pass. Areas in shadow from that light are masked and are thus not updated. The resulting masked colour is added to the existing colour in the scene. Again, no textures are used in this pass (except for textures used for lighting calculations such as normal maps).</li>
<li>decal Passes categorised as ’decal’ add the final texture colour to the scene, which is modulated by the accumulated light built up from all the ambient and diffuse/specular passes.</li>
</ol>
<p>In practice, <a href="#Passes">Passes</a> rarely fall nicely into just one of these categories. For each Technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> compiles a list of ’Illumination Passes’, which are derived from the user defined passes, but can be split, to ensure that the divisions between illumination pass categories can be maintained. For example, if we take a very simple material definition:</p>
<div class="fragment"><div class="line">material TestIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will split this into 3 illumination passes, which will be the equivalent of this:</p>
<div class="fragment"><div class="line">material TestIlluminationSplitIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 0 0 0</div><div class="line">            specular 0 0 0</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Diffuse / specular pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend add</div><div class="line">            iteration once_per_light</div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend modulate</div><div class="line">            lighting off</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>So as you can see, even a simple material requires a minimum of 3 passes when using this shadow technique, and in fact it requires (num_lights + 2) passes in the general sense. You can use more passes in your original material and <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will cope with that too, but be aware that each pass may turn into multiple ones if it uses more than one type of light contribution (ambient vs diffuse/specular) and / or has texture units. The main nice thing is that you get the full multipass lighting behaviour even if you don’t define your materials in terms of it, meaning that your material definitions can remain the same no matter what lighting approach you decide to use.</p>
<p><a class="anchor" id="Manually-Categorising-Illumination-Passes"></a><a class="anchor" id="Manually-Categorising-Illumination-Passes-1"></a></p>
<h2>Manually Categorising Illumination Passes</h2>
<p>Alternatively, if you want more direct control over the categorisation of your passes, you can use the <a href="#illumination_005fstage">illumination_stage</a> option in your pass to explicitly assign a pass unchanged to an illumination stage. This way you can make sure you know precisely how your material will be rendered under additive lighting conditions.</p>
<p><a class="anchor" id="Pass-Classification-and-Vertex-Programs"></a><a class="anchor" id="Pass-Classification-and-Vertex-Programs-1"></a></p>
<h2>Pass Classification and Vertex Programs</h2>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at classifying and splitting your passes to ensure that the multipass rendering approach required by additive lighting works correctly without you having to change your material definitions. However, there is one exception; when you use vertex programs, the normal lighting attributes ambient, diffuse, specular etc are not used, because all of that is determined by the vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has no way of knowing what you’re doing inside that vertex program, so you have to tell it.</p>
<p>In practice this is very easy. Even though your vertex program could be doing a lot of complex, highly customised processing, it can still be classified into one of the 3 types listed above. All you need to do to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> what you’re doing is to use the pass attributes ambient, diffuse, specular and self_illumination, just as if you were not using a vertex program. Sure, these attributes do nothing (as far as rendering is concerned) when you’re using vertex programs, but it’s the easiest way to indicate to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> which light components you’re using in your vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will then classify and potentially split your programmable pass based on this information - it will leave the vertex program as-is (so that any split passes will respect any vertex modification that is being done).</p>
<p>Note that when classifying a diffuse/specular programmable pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> checks to see whether you have indicated the pass can be run once per light (iteration once_per_light). If so, the pass is left intact, including it’s vertex and fragment programs. However, if this attribute is not included in the pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> tries to split off the per-light part, and in doing so it will disable the fragment program, since in the absence of the ’iteration once_per_light’ attribute it can only assume that the fragment program is performing decal work and hence must not be used per light.</p>
<p>So clearly, when you use additive light masking as a shadow technique, you need to make sure that programmable passes you use are properly set up so that they can be classified correctly. However, also note that the changes you have to make to ensure the classification is correct does not affect the way the material renders when you choose not to use additive lighting, so the principle that you should be able to use the same material definitions for all lighting scenarios still holds. Here is an example of a programmable material which will be classified correctly by the illumination pass classifier:</p>
<div class="fragment"><div class="line"><span class="comment">// Per-pixel normal mapping Any number of lights, diffuse and specular</span></div><div class="line">material Examples/BumpMapping/MultiLightSpecular</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Base ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// ambient only, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 1 1 1</div><div class="line">            diffuse 0 0 0 </div><div class="line">            specular 0 0 0 0</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named_auto ambient ambient_light_colour</div><div class="line">            }</div><div class="line">        }</div><div class="line">        <span class="comment">// Now do the lighting pass</span></div><div class="line">        <span class="comment">// NB we don&#39;t do decal texture here because this is repeated per light</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// set ambient off, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 0 0 0 </div><div class="line">            <span class="comment">// do this for each light</span></div><div class="line">            iteration once_per_light</div><div class="line">            scene_blend add</div><div class="line"></div><div class="line">            <span class="comment">// Vertex program reference</span></div><div class="line">            vertex_program_ref Examples/BumpMapVPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightPosition light_position_object_space 0</div><div class="line">                param_named_auto eyePosition camera_position_object_space</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Fragment program</span></div><div class="line">            fragment_program_ref Examples/BumpMapFPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Base bump map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture NMBumpsOut.png</div><div class="line">                colour_op replace</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map #2</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            lighting off</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named ambient float4 1 1 1 1</div><div class="line">            }</div><div class="line">            scene_blend dest_colour zero</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture RustedMetal.jpg </div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to use this explicit sequence - allowing you to compress the number of passes into a much smaller number at the expense of defining an upper number of shadow casting lights. In this case the ’additive’ aspect of the shadow technique just affects the colour of the shadow texture and it’s up to you to combine the shadow textures in your receivers however you like.</p>
<p><a class="anchor" id="Static-Lighting"></a></p>
<h2>Static Lighting</h2>
<p>Despite their power, additive lighting techniques have an additional limitation; they do not combine well with pre-calculated static lighting in the scene. This is because they are based on the principle that shadow is an absence of light, but since static lighting in the scene already includes areas of light and shadow, additive lighting cannot remove light to create new shadows. Therefore, if you use the additive lighting technique you must either use it exclusively as your lighting solution (and you can combine it with per-pixel lighting to create a very impressive dynamic lighting solution), or you must use <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> to combine the static lighting according to your chosen approach. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Shadows</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_shadows.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Shadows </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Enabling-shadows">Enabling shadows</a></li>
<li class="level1"><a href="#Opting-out-of-shadows">Opting out of shadows</a></li>
<li class="level1"><a href="#Stencil-Shadows">Stencil Shadows</a></li>
<li class="level1"><a href="#Texture_002dbased-Shadows">Texture-based Shadows</a></li>
<li class="level1"><a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a></li>
<li class="level1"><a href="#Modulative-Shadows">Modulative Shadows</a></li>
<li class="level1"><a href="#Additive-Light-Masking">Additive Light Masking</a></li>
</ul>
</div>
<div class="textblock"><p>Shadows are clearly an important part of rendering a believable scene - they provide a more tangible feel to the objects in the scene, and aid the viewer in understanding the spatial relationship between objects. Unfortunately, shadows are also one of the most challenging aspects of 3D rendering, and they are still very much an active area of research. Whilst there are many techniques to render shadows, none is perfect and they all come with advantages and disadvantages. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides multiple shadow implementations, with plenty of configuration settings, so you can choose which technique is most appropriate for your scene.</p>
<p>Shadow implementations fall into basically 2 broad categories: <a href="#Stencil-Shadows">Stencil Shadows</a> and <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>. This describes the method by which the shape of the shadow is generated. In addition, there is more than one way to render the shadow into the scene: <a href="#Modulative-Shadows">Modulative Shadows</a>, which darkens the scene in areas of shadow, and <a href="#Additive-Light-Masking">Additive Light Masking</a> which by contrast builds up light contribution in areas which are not in shadow. You also have the option of <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> which gives you complete control over texture shadow application, allowing for complex single-pass shadowing shaders. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports all these combinations.</p>
<h1><a class="anchor" id="Enabling-shadows"></a>
Enabling shadows</h1>
<p>Shadows are disabled by default, here’s how you turn them on and configure them in the general sense:</p>
<ol type="1">
<li><p class="startli">Enable a shadow technique on the SceneManager as the <b>first</b> thing you doing your scene setup. It is important that this is done first because the shadow technique can alter the way meshes are loaded. Here’s an example:</p>
<p class="startli"><code>mSceneMgr-&gt;setShadowTechnique(SHADOWTYPE_STENCIL_ADDITIVE);</code></p>
</li>
<li>Create one or more lights. Note that not all light types are necessarily supported by all shadow techniques, you should check the sections about each technique to check. Note that if certain lights should not cast shadows, you can turn that off by calling setCastShadows(false) on the light, the default is true.</li>
<li>Disable shadow casting on objects which should not cast shadows. Call setCastShadows(false) on objects you don’t want to cast shadows, the default for all objects is to cast shadows.</li>
<li>Configure shadow far distance. You can limit the distance at which shadows are considered for performance reasons, by calling SceneManager::setShadowFarDistance.</li>
<li>Turn off the receipt of shadows on materials that should not receive them. You can turn off the receipt of shadows (note, not the casting of shadows - that is done per-object) by calling Material::setReceiveShadows or using the receive_shadows material attribute. This is useful for materials which should be considered self-illuminated for example. Note that transparent materials are typically excluded from receiving and casting shadows, although see the <a href="#transparency_005fcasts_005fshadows">transparency_casts_shadows</a> option for exceptions.</li>
</ol>
<h1><a class="anchor" id="Opting-out-of-shadows"></a>
Opting out of shadows</h1>
<p>By default <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> treats all non-transparent objects as shadow casters and receivers (depending on the shadow technique they may not be able to be both at once, check the docs for your chosen technique first). You can disable shadows in various ways:</p>
<dl compact="compact">
<dt>Turning off shadow casting on the light </dt>
<dd><p class="startdd"></p>
<p>Calling Light::setCastsShadows(false) will mean this light casts no shadows at all.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow receipt on a material </dt>
<dd><p class="startdd"></p>
<p>Calling Material::setReceiveShadows(false) will prevent any objects using this material from receiving shadows.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow casting on individual objects </dt>
<dd><p class="startdd"></p>
<p>Calling MovableObject::setCastsShadows(false) will disable shadow casting for this object.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadows on an entire rendering queue group </dt>
<dd><p class="startdd"></p>
<p>Calling RenderQueueGroup::setShadowsEnabled(false) will turn off both shadow casting and receiving on an entire rendering queue group. This is useful because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has to do light setup tasks per group in order to preserve the inter-group ordering. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically disables shadows on a number of groups automatically, such as RENDER_QUEUE_BACKGROUND, RENDER_QUEUE_OVERLAY, RENDER_QUEUE_SKIES_EARLY and RENDER_QUEUE_SKIES_LATE. If you choose to use more rendering queues (and by default, you won’t be using any more than this plus the ’standard’ queue, so ignore this if you don’t know what it means!), be aware that each one can incur a light setup cost, and you should disable shadows on the additional ones you use if you can.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Stencil-Shadows"></a>
Stencil Shadows</h1>
<p>Stencil shadows are a method by which a ’mask’ is created for the screen using a feature called the stencil buffer. This mask can be used to exclude areas of the screen from subsequent renders, and thus it can be used to either include or exclude areas in shadow. They are enabled by calling SceneManager::setShadowTechnique with a parameter of either <code>SHADOWTYPE_STENCIL_ADDITIVE</code> or <code>SHADOWTYPE_STENCIL_MODULATIVE</code>. Because the stencil can only mask areas to be either ’enabled’ or ’disabled’, stencil shadows have ’hard’ edges, that is to say clear dividing lines between light and shadow - it is not possible to soften these edges.</p>
<p>In order to generate the stencil, ’shadow volumes’ are rendered by extruding the silhouette of the shadow caster away from the light. Where these shadow volumes intersect other objects (or the caster, since self-shadowing is supported using this technique), the stencil is updated, allowing subsequent operations to differentiate between light and shadow. How exactly this is used to render the shadows depends on whether <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> is being used. Objects can both cast and receive stencil shadows, so self-shadowing is inbuilt.</p>
<p>The advantage of stencil shadows is that they can do self-shadowing simply on low-end hardware, provided you keep your poly count under control. In contrast doing self-shadowing with texture shadows requires a fairly modern machine (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>). For this reason, you’re likely to pick stencil shadows if you need an accurate shadowing solution for an application aimed at older or lower-spec machines.</p>
<p>The disadvantages of stencil shadows are numerous though, especially on more modern hardware. Because stencil shadows are a geometric technique, they are inherently more costly the higher the number of polygons you use, meaning you are penalized the more detailed you make your meshes. The fillrate cost, which comes from having to render shadow volumes, also escalates the same way. Since more modern applications are likely to use higher polygon counts, stencil shadows can start to become a bottleneck. In addition, the visual aspects of stencil shadows are pretty primitive - your shadows will always be hard-edged, and you have no possibility of doing clever things with shaders since the stencil is not available for manipulation there. Therefore, if your application is aimed at higher-end machines you should definitely consider switching to texture shadows (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>).</p>
<p>There are a number of issues to consider which are specific to stencil shadows:</p>
<ul>
<li><a href="#CPU-Overhead">CPU Overhead</a></li>
<li><a href="#Extrusion-distance">Extrusion distance</a></li>
<li><a href="#Camera-far-plane-positioning">Camera far plane positioning</a></li>
<li><a href="#Mesh-edge-lists">Mesh edge lists</a></li>
<li><a href="#The-Silhouette-Edge">The Silhouette Edge</a></li>
<li><a href="#Be-realistic">Be realistic</a></li>
<li><a href="#Stencil-Optimisations-Performed-By-Ogre">Stencil Optimisations Performed By Ogre</a></li>
</ul>
<p><a class="anchor" id="CPU-Overhead"></a><a class="anchor" id="CPU-Overhead-1"></a></p>
<h2>CPU Overhead</h2>
<p>Calculating the shadow volume for a mesh can be expensive, and it has to be done on the CPU, it is not a hardware accelerated feature. Therefore, you can find that if you overuse this feature, you can create a CPU bottleneck for your application. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> quite aggressively eliminates objects which cannot be casting shadows on the frustum, but there are limits to how much it can do, and large, elongated shadows (e.g. representing a very low sun position) are very difficult to cull efficiently. Try to avoid having too many shadow casters around at once, and avoid long shadows if you can. Also, make use of the ’shadow far distance’ parameter on the SceneManager, this can eliminate distant shadow casters from the shadow volume construction and save you some time, at the expense of only having shadows for closer objects. Lastly, make use of Ogre’s Level-Of-Detail (LOD) features; you can generate automatically calculated LODs for your meshes in code (see the Mesh API docs) or when using the mesh tools such as OgreXMLConverter and OgreMeshUpgrader. Alternatively, you can assign your own manual LODs by providing alternative mesh files at lower detail levels. Both methods will cause the shadow volume complexity to decrease as the object gets further away, which saves you valuable volume calculation time.</p>
<p><a class="anchor" id="Extrusion-distance"></a><a class="anchor" id="Extrusion-distance-1"></a></p>
<h2>Extrusion distance</h2>
<p>When vertex programs are not available, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only extrude shadow volumes a finite distance from the object. If an object gets too close to a light, any finite extrusion distance will be inadequate to guarantee all objects will be shadowed properly by this object. Therefore, you are advised not to let shadow casters pass too close to light sources if you can avoid it, unless you can guarantee that your target audience will have vertex program capable hardware (in this case, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> extrudes the volume to infinity using a vertex program so the problem does not occur). When infinite extrusion is not possible, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses finite extrusion, either derived from the attenuation range of a light (in the case of a point light or spotlight), or a fixed extrusion distance set in the application in the case of directional lights. To change the directional light extrusion distance, use SceneManager::setShadowDirectionalLightExtrusionDistance.</p>
<p><a class="anchor" id="Camera-far-plane-positioning"></a><a class="anchor" id="Camera-far-plane-positioning-1"></a></p>
<h2>Camera far plane positioning</h2>
<p>Stencil shadow volumes rely very much on not being clipped by the far plane. When you enable stencil shadows, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> internally changes the far plane settings of your cameras such that there is no far plane - i.e. it is placed at infinity (Camera::setFarClipDistance(0)). This avoids artifacts caused by clipping the dark caps on shadow volumes, at the expense of a (very) small amount of depth precision.</p>
<p><a class="anchor" id="Mesh-edge-lists"></a><a class="anchor" id="Mesh-edge-lists-1"></a></p>
<h2>Mesh edge lists</h2>
<p>Stencil shadows can only be calculated when an ’edge list’ has been built for all the geometry in a mesh. The official exporters and tools automatically build this for you (or have an option to do so), but if you create your own meshes, you must remember to build edge lists for them before using them with stencil shadows - you can do that by using OgreMeshUpgrade or OgreXmlConverter, or by calling Mesh::buildEdgeList before you export or use the mesh. If a mesh doesn’t have edge lists, OGRE assumes that it is not supposed to cast stencil shadows.</p>
<p><a class="anchor" id="The-Silhouette-Edge"></a><a class="anchor" id="The-Silhouette-Edge-1"></a></p>
<h2>The Silhouette Edge</h2>
<p>Stencil shadowing is about finding a silhouette of the mesh, and projecting it away to form a volume. What this means is that there is a definite boundary on the shadow caster between light and shadow; a set of edges where where the triangle on one side is facing toward the light, and one is facing away. This produces a sharp edge around the mesh as the transition occurs. Provided there is little or no other light in the scene, and the mesh has smooth normals to produce a gradual light change in its underlying shading, the silhouette edge can be hidden - this works better the higher the tessellation of the mesh. However, if the scene includes ambient light, then the difference is far more marked. This is especially true when using <a href="#Modulative-Shadows">Modulative Shadows</a>, because the light contribution of each shadowed area is not taken into account by this simplified approach, and so using 2 or more lights in a scene using modulative stencil shadows is not advisable; the silhouette edges will be very marked. Additive lights do not suffer from this as badly because each light is masked individually, meaning that it is only ambient light which can show up the silhouette edges.</p>
<p><a class="anchor" id="Be-realistic"></a><a class="anchor" id="Be-realistic-1"></a></p>
<h2>Be realistic</h2>
<p>Don’t expect to be able to throw any scene using any hardware at the stencil shadow algorithm and expect to get perfect, optimum speed results. Shadows are a complex and expensive technique, so you should impose some reasonable limitations on your placing of lights and objects; they’re not really that restricting, but you should be aware that this is not a complete free-for-all.</p>
<ul>
<li>Try to avoid letting objects pass very close (or even through) lights - it might look nice but it’s one of the cases where artifacts can occur on machines not capable of running vertex programs.</li>
<li>Be aware that shadow volumes do not respect the ’solidity’ of the objects they pass through, and if those objects do not themselves cast shadows (which would hide the effect) then the result will be that you can see shadows on the other side of what should be an occluding object.</li>
<li>Make use of SceneManager::setShadowFarDistance to limit the number of shadow volumes constructed</li>
<li>Make use of LOD to reduce shadow volume complexity at distance</li>
<li><p class="startli">Avoid very long (dusk and dawn) shadows - they exacerbate other issues such as volume clipping, fillrate, and cause many more objects at a greater distance to require volume construction.</p>
<p class="startli"><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre"></a><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre-1"></a></p>
</li>
</ul>
<h2>Stencil Optimisations Performed By <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a></h2>
<p>Despite all that, stencil shadows can look very nice (especially with <a href="#Additive-Light-Masking">Additive Light Masking</a>) and can be fast if you respect the rules above. In addition, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> comes pre-packed with a lot of optimisations which help to make this as quick as possible. This section is more for developers or people interested in knowing something about the ’under the hood’ behaviour of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl compact="compact">
<dt>Vertex program extrusion </dt>
<dd><p class="startdd"></p>
<p>As previously mentioned, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> performs the extrusion of shadow volumes in hardware on vertex program-capable hardware (e.g. GeForce3, Radeon 8500 or better). This has 2 major benefits; the obvious one being speed, but secondly that vertex programs can extrude points to infinity, which the fixed-function pipeline cannot, at least not without performing all calculations in software. This leads to more robust volumes, and also eliminates more than half the volume triangles on directional lights since all points are projected to a single point at infinity.</p>
<p class="enddd"></p>
</dd>
<dt>Scissor test optimisation </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a scissor rectangle to limit the effect of point / spot lights when their range does not cover the entire viewport; that means we save fillrate when rendering stencil volumes, especially with distant lights</p>
<p class="enddd"></p>
</dd>
<dt>Z-Pass and Z-Fail algorithms </dt>
<dd><p class="startdd"></p>
<p>The Z-Fail algorithm, often attributed to John Carmack, is used in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to make sure shadows are robust when the camera passes through the shadow volume. However, the Z-Fail algorithm is more expensive than the traditional Z-Pass; so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> detects when Z-Fail is required and only uses it then, Z-Pass is used at all other times.</p>
<p class="enddd"></p>
</dd>
<dt>2-Sided stenciling and stencil wrapping </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports the 2-Sided stenciling / stencil wrapping extensions, which when supported allow volumes to be rendered in a single pass instead of having to do one pass for back facing tris and another for front-facing tris. This doesn’t save fillrate, since the same number of stencil updates are done, but it does save primitive setup and the overhead incurred in the driver every time a render call is made.</p>
<p class="enddd"></p>
</dd>
<dt>Aggressive shadow volume culling </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at detecting which lights could be affecting the frustum, and from that, which objects could be casting a shadow on the frustum. This means we don’t waste time constructing shadow geometry we don’t need. Setting the shadow far distance is another important way you can reduce stencil shadow overhead since it culls far away shadow volumes even if they are visible, which is beneficial in practice since you’re most interested in shadows for close-up objects.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Texture_002dbased-Shadows"></a>
Texture-based Shadows</h1>
<p>Texture shadows involve rendering shadow casters from the point of view of the light into a texture, which is then projected onto shadow receivers. The main advantage of texture shadows as opposed to <a href="#Stencil-Shadows">Stencil Shadows</a> is that the overhead of increasing the geometric detail is far lower, since there is no need to perform per-triangle calculations. Most of the work in rendering texture shadows is done by the graphics card, meaning the technique scales well when taking advantage of the latest cards, which are at present outpacing CPUs in terms of their speed of development. In addition, texture shadows are <b>much</b> more customisable - you can pull them into shaders to apply as you like (particularly with <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a>, you can perform filtering to create softer shadows or perform other special effects on them. Basically, most modern engines use texture shadows as their primary shadow technique simply because they are more powerful, and the increasing speed of GPUs is rapidly amortizing the fillrate / texture access costs of using them.</p>
<p>The main disadvantage to texture shadows is that, because they are simply a texture, they have a fixed resolution which means if stretched, the pixellation of the texture can become obvious. There are ways to combat this though:</p>
<dl compact="compact">
<dt>Choosing a projection basis </dt>
<dd><p class="startdd"></p>
<p>The simplest projection is just to render the shadow casters from the lights perspective using a regular camera setup. This can look bad though, so there are many other projections which can help to improve the quality from the main camera’s perspective. OGRE supports pluggable projection bases via it’s ShadowCameraSetup class, and comes with several existing options - <b>Uniform</b> (which is the simplest), <b>Uniform Focussed</b> (which is still a normal camera projection, except that the camera is focussed into the area that the main viewing camera is looking at), LiSPSM (Light Space Perspective Shadow Mapping - which both focusses and distorts the shadow frustum based on the main view camera) and Plan Optimal (which seeks to optimise the shadow fidelity for a single receiver plane).</p>
<p class="enddd"></p>
</dd>
<dt>Filtering </dt>
<dd><p class="startdd"></p>
<p>You can also sample the shadow texture multiple times rather than once to soften the shadow edges and improve the appearance. Percentage Closest Filtering (PCF) is the most popular approach, although there are multiple variants depending on the number and pattern of the samples you take. Our shadows demo includes a 5-tap PCF example combined with depth shadow mapping.</p>
<p class="enddd"></p>
</dd>
<dt>Using a larger texture </dt>
<dd><p class="startdd"></p>
<p>Again as GPUs get faster and gain more memory, you can scale up to take advantage of this.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If you combine all 3 of these techniques you can get a very high quality shadow solution.</p>
<p>The other issue is with point lights. Because texture shadows require a render to texture in the direction of the light, omnidirectional lights (point lights) would require 6 renders to totally cover all the directions shadows might be cast. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> primarily supports directional lights and spotlights for generating texture shadows; you can use point lights but they will only work if off-camera since they are essentially turned into a spotlight shining into your camera frustum for the purposes of texture shadows.</p>
<p><a class="anchor" id="Directional-Lights"></a></p>
<h2>Directional Lights</h2>
<p>Directional lights in theory shadow the entire scene from an infinitely distant light. Now, since we only have a finite texture which will look very poor quality if stretched over the entire scene, clearly a simplification is required. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> places a shadow texture over the area immediately in front of the camera, and moves it as the camera moves (although it rounds this movement to multiples of texels so that the slight ’swimming shadow’ effect caused by moving the texture is minimised). The range to which this shadow extends, and the offset used to move it in front of the camera, are configurable (See <a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a>). At the far edge of the shadow, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> fades out the shadow based on other configurable parameters so that the termination of the shadow is softened.</p>
<p><a class="anchor" id="Spotlights"></a></p>
<h2>Spotlights</h2>
<p>Spotlights are much easier to represent as renderable shadow textures than directional lights, since they are naturally a frustum. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> represents spotlight directly by rendering the shadow from the light position, in the direction of the light cone; the field-of-view of the texture camera is adjusted based on the spotlight falloff angles. In addition, to hide the fact that the shadow texture is square and has definite edges which could show up outside the spotlight, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a second texture unit when projecting the shadow onto the scene which fades out the shadow gradually in a projected circle around the spotlight.</p>
<p><a class="anchor" id="Point-Lights"></a></p>
<h2>Point Lights</h2>
<p>As mentioned above, to support point lights properly would require multiple renders (either 6 for a cubic render or perhaps 2 for a less precise parabolic mapping), so rather than do that we approximate point lights as spotlights, where the configuration is changed on the fly to make the light shine from its position over the whole of the viewing frustum. This is not an ideal setup since it means it can only really work if the point light’s position is out of view, and in addition the changing parameterisation can cause some ’swimming’ of the texture. Generally we recommend avoiding making point lights cast texture shadows.</p>
<p><a class="anchor" id="Shadow-Casters-and-Shadow-Receivers"></a></p>
<h2>Shadow Casters and Shadow Receivers</h2>
<p>To enable texture shadows, use the shadow technique SHADOWTYPE_TEXTURE_MODULATIVE or SHADOWTYPE_TEXTURE_ADDITIVE; as the name suggests this produces <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> respectively. The cheapest and simplest texture shadow techniques do not use depth information, they merely render casters to a texture and render this onto receivers as plain colour - this means self-shadowing is not possible using these methods. This is the default behaviour if you use the automatic, fixed-function compatible (and thus usable on lower end hardware) texture shadow techniques. You can however use shaders-based techniques through custom shadow materials for casters and receivers to perform more complex shadow algorithms, such as depth shadow mapping which does allow self-shadowing. OGRE comes with an example of this in its shadows demo, although it’s only usable on Shader Model 2 cards or better. Whilst fixed-function depth shadow mapping is available in OpenGL, it was never standardised in Direct3D so using shaders in custom caster &amp; receiver materials is the only portable way to do it. If you use this approach, call SceneManager::setShadowTextureSelfShadow with a parameter of ’true’ to allow texture shadow casters to also be receivers. If you’re not using depth shadow mapping, OGRE divides shadow casters and receivers into 2 disjoint groups. Simply by turning off shadow casting on an object, you automatically make it a shadow receiver (although this can be disabled by setting the ’receive_shadows’ option to ’false’ in a material script. Similarly, if an object is set as a shadow caster, it cannot receive shadows.</p>
<h1><a class="anchor" id="Configuring-Texture-Shadows"></a>
Configuring Texture Shadows</h1>
<p>There are a number of settings which will help you configure your texture-based shadows so that they match your requirements.</p>
<ul>
<li><a href="#Maximum-number-of-shadow-textures">Maximum number of shadow textures</a></li>
<li><a href="#Shadow-texture-size">Shadow texture size</a></li>
<li><a href="#Shadow-far-distance">Shadow far distance</a></li>
<li><a href="#Shadow-texture-offset-Directional-Lights_0029">Shadow texture offset (Directional Lights)</a></li>
<li><a href="#Shadow-fade-settings">Shadow fade settings</a></li>
<li><a href="#Custom-shadow-camera-setups">Custom shadow camera setups</a></li>
<li><a href="#Shadow-texture-Depth-Buffer-sharing">Shadow texture Depth Buffer sharing</a></li>
<li><a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a></li>
</ul>
<p><a class="anchor" id="Maximum-number-of-shadow-textures"></a><a class="anchor" id="Maximum-number-of-shadow-textures-1"></a></p>
<h2>Maximum number of shadow textures</h2>
<p>Shadow textures take up texture memory, and to avoid stalling the rendering pipeline <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> does not reuse the same shadow texture for multiple lights within the same frame. This means that each light which is to cast shadows must have its own shadow texture. In practice, if you have a lot of lights in your scene you would not wish to incur that sort of texture overhead. You can adjust this manually by simply turning off shadow casting for lights you do not wish to cast shadows. In addition, you can set a maximum limit on the number of shadow textures <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is allowed to use by calling SceneManager::setShadowTextureCount. Each frame, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> determines the lights which could be affecting the frustum, and then allocates the number of shadow textures it is allowed to use to the lights on a first-come-first-served basis. Any additional lights will not cast shadows that frame. Note that you can set the number of shadow textures and their size at the same time by using the SceneManager::setShadowTextureSettings method; this is useful because both the individual calls require the potential creation / destruction of texture resources.</p>
<p><a class="anchor" id="Shadow-texture-size"></a><a class="anchor" id="Shadow-texture-size-1"></a></p>
<h2>Shadow texture size</h2>
<p>The size of the textures used for rendering the shadow casters into can be altered; clearly using larger textures will give you better quality shadows, but at the expense of greater memory usage. Changing the texture size is done by calling SceneManager::setShadowTextureSize - textures are assumed to be square and you must specify a texture size that is a power of 2. Be aware that each modulative shadow texture will take size*size*3 bytes of texture memory. <b>Important</b>: if you use the GL render system your shadow texture size can only be larger (in either dimension) than the size of your primary window surface if the hardware supports the Frame Buffer Object (FBO) or Pixel Buffer Object (PBO) extensions. Most modern cards support this now, but be careful of older cards - you can check the ability of the hardware to manage this through ogreRoot-&gt;getRenderSystem()-&gt;getCapabilities()-&gt;hasCapability(RSC_HWRENDER_TO_TEXTURE). If this returns false, if you create a shadow texture larger in any dimension than the primary surface, the rest of the shadow texture will be blank.</p>
<p><a class="anchor" id="Shadow-far-distance"></a><a class="anchor" id="Shadow-far-distance-1"></a></p>
<h2>Shadow far distance</h2>
<p>This determines the distance at which shadows are terminated; it also determines how far into the distance the texture shadows for directional lights are stretched - by reducing this value, or increasing the texture size, you can improve the quality of shadows from directional lights at the expense of closer shadow termination or increased memory usage, respectively.</p>
<p><a class="anchor" id="Shadow-texture-offset-Directional-Lights_0029"></a></p>
<h2>Shadow texture offset (Directional Lights)</h2>
<p>As mentioned above in the directional lights section, the rendering of shadows for directional lights is an approximation that allows us to use a single render to cover a largish area with shadows. This offset parameter affects how far from the camera position the center of the shadow texture is offset, as a proportion of the shadow far distance. The greater this value, the more of the shadow texture is ’useful’ to you since it’s ahead of the camera, but also the further you offset it, the more chance there is of accidentally seeing the edge of the shadow texture at more extreme angles. You change this value by calling SceneManager::setShadowDirLightTextureOffset, the default is 0.6.</p>
<p><a class="anchor" id="Shadow-fade-settings"></a><a class="anchor" id="Shadow-fade-settings-1"></a></p>
<h2>Shadow fade settings</h2>
<p>Shadows fade out before the shadow far distance so that the termination of shadow is not abrupt. You can configure the start and end points of this fade by calling the SceneManager::setShadowTextureFadeStart and SceneManager::setShadowTextureFadeEnd methods, both take distances as a proportion of the shadow far distance. Because of the inaccuracies caused by using a square texture and a radial fade distance, you cannot use 1.0 as the fade end, if you do you’ll see artifacts at the extreme edges. The default values are 0.7 and 0.9, which serve most purposes but you can change them if you like.</p>
<h1>Texture shadows and vertex / fragment programs</h1>
<p>When rendering shadow casters into a modulative shadow texture, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> turns off all textures, and all lighting contributions except for ambient light, which it sets to the colour of the shadow (<a href="#Shadow-Colour">Shadow Colour</a>). For additive shadows, it render the casters into a black &amp; white texture instead. This is enough to render shadow casters for fixed-function material techniques, however where a vertex program is used <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> doesn’t have so much control. If you use a vertex program in the <b>first pass</b> of your technique, then you must also tell ogre which vertex program you want it to use when rendering the shadow caster; see <a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a> for full details.</p>
<p><a class="anchor" id="Custom-shadow-camera-setups"></a><a class="anchor" id="Custom-shadow-camera-setups-1"></a></p>
<h2>Custom shadow camera setups</h2>
<p>As previously mentioned, one of the downsides of texture shadows is that the texture resolution is finite, and it’s possible to get aliasing when the size of the shadow texel is larger than a screen pixel, due to the projection of the texture. In order to address this, you can specify alternative projection bases by using or creating subclasses of the ShadowCameraSetup class. The default version is called DefaultShadowCameraSetup and this sets up a simple regular frustum for point and spotlights, and an orthographic frustum for directional lights. There is also a PlaneOptimalShadowCameraSetup class which specialises the projection to a plane, thus giving you much better definition provided your shadow receivers exist mostly in a single plane. Other setup classes (e.g. you might create a perspective or trapezoid shadow mapping version) can be created and plugged in at runtime, either on individual lights or on the SceneManager as a whole.</p>
<p><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing"></a><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing-1"></a></p>
<h2>Shadow texture Depth Buffer sharing</h2>
<p>Shadow textures need a depth buffer like many other RTs (Render Textures). Prior to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8, the depth buffer behavior was left undefined leaving a very small possibility of causing inconsistencies across different window resolutions and render systems. Depending on the render window’s resolutions and/or rendersystem being used, the depth buffer might been shared with the render window or a new one could get created to suite the shadow textures. If the application was depending on the depth buffer contents from the previous scene render (that is, no clear was performed) where a shadow texture render pass was in the middle; then the depth buffer would’ve contained garbage (but not consistent on all machines) causing graphical glitches hard to spot.</p>
<p>From <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8 onwards the depth buffer usage &amp; sharing can be flexible controlled through the use of depth pool IDs. These pool IDs are not specifically part of shadow textures, but rather anything involving RTs. All RTs with the same pool ID share the same depth buffers when possible (following RenderSystem API rules, check RenderSystemCapabilities flags to find out what the behavior will be). The default ID for shadow textures is 1; which is the same default value for render windows, and RTTs; thus maintaining the same behavior with older applications while achieving maximum memory saving and performance efficiency because the number of created depth buffers is as lowest as possible.</p>
<p>However there are some reasons to put shadow textures in a separate pool. This holds specially true if the application depends on the previous contents from the depth buffer before the shadow pass, instead of doing a clear:</p>
<ul>
<li>In Direct3D9, the shadow texture is more likely to share the depth buffer with the render window at high resolutions (when the window is bigger than the shadow texture resolution), but at low resolutions it won’t be shared, thus causing two different behaviors. Also probably the shadow texture will share the depth buffers with most other RTTs (i.e. compositors)</li>
<li>In OpenGL 2.1, the shadow texture can’t be shared with the main render window; and most likely will NOT be shared with many other RTTs (i.e. compositors) since OGL 2.1 has a requirement that texture resolutions should exactly match, while D3D9 specifies depth buffers can be shared as long as the resolutions are equal or less.</li>
</ul>
<p>For example, the DeferredShading sample suffers from this problem. If this is a problem for a particular effect you’re trying to achieve, you can specify a custom pool ID so that shadow textures get their own depth buffer(s), ensuring they aren’t shared with other RTs. You can set the poolId parameter from either SceneManager::setShadowTextureSettings or setShadowTextureConfig</p>
<div class="fragment"><div class="line">mSceneMgr-&gt;setShadowTextureSettings( size, count, format, PoolId );</div><div class="line">mSceneMgr-&gt;setShadowTextureConfig( 0, 512, 512, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39cab63dbcddd53d1f766d742fd557c26be4">PF_FLOAT16_R</a>, 50 );</div></div><!-- fragment --><p>Note a poolId of 0 will make the shadow textures not to use a depth buffer, which isn’t usually a desired behavior.</p>
<p><a class="anchor" id="Integrated-Texture-Shadows"></a><a class="anchor" id="Integrated-Texture-Shadows-1"></a></p>
<h2>Integrated Texture Shadows</h2>
<p>Texture shadows have one major advantage over stencil shadows - the data used to represent them can be referenced in regular shaders. Whilst the default texture shadow modes (SHADOWTYPE_TEXTURE_MODULATIVE and SHADOWTYPE_TEXTURE_ADDITIVE) automatically render shadows for you, their disadvantage is that because they are generalised add-ons to your own materials, they tend to take more passes of the scene to use. In addition, you don’t have a lot of control over the composition of the shadows.</p>
<p>Here is where ’integrated’ texture shadows step in. Both of the texture shadow types above have alternative versions called SHADOWTYPE_TEXTURE_MODULATIVE_INTEGRATED and SHADOWTYPE_TEXTURE_ADDITIVE_INTEGRATED, where instead of rendering the shadows for you, it just creates the texture shadow and then expects you to use that shadow texture as you see fit when rendering receiver objects in the scene. The downside is that you have to take into account shadow receipt in every one of your materials if you use this option - the upside is that you have total control over how the shadow textures are used. The big advantage here is that you can can perform more complex shading, taking into account shadowing, than is possible using the generalised bolt-on approaches, AND you can probably write them in a smaller number of passes, since you know precisely what you need and can combine passes where possible. When you use one of these shadowing approaches, the only difference between additive and modulative is the colour of the casters in the shadow texture (the shadow colour for modulative, black for additive) - the actual calculation of how the texture affects the receivers is of course up to you. No separate modulative pass will be performed, and no splitting of your materials into ambient / per-light / decal etc will occur - absolutely everything is determined by your original material (which may have modulative passes or per-light iteration if you want of course, but it’s not required).</p>
<p>You reference a shadow texture in a material which implements this approach by using the ’<a href="#content_005ftype">content_type</a> shadow’ directive in your <a href="#Texture-Units">texture_unit</a>. It implicitly references a shadow texture based on the number of times you’ve used this directive in the same pass, and the light_start option or light-based pass iteration, which might start the light index higher than 0.</p>
<h1><a class="anchor" id="Modulative-Shadows"></a>
Modulative Shadows</h1>
<p>Modulative shadows work by darkening an already rendered scene with a fixed colour. First, the scene is rendered normally containing all the objects which will be shadowed, then a modulative pass is done per light, which darkens areas in shadow. Finally, objects which do not receive shadows are rendered.</p>
<p>There are 2 modulative shadow techniques; stencil-based (See <a href="#Stencil-Shadows">Stencil Shadows</a> : SHADOWTYPE_STENCIL_MODULATIVE) and texture-based (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a> : SHADOWTYPE_TEXTURE_MODULATIVE). Modulative shadows are an inaccurate lighting model, since they darken the areas of shadow uniformly, irrespective of the amount of light which would have fallen on the shadow area anyway. However, they can give fairly attractive results for a much lower overhead than more ’correct’ methods like <a href="#Additive-Light-Masking">Additive Light Masking</a>, and they also combine well with pre-baked static lighting (such as pre-calculated lightmaps), which additive lighting does not. The main thing to consider is that using multiple light sources can result in overly dark shadows (where shadows overlap, which intuitively looks right in fact, but it’s not physically correct) and artifacts when using stencil shadows (See <a href="#The-Silhouette-Edge">The Silhouette Edge</a>).</p>
<p><a class="anchor" id="Shadow-Colour"></a><a class="anchor" id="Shadow-Colour-1"></a></p>
<h2>Shadow Colour</h2>
<p>The colour which is used to darken the areas in shadow is set by SceneManager::setShadowColour; it defaults to a dark grey (so that the underlying colour still shows through a bit).</p>
<p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to have a separate pass of the scene to render shadows. In this case the ’modulative’ aspect of the shadow technique just affects the colour of the shadow texture.</p>
<h1><a class="anchor" id="Additive-Light-Masking"></a>
Additive Light Masking</h1>
<p>Additive light masking is about rendering the scene many times, each time representing a single light contribution whose influence is masked out in areas of shadow. Each pass is combined with (added to) the previous one such that when all the passes are complete, all the light contribution has correctly accumulated in the scene, and each light has been prevented from affecting areas which it should not be able to because of shadow casters. This is an effective technique which results in very realistic looking lighting, but it comes at a price: more rendering passes.</p>
<p>As many technical papers (and game marketing) will tell you, rendering realistic lighting like this requires multiple passes. Being a friendly sort of engine, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> frees you from most of the hard work though, and will let you use the exact same material definitions whether you use this lighting technique or not (for the most part, see <a href="#Pass-Classification-and-Vertex-Programs">Pass Classification and Vertex Programs</a>). In order to do this technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically categorises the <a href="#Passes">Passes</a> you define in your materials into 3 types:</p>
<ol type="1">
<li>ambient Passes categorised as ’ambient’ include any base pass which is not lit by any particular light, i.e. it occurs even if there is no ambient light in the scene. The ambient pass always happens first, and sets up the initial depth value of the fragments, and the ambient colour if applicable. It also includes any emissive / self illumination contribution. Only textures which affect ambient light (e.g. ambient occlusion maps) should be rendered in this pass.</li>
<li>diffuse/specular Passes categorised as ’diffuse/specular’ (or ’per-light’) are rendered once per light, and each pass contributes the diffuse and specular colour from that single light as reflected by the diffuse / specular terms in the pass. Areas in shadow from that light are masked and are thus not updated. The resulting masked colour is added to the existing colour in the scene. Again, no textures are used in this pass (except for textures used for lighting calculations such as normal maps).</li>
<li>decal Passes categorised as ’decal’ add the final texture colour to the scene, which is modulated by the accumulated light built up from all the ambient and diffuse/specular passes.</li>
</ol>
<p>In practice, <a href="#Passes">Passes</a> rarely fall nicely into just one of these categories. For each Technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> compiles a list of ’Illumination Passes’, which are derived from the user defined passes, but can be split, to ensure that the divisions between illumination pass categories can be maintained. For example, if we take a very simple material definition:</p>
<div class="fragment"><div class="line">material TestIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will split this into 3 illumination passes, which will be the equivalent of this:</p>
<div class="fragment"><div class="line">material TestIlluminationSplitIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 0 0 0</div><div class="line">            specular 0 0 0</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Diffuse / specular pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend add</div><div class="line">            iteration once_per_light</div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend modulate</div><div class="line">            lighting off</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>So as you can see, even a simple material requires a minimum of 3 passes when using this shadow technique, and in fact it requires (num_lights + 2) passes in the general sense. You can use more passes in your original material and <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will cope with that too, but be aware that each pass may turn into multiple ones if it uses more than one type of light contribution (ambient vs diffuse/specular) and / or has texture units. The main nice thing is that you get the full multipass lighting behaviour even if you don’t define your materials in terms of it, meaning that your material definitions can remain the same no matter what lighting approach you decide to use.</p>
<p><a class="anchor" id="Manually-Categorising-Illumination-Passes"></a><a class="anchor" id="Manually-Categorising-Illumination-Passes-1"></a></p>
<h2>Manually Categorising Illumination Passes</h2>
<p>Alternatively, if you want more direct control over the categorisation of your passes, you can use the <a href="#illumination_005fstage">illumination_stage</a> option in your pass to explicitly assign a pass unchanged to an illumination stage. This way you can make sure you know precisely how your material will be rendered under additive lighting conditions.</p>
<p><a class="anchor" id="Pass-Classification-and-Vertex-Programs"></a><a class="anchor" id="Pass-Classification-and-Vertex-Programs-1"></a></p>
<h2>Pass Classification and Vertex Programs</h2>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at classifying and splitting your passes to ensure that the multipass rendering approach required by additive lighting works correctly without you having to change your material definitions. However, there is one exception; when you use vertex programs, the normal lighting attributes ambient, diffuse, specular etc are not used, because all of that is determined by the vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has no way of knowing what you’re doing inside that vertex program, so you have to tell it.</p>
<p>In practice this is very easy. Even though your vertex program could be doing a lot of complex, highly customised processing, it can still be classified into one of the 3 types listed above. All you need to do to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> what you’re doing is to use the pass attributes ambient, diffuse, specular and self_illumination, just as if you were not using a vertex program. Sure, these attributes do nothing (as far as rendering is concerned) when you’re using vertex programs, but it’s the easiest way to indicate to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> which light components you’re using in your vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will then classify and potentially split your programmable pass based on this information - it will leave the vertex program as-is (so that any split passes will respect any vertex modification that is being done).</p>
<p>Note that when classifying a diffuse/specular programmable pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> checks to see whether you have indicated the pass can be run once per light (iteration once_per_light). If so, the pass is left intact, including it’s vertex and fragment programs. However, if this attribute is not included in the pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> tries to split off the per-light part, and in doing so it will disable the fragment program, since in the absence of the ’iteration once_per_light’ attribute it can only assume that the fragment program is performing decal work and hence must not be used per light.</p>
<p>So clearly, when you use additive light masking as a shadow technique, you need to make sure that programmable passes you use are properly set up so that they can be classified correctly. However, also note that the changes you have to make to ensure the classification is correct does not affect the way the material renders when you choose not to use additive lighting, so the principle that you should be able to use the same material definitions for all lighting scenarios still holds. Here is an example of a programmable material which will be classified correctly by the illumination pass classifier:</p>
<div class="fragment"><div class="line"><span class="comment">// Per-pixel normal mapping Any number of lights, diffuse and specular</span></div><div class="line">material Examples/BumpMapping/MultiLightSpecular</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Base ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// ambient only, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 1 1 1</div><div class="line">            diffuse 0 0 0 </div><div class="line">            specular 0 0 0 0</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named_auto ambient ambient_light_colour</div><div class="line">            }</div><div class="line">        }</div><div class="line">        <span class="comment">// Now do the lighting pass</span></div><div class="line">        <span class="comment">// NB we don&#39;t do decal texture here because this is repeated per light</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// set ambient off, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 0 0 0 </div><div class="line">            <span class="comment">// do this for each light</span></div><div class="line">            iteration once_per_light</div><div class="line">            scene_blend add</div><div class="line"></div><div class="line">            <span class="comment">// Vertex program reference</span></div><div class="line">            vertex_program_ref Examples/BumpMapVPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightPosition light_position_object_space 0</div><div class="line">                param_named_auto eyePosition camera_position_object_space</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Fragment program</span></div><div class="line">            fragment_program_ref Examples/BumpMapFPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Base bump map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture NMBumpsOut.png</div><div class="line">                colour_op replace</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map #2</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            lighting off</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named ambient float4 1 1 1 1</div><div class="line">            }</div><div class="line">            scene_blend dest_colour zero</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture RustedMetal.jpg </div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to use this explicit sequence - allowing you to compress the number of passes into a much smaller number at the expense of defining an upper number of shadow casting lights. In this case the ’additive’ aspect of the shadow technique just affects the colour of the shadow texture and it’s up to you to combine the shadow textures in your receivers however you like.</p>
<p><a class="anchor" id="Static-Lighting"></a></p>
<h2>Static Lighting</h2>
<p>Despite their power, additive lighting techniques have an additional limitation; they do not combine well with pre-calculated static lighting in the scene. This is because they are based on the principle that shadow is an absence of light, but since static lighting in the scene already includes areas of light and shadow, additive lighting cannot remove light to create new shadows. Therefore, if you use the additive lighting technique you must either use it exclusively as your lighting solution (and you can combine it with per-pixel lighting to create a very impressive dynamic lighting solution), or you must use <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> to combine the static lighting according to your chosen approach. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Shadows</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_shadows.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Shadows </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Enabling-shadows">Enabling shadows</a></li>
<li class="level1"><a href="#Opting-out-of-shadows">Opting out of shadows</a></li>
<li class="level1"><a href="#Stencil-Shadows">Stencil Shadows</a></li>
<li class="level1"><a href="#Texture_002dbased-Shadows">Texture-based Shadows</a></li>
<li class="level1"><a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a></li>
<li class="level1"><a href="#Modulative-Shadows">Modulative Shadows</a></li>
<li class="level1"><a href="#Additive-Light-Masking">Additive Light Masking</a></li>
</ul>
</div>
<div class="textblock"><p>Shadows are clearly an important part of rendering a believable scene - they provide a more tangible feel to the objects in the scene, and aid the viewer in understanding the spatial relationship between objects. Unfortunately, shadows are also one of the most challenging aspects of 3D rendering, and they are still very much an active area of research. Whilst there are many techniques to render shadows, none is perfect and they all come with advantages and disadvantages. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides multiple shadow implementations, with plenty of configuration settings, so you can choose which technique is most appropriate for your scene.</p>
<p>Shadow implementations fall into basically 2 broad categories: <a href="#Stencil-Shadows">Stencil Shadows</a> and <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>. This describes the method by which the shape of the shadow is generated. In addition, there is more than one way to render the shadow into the scene: <a href="#Modulative-Shadows">Modulative Shadows</a>, which darkens the scene in areas of shadow, and <a href="#Additive-Light-Masking">Additive Light Masking</a> which by contrast builds up light contribution in areas which are not in shadow. You also have the option of <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> which gives you complete control over texture shadow application, allowing for complex single-pass shadowing shaders. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports all these combinations.</p>
<h1><a class="anchor" id="Enabling-shadows"></a>
Enabling shadows</h1>
<p>Shadows are disabled by default, here’s how you turn them on and configure them in the general sense:</p>
<ol type="1">
<li><p class="startli">Enable a shadow technique on the SceneManager as the <b>first</b> thing you doing your scene setup. It is important that this is done first because the shadow technique can alter the way meshes are loaded. Here’s an example:</p>
<p class="startli"><code>mSceneMgr-&gt;setShadowTechnique(SHADOWTYPE_STENCIL_ADDITIVE);</code></p>
</li>
<li>Create one or more lights. Note that not all light types are necessarily supported by all shadow techniques, you should check the sections about each technique to check. Note that if certain lights should not cast shadows, you can turn that off by calling setCastShadows(false) on the light, the default is true.</li>
<li>Disable shadow casting on objects which should not cast shadows. Call setCastShadows(false) on objects you don’t want to cast shadows, the default for all objects is to cast shadows.</li>
<li>Configure shadow far distance. You can limit the distance at which shadows are considered for performance reasons, by calling SceneManager::setShadowFarDistance.</li>
<li>Turn off the receipt of shadows on materials that should not receive them. You can turn off the receipt of shadows (note, not the casting of shadows - that is done per-object) by calling Material::setReceiveShadows or using the receive_shadows material attribute. This is useful for materials which should be considered self-illuminated for example. Note that transparent materials are typically excluded from receiving and casting shadows, although see the <a href="#transparency_005fcasts_005fshadows">transparency_casts_shadows</a> option for exceptions.</li>
</ol>
<h1><a class="anchor" id="Opting-out-of-shadows"></a>
Opting out of shadows</h1>
<p>By default <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> treats all non-transparent objects as shadow casters and receivers (depending on the shadow technique they may not be able to be both at once, check the docs for your chosen technique first). You can disable shadows in various ways:</p>
<dl compact="compact">
<dt>Turning off shadow casting on the light </dt>
<dd><p class="startdd"></p>
<p>Calling Light::setCastsShadows(false) will mean this light casts no shadows at all.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow receipt on a material </dt>
<dd><p class="startdd"></p>
<p>Calling Material::setReceiveShadows(false) will prevent any objects using this material from receiving shadows.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow casting on individual objects </dt>
<dd><p class="startdd"></p>
<p>Calling MovableObject::setCastsShadows(false) will disable shadow casting for this object.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadows on an entire rendering queue group </dt>
<dd><p class="startdd"></p>
<p>Calling RenderQueueGroup::setShadowsEnabled(false) will turn off both shadow casting and receiving on an entire rendering queue group. This is useful because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has to do light setup tasks per group in order to preserve the inter-group ordering. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically disables shadows on a number of groups automatically, such as RENDER_QUEUE_BACKGROUND, RENDER_QUEUE_OVERLAY, RENDER_QUEUE_SKIES_EARLY and RENDER_QUEUE_SKIES_LATE. If you choose to use more rendering queues (and by default, you won’t be using any more than this plus the ’standard’ queue, so ignore this if you don’t know what it means!), be aware that each one can incur a light setup cost, and you should disable shadows on the additional ones you use if you can.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Stencil-Shadows"></a>
Stencil Shadows</h1>
<p>Stencil shadows are a method by which a ’mask’ is created for the screen using a feature called the stencil buffer. This mask can be used to exclude areas of the screen from subsequent renders, and thus it can be used to either include or exclude areas in shadow. They are enabled by calling SceneManager::setShadowTechnique with a parameter of either <code>SHADOWTYPE_STENCIL_ADDITIVE</code> or <code>SHADOWTYPE_STENCIL_MODULATIVE</code>. Because the stencil can only mask areas to be either ’enabled’ or ’disabled’, stencil shadows have ’hard’ edges, that is to say clear dividing lines between light and shadow - it is not possible to soften these edges.</p>
<p>In order to generate the stencil, ’shadow volumes’ are rendered by extruding the silhouette of the shadow caster away from the light. Where these shadow volumes intersect other objects (or the caster, since self-shadowing is supported using this technique), the stencil is updated, allowing subsequent operations to differentiate between light and shadow. How exactly this is used to render the shadows depends on whether <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> is being used. Objects can both cast and receive stencil shadows, so self-shadowing is inbuilt.</p>
<p>The advantage of stencil shadows is that they can do self-shadowing simply on low-end hardware, provided you keep your poly count under control. In contrast doing self-shadowing with texture shadows requires a fairly modern machine (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>). For this reason, you’re likely to pick stencil shadows if you need an accurate shadowing solution for an application aimed at older or lower-spec machines.</p>
<p>The disadvantages of stencil shadows are numerous though, especially on more modern hardware. Because stencil shadows are a geometric technique, they are inherently more costly the higher the number of polygons you use, meaning you are penalized the more detailed you make your meshes. The fillrate cost, which comes from having to render shadow volumes, also escalates the same way. Since more modern applications are likely to use higher polygon counts, stencil shadows can start to become a bottleneck. In addition, the visual aspects of stencil shadows are pretty primitive - your shadows will always be hard-edged, and you have no possibility of doing clever things with shaders since the stencil is not available for manipulation there. Therefore, if your application is aimed at higher-end machines you should definitely consider switching to texture shadows (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>).</p>
<p>There are a number of issues to consider which are specific to stencil shadows:</p>
<ul>
<li><a href="#CPU-Overhead">CPU Overhead</a></li>
<li><a href="#Extrusion-distance">Extrusion distance</a></li>
<li><a href="#Camera-far-plane-positioning">Camera far plane positioning</a></li>
<li><a href="#Mesh-edge-lists">Mesh edge lists</a></li>
<li><a href="#The-Silhouette-Edge">The Silhouette Edge</a></li>
<li><a href="#Be-realistic">Be realistic</a></li>
<li><a href="#Stencil-Optimisations-Performed-By-Ogre">Stencil Optimisations Performed By Ogre</a></li>
</ul>
<p><a class="anchor" id="CPU-Overhead"></a><a class="anchor" id="CPU-Overhead-1"></a></p>
<h2>CPU Overhead</h2>
<p>Calculating the shadow volume for a mesh can be expensive, and it has to be done on the CPU, it is not a hardware accelerated feature. Therefore, you can find that if you overuse this feature, you can create a CPU bottleneck for your application. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> quite aggressively eliminates objects which cannot be casting shadows on the frustum, but there are limits to how much it can do, and large, elongated shadows (e.g. representing a very low sun position) are very difficult to cull efficiently. Try to avoid having too many shadow casters around at once, and avoid long shadows if you can. Also, make use of the ’shadow far distance’ parameter on the SceneManager, this can eliminate distant shadow casters from the shadow volume construction and save you some time, at the expense of only having shadows for closer objects. Lastly, make use of Ogre’s Level-Of-Detail (LOD) features; you can generate automatically calculated LODs for your meshes in code (see the Mesh API docs) or when using the mesh tools such as OgreXMLConverter and OgreMeshUpgrader. Alternatively, you can assign your own manual LODs by providing alternative mesh files at lower detail levels. Both methods will cause the shadow volume complexity to decrease as the object gets further away, which saves you valuable volume calculation time.</p>
<p><a class="anchor" id="Extrusion-distance"></a><a class="anchor" id="Extrusion-distance-1"></a></p>
<h2>Extrusion distance</h2>
<p>When vertex programs are not available, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only extrude shadow volumes a finite distance from the object. If an object gets too close to a light, any finite extrusion distance will be inadequate to guarantee all objects will be shadowed properly by this object. Therefore, you are advised not to let shadow casters pass too close to light sources if you can avoid it, unless you can guarantee that your target audience will have vertex program capable hardware (in this case, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> extrudes the volume to infinity using a vertex program so the problem does not occur). When infinite extrusion is not possible, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses finite extrusion, either derived from the attenuation range of a light (in the case of a point light or spotlight), or a fixed extrusion distance set in the application in the case of directional lights. To change the directional light extrusion distance, use SceneManager::setShadowDirectionalLightExtrusionDistance.</p>
<p><a class="anchor" id="Camera-far-plane-positioning"></a><a class="anchor" id="Camera-far-plane-positioning-1"></a></p>
<h2>Camera far plane positioning</h2>
<p>Stencil shadow volumes rely very much on not being clipped by the far plane. When you enable stencil shadows, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> internally changes the far plane settings of your cameras such that there is no far plane - i.e. it is placed at infinity (Camera::setFarClipDistance(0)). This avoids artifacts caused by clipping the dark caps on shadow volumes, at the expense of a (very) small amount of depth precision.</p>
<p><a class="anchor" id="Mesh-edge-lists"></a><a class="anchor" id="Mesh-edge-lists-1"></a></p>
<h2>Mesh edge lists</h2>
<p>Stencil shadows can only be calculated when an ’edge list’ has been built for all the geometry in a mesh. The official exporters and tools automatically build this for you (or have an option to do so), but if you create your own meshes, you must remember to build edge lists for them before using them with stencil shadows - you can do that by using OgreMeshUpgrade or OgreXmlConverter, or by calling Mesh::buildEdgeList before you export or use the mesh. If a mesh doesn’t have edge lists, OGRE assumes that it is not supposed to cast stencil shadows.</p>
<p><a class="anchor" id="The-Silhouette-Edge"></a><a class="anchor" id="The-Silhouette-Edge-1"></a></p>
<h2>The Silhouette Edge</h2>
<p>Stencil shadowing is about finding a silhouette of the mesh, and projecting it away to form a volume. What this means is that there is a definite boundary on the shadow caster between light and shadow; a set of edges where where the triangle on one side is facing toward the light, and one is facing away. This produces a sharp edge around the mesh as the transition occurs. Provided there is little or no other light in the scene, and the mesh has smooth normals to produce a gradual light change in its underlying shading, the silhouette edge can be hidden - this works better the higher the tessellation of the mesh. However, if the scene includes ambient light, then the difference is far more marked. This is especially true when using <a href="#Modulative-Shadows">Modulative Shadows</a>, because the light contribution of each shadowed area is not taken into account by this simplified approach, and so using 2 or more lights in a scene using modulative stencil shadows is not advisable; the silhouette edges will be very marked. Additive lights do not suffer from this as badly because each light is masked individually, meaning that it is only ambient light which can show up the silhouette edges.</p>
<p><a class="anchor" id="Be-realistic"></a><a class="anchor" id="Be-realistic-1"></a></p>
<h2>Be realistic</h2>
<p>Don’t expect to be able to throw any scene using any hardware at the stencil shadow algorithm and expect to get perfect, optimum speed results. Shadows are a complex and expensive technique, so you should impose some reasonable limitations on your placing of lights and objects; they’re not really that restricting, but you should be aware that this is not a complete free-for-all.</p>
<ul>
<li>Try to avoid letting objects pass very close (or even through) lights - it might look nice but it’s one of the cases where artifacts can occur on machines not capable of running vertex programs.</li>
<li>Be aware that shadow volumes do not respect the ’solidity’ of the objects they pass through, and if those objects do not themselves cast shadows (which would hide the effect) then the result will be that you can see shadows on the other side of what should be an occluding object.</li>
<li>Make use of SceneManager::setShadowFarDistance to limit the number of shadow volumes constructed</li>
<li>Make use of LOD to reduce shadow volume complexity at distance</li>
<li><p class="startli">Avoid very long (dusk and dawn) shadows - they exacerbate other issues such as volume clipping, fillrate, and cause many more objects at a greater distance to require volume construction.</p>
<p class="startli"><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre"></a><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre-1"></a></p>
</li>
</ul>
<h2>Stencil Optimisations Performed By <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a></h2>
<p>Despite all that, stencil shadows can look very nice (especially with <a href="#Additive-Light-Masking">Additive Light Masking</a>) and can be fast if you respect the rules above. In addition, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> comes pre-packed with a lot of optimisations which help to make this as quick as possible. This section is more for developers or people interested in knowing something about the ’under the hood’ behaviour of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl compact="compact">
<dt>Vertex program extrusion </dt>
<dd><p class="startdd"></p>
<p>As previously mentioned, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> performs the extrusion of shadow volumes in hardware on vertex program-capable hardware (e.g. GeForce3, Radeon 8500 or better). This has 2 major benefits; the obvious one being speed, but secondly that vertex programs can extrude points to infinity, which the fixed-function pipeline cannot, at least not without performing all calculations in software. This leads to more robust volumes, and also eliminates more than half the volume triangles on directional lights since all points are projected to a single point at infinity.</p>
<p class="enddd"></p>
</dd>
<dt>Scissor test optimisation </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a scissor rectangle to limit the effect of point / spot lights when their range does not cover the entire viewport; that means we save fillrate when rendering stencil volumes, especially with distant lights</p>
<p class="enddd"></p>
</dd>
<dt>Z-Pass and Z-Fail algorithms </dt>
<dd><p class="startdd"></p>
<p>The Z-Fail algorithm, often attributed to John Carmack, is used in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to make sure shadows are robust when the camera passes through the shadow volume. However, the Z-Fail algorithm is more expensive than the traditional Z-Pass; so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> detects when Z-Fail is required and only uses it then, Z-Pass is used at all other times.</p>
<p class="enddd"></p>
</dd>
<dt>2-Sided stenciling and stencil wrapping </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports the 2-Sided stenciling / stencil wrapping extensions, which when supported allow volumes to be rendered in a single pass instead of having to do one pass for back facing tris and another for front-facing tris. This doesn’t save fillrate, since the same number of stencil updates are done, but it does save primitive setup and the overhead incurred in the driver every time a render call is made.</p>
<p class="enddd"></p>
</dd>
<dt>Aggressive shadow volume culling </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at detecting which lights could be affecting the frustum, and from that, which objects could be casting a shadow on the frustum. This means we don’t waste time constructing shadow geometry we don’t need. Setting the shadow far distance is another important way you can reduce stencil shadow overhead since it culls far away shadow volumes even if they are visible, which is beneficial in practice since you’re most interested in shadows for close-up objects.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Texture_002dbased-Shadows"></a>
Texture-based Shadows</h1>
<p>Texture shadows involve rendering shadow casters from the point of view of the light into a texture, which is then projected onto shadow receivers. The main advantage of texture shadows as opposed to <a href="#Stencil-Shadows">Stencil Shadows</a> is that the overhead of increasing the geometric detail is far lower, since there is no need to perform per-triangle calculations. Most of the work in rendering texture shadows is done by the graphics card, meaning the technique scales well when taking advantage of the latest cards, which are at present outpacing CPUs in terms of their speed of development. In addition, texture shadows are <b>much</b> more customisable - you can pull them into shaders to apply as you like (particularly with <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a>, you can perform filtering to create softer shadows or perform other special effects on them. Basically, most modern engines use texture shadows as their primary shadow technique simply because they are more powerful, and the increasing speed of GPUs is rapidly amortizing the fillrate / texture access costs of using them.</p>
<p>The main disadvantage to texture shadows is that, because they are simply a texture, they have a fixed resolution which means if stretched, the pixellation of the texture can become obvious. There are ways to combat this though:</p>
<dl compact="compact">
<dt>Choosing a projection basis </dt>
<dd><p class="startdd"></p>
<p>The simplest projection is just to render the shadow casters from the lights perspective using a regular camera setup. This can look bad though, so there are many other projections which can help to improve the quality from the main camera’s perspective. OGRE supports pluggable projection bases via it’s ShadowCameraSetup class, and comes with several existing options - <b>Uniform</b> (which is the simplest), <b>Uniform Focussed</b> (which is still a normal camera projection, except that the camera is focussed into the area that the main viewing camera is looking at), LiSPSM (Light Space Perspective Shadow Mapping - which both focusses and distorts the shadow frustum based on the main view camera) and Plan Optimal (which seeks to optimise the shadow fidelity for a single receiver plane).</p>
<p class="enddd"></p>
</dd>
<dt>Filtering </dt>
<dd><p class="startdd"></p>
<p>You can also sample the shadow texture multiple times rather than once to soften the shadow edges and improve the appearance. Percentage Closest Filtering (PCF) is the most popular approach, although there are multiple variants depending on the number and pattern of the samples you take. Our shadows demo includes a 5-tap PCF example combined with depth shadow mapping.</p>
<p class="enddd"></p>
</dd>
<dt>Using a larger texture </dt>
<dd><p class="startdd"></p>
<p>Again as GPUs get faster and gain more memory, you can scale up to take advantage of this.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If you combine all 3 of these techniques you can get a very high quality shadow solution.</p>
<p>The other issue is with point lights. Because texture shadows require a render to texture in the direction of the light, omnidirectional lights (point lights) would require 6 renders to totally cover all the directions shadows might be cast. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> primarily supports directional lights and spotlights for generating texture shadows; you can use point lights but they will only work if off-camera since they are essentially turned into a spotlight shining into your camera frustum for the purposes of texture shadows.</p>
<p><a class="anchor" id="Directional-Lights"></a></p>
<h2>Directional Lights</h2>
<p>Directional lights in theory shadow the entire scene from an infinitely distant light. Now, since we only have a finite texture which will look very poor quality if stretched over the entire scene, clearly a simplification is required. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> places a shadow texture over the area immediately in front of the camera, and moves it as the camera moves (although it rounds this movement to multiples of texels so that the slight ’swimming shadow’ effect caused by moving the texture is minimised). The range to which this shadow extends, and the offset used to move it in front of the camera, are configurable (See <a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a>). At the far edge of the shadow, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> fades out the shadow based on other configurable parameters so that the termination of the shadow is softened.</p>
<p><a class="anchor" id="Spotlights"></a></p>
<h2>Spotlights</h2>
<p>Spotlights are much easier to represent as renderable shadow textures than directional lights, since they are naturally a frustum. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> represents spotlight directly by rendering the shadow from the light position, in the direction of the light cone; the field-of-view of the texture camera is adjusted based on the spotlight falloff angles. In addition, to hide the fact that the shadow texture is square and has definite edges which could show up outside the spotlight, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a second texture unit when projecting the shadow onto the scene which fades out the shadow gradually in a projected circle around the spotlight.</p>
<p><a class="anchor" id="Point-Lights"></a></p>
<h2>Point Lights</h2>
<p>As mentioned above, to support point lights properly would require multiple renders (either 6 for a cubic render or perhaps 2 for a less precise parabolic mapping), so rather than do that we approximate point lights as spotlights, where the configuration is changed on the fly to make the light shine from its position over the whole of the viewing frustum. This is not an ideal setup since it means it can only really work if the point light’s position is out of view, and in addition the changing parameterisation can cause some ’swimming’ of the texture. Generally we recommend avoiding making point lights cast texture shadows.</p>
<p><a class="anchor" id="Shadow-Casters-and-Shadow-Receivers"></a></p>
<h2>Shadow Casters and Shadow Receivers</h2>
<p>To enable texture shadows, use the shadow technique SHADOWTYPE_TEXTURE_MODULATIVE or SHADOWTYPE_TEXTURE_ADDITIVE; as the name suggests this produces <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> respectively. The cheapest and simplest texture shadow techniques do not use depth information, they merely render casters to a texture and render this onto receivers as plain colour - this means self-shadowing is not possible using these methods. This is the default behaviour if you use the automatic, fixed-function compatible (and thus usable on lower end hardware) texture shadow techniques. You can however use shaders-based techniques through custom shadow materials for casters and receivers to perform more complex shadow algorithms, such as depth shadow mapping which does allow self-shadowing. OGRE comes with an example of this in its shadows demo, although it’s only usable on Shader Model 2 cards or better. Whilst fixed-function depth shadow mapping is available in OpenGL, it was never standardised in Direct3D so using shaders in custom caster &amp; receiver materials is the only portable way to do it. If you use this approach, call SceneManager::setShadowTextureSelfShadow with a parameter of ’true’ to allow texture shadow casters to also be receivers. If you’re not using depth shadow mapping, OGRE divides shadow casters and receivers into 2 disjoint groups. Simply by turning off shadow casting on an object, you automatically make it a shadow receiver (although this can be disabled by setting the ’receive_shadows’ option to ’false’ in a material script. Similarly, if an object is set as a shadow caster, it cannot receive shadows.</p>
<h1><a class="anchor" id="Configuring-Texture-Shadows"></a>
Configuring Texture Shadows</h1>
<p>There are a number of settings which will help you configure your texture-based shadows so that they match your requirements.</p>
<ul>
<li><a href="#Maximum-number-of-shadow-textures">Maximum number of shadow textures</a></li>
<li><a href="#Shadow-texture-size">Shadow texture size</a></li>
<li><a href="#Shadow-far-distance">Shadow far distance</a></li>
<li><a href="#Shadow-texture-offset-Directional-Lights_0029">Shadow texture offset (Directional Lights)</a></li>
<li><a href="#Shadow-fade-settings">Shadow fade settings</a></li>
<li><a href="#Custom-shadow-camera-setups">Custom shadow camera setups</a></li>
<li><a href="#Shadow-texture-Depth-Buffer-sharing">Shadow texture Depth Buffer sharing</a></li>
<li><a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a></li>
</ul>
<p><a class="anchor" id="Maximum-number-of-shadow-textures"></a><a class="anchor" id="Maximum-number-of-shadow-textures-1"></a></p>
<h2>Maximum number of shadow textures</h2>
<p>Shadow textures take up texture memory, and to avoid stalling the rendering pipeline <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> does not reuse the same shadow texture for multiple lights within the same frame. This means that each light which is to cast shadows must have its own shadow texture. In practice, if you have a lot of lights in your scene you would not wish to incur that sort of texture overhead. You can adjust this manually by simply turning off shadow casting for lights you do not wish to cast shadows. In addition, you can set a maximum limit on the number of shadow textures <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is allowed to use by calling SceneManager::setShadowTextureCount. Each frame, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> determines the lights which could be affecting the frustum, and then allocates the number of shadow textures it is allowed to use to the lights on a first-come-first-served basis. Any additional lights will not cast shadows that frame. Note that you can set the number of shadow textures and their size at the same time by using the SceneManager::setShadowTextureSettings method; this is useful because both the individual calls require the potential creation / destruction of texture resources.</p>
<p><a class="anchor" id="Shadow-texture-size"></a><a class="anchor" id="Shadow-texture-size-1"></a></p>
<h2>Shadow texture size</h2>
<p>The size of the textures used for rendering the shadow casters into can be altered; clearly using larger textures will give you better quality shadows, but at the expense of greater memory usage. Changing the texture size is done by calling SceneManager::setShadowTextureSize - textures are assumed to be square and you must specify a texture size that is a power of 2. Be aware that each modulative shadow texture will take size*size*3 bytes of texture memory. <b>Important</b>: if you use the GL render system your shadow texture size can only be larger (in either dimension) than the size of your primary window surface if the hardware supports the Frame Buffer Object (FBO) or Pixel Buffer Object (PBO) extensions. Most modern cards support this now, but be careful of older cards - you can check the ability of the hardware to manage this through ogreRoot-&gt;getRenderSystem()-&gt;getCapabilities()-&gt;hasCapability(RSC_HWRENDER_TO_TEXTURE). If this returns false, if you create a shadow texture larger in any dimension than the primary surface, the rest of the shadow texture will be blank.</p>
<p><a class="anchor" id="Shadow-far-distance"></a><a class="anchor" id="Shadow-far-distance-1"></a></p>
<h2>Shadow far distance</h2>
<p>This determines the distance at which shadows are terminated; it also determines how far into the distance the texture shadows for directional lights are stretched - by reducing this value, or increasing the texture size, you can improve the quality of shadows from directional lights at the expense of closer shadow termination or increased memory usage, respectively.</p>
<p><a class="anchor" id="Shadow-texture-offset-Directional-Lights_0029"></a></p>
<h2>Shadow texture offset (Directional Lights)</h2>
<p>As mentioned above in the directional lights section, the rendering of shadows for directional lights is an approximation that allows us to use a single render to cover a largish area with shadows. This offset parameter affects how far from the camera position the center of the shadow texture is offset, as a proportion of the shadow far distance. The greater this value, the more of the shadow texture is ’useful’ to you since it’s ahead of the camera, but also the further you offset it, the more chance there is of accidentally seeing the edge of the shadow texture at more extreme angles. You change this value by calling SceneManager::setShadowDirLightTextureOffset, the default is 0.6.</p>
<p><a class="anchor" id="Shadow-fade-settings"></a><a class="anchor" id="Shadow-fade-settings-1"></a></p>
<h2>Shadow fade settings</h2>
<p>Shadows fade out before the shadow far distance so that the termination of shadow is not abrupt. You can configure the start and end points of this fade by calling the SceneManager::setShadowTextureFadeStart and SceneManager::setShadowTextureFadeEnd methods, both take distances as a proportion of the shadow far distance. Because of the inaccuracies caused by using a square texture and a radial fade distance, you cannot use 1.0 as the fade end, if you do you’ll see artifacts at the extreme edges. The default values are 0.7 and 0.9, which serve most purposes but you can change them if you like.</p>
<h1>Texture shadows and vertex / fragment programs</h1>
<p>When rendering shadow casters into a modulative shadow texture, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> turns off all textures, and all lighting contributions except for ambient light, which it sets to the colour of the shadow (<a href="#Shadow-Colour">Shadow Colour</a>). For additive shadows, it render the casters into a black &amp; white texture instead. This is enough to render shadow casters for fixed-function material techniques, however where a vertex program is used <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> doesn’t have so much control. If you use a vertex program in the <b>first pass</b> of your technique, then you must also tell ogre which vertex program you want it to use when rendering the shadow caster; see <a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a> for full details.</p>
<p><a class="anchor" id="Custom-shadow-camera-setups"></a><a class="anchor" id="Custom-shadow-camera-setups-1"></a></p>
<h2>Custom shadow camera setups</h2>
<p>As previously mentioned, one of the downsides of texture shadows is that the texture resolution is finite, and it’s possible to get aliasing when the size of the shadow texel is larger than a screen pixel, due to the projection of the texture. In order to address this, you can specify alternative projection bases by using or creating subclasses of the ShadowCameraSetup class. The default version is called DefaultShadowCameraSetup and this sets up a simple regular frustum for point and spotlights, and an orthographic frustum for directional lights. There is also a PlaneOptimalShadowCameraSetup class which specialises the projection to a plane, thus giving you much better definition provided your shadow receivers exist mostly in a single plane. Other setup classes (e.g. you might create a perspective or trapezoid shadow mapping version) can be created and plugged in at runtime, either on individual lights or on the SceneManager as a whole.</p>
<p><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing"></a><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing-1"></a></p>
<h2>Shadow texture Depth Buffer sharing</h2>
<p>Shadow textures need a depth buffer like many other RTs (Render Textures). Prior to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8, the depth buffer behavior was left undefined leaving a very small possibility of causing inconsistencies across different window resolutions and render systems. Depending on the render window’s resolutions and/or rendersystem being used, the depth buffer might been shared with the render window or a new one could get created to suite the shadow textures. If the application was depending on the depth buffer contents from the previous scene render (that is, no clear was performed) where a shadow texture render pass was in the middle; then the depth buffer would’ve contained garbage (but not consistent on all machines) causing graphical glitches hard to spot.</p>
<p>From <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8 onwards the depth buffer usage &amp; sharing can be flexible controlled through the use of depth pool IDs. These pool IDs are not specifically part of shadow textures, but rather anything involving RTs. All RTs with the same pool ID share the same depth buffers when possible (following RenderSystem API rules, check RenderSystemCapabilities flags to find out what the behavior will be). The default ID for shadow textures is 1; which is the same default value for render windows, and RTTs; thus maintaining the same behavior with older applications while achieving maximum memory saving and performance efficiency because the number of created depth buffers is as lowest as possible.</p>
<p>However there are some reasons to put shadow textures in a separate pool. This holds specially true if the application depends on the previous contents from the depth buffer before the shadow pass, instead of doing a clear:</p>
<ul>
<li>In Direct3D9, the shadow texture is more likely to share the depth buffer with the render window at high resolutions (when the window is bigger than the shadow texture resolution), but at low resolutions it won’t be shared, thus causing two different behaviors. Also probably the shadow texture will share the depth buffers with most other RTTs (i.e. compositors)</li>
<li>In OpenGL 2.1, the shadow texture can’t be shared with the main render window; and most likely will NOT be shared with many other RTTs (i.e. compositors) since OGL 2.1 has a requirement that texture resolutions should exactly match, while D3D9 specifies depth buffers can be shared as long as the resolutions are equal or less.</li>
</ul>
<p>For example, the DeferredShading sample suffers from this problem. If this is a problem for a particular effect you’re trying to achieve, you can specify a custom pool ID so that shadow textures get their own depth buffer(s), ensuring they aren’t shared with other RTs. You can set the poolId parameter from either SceneManager::setShadowTextureSettings or setShadowTextureConfig</p>
<div class="fragment"><div class="line">mSceneMgr-&gt;setShadowTextureSettings( size, count, format, PoolId );</div><div class="line">mSceneMgr-&gt;setShadowTextureConfig( 0, 512, 512, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39cab63dbcddd53d1f766d742fd557c26be4">PF_FLOAT16_R</a>, 50 );</div></div><!-- fragment --><p>Note a poolId of 0 will make the shadow textures not to use a depth buffer, which isn’t usually a desired behavior.</p>
<p><a class="anchor" id="Integrated-Texture-Shadows"></a><a class="anchor" id="Integrated-Texture-Shadows-1"></a></p>
<h2>Integrated Texture Shadows</h2>
<p>Texture shadows have one major advantage over stencil shadows - the data used to represent them can be referenced in regular shaders. Whilst the default texture shadow modes (SHADOWTYPE_TEXTURE_MODULATIVE and SHADOWTYPE_TEXTURE_ADDITIVE) automatically render shadows for you, their disadvantage is that because they are generalised add-ons to your own materials, they tend to take more passes of the scene to use. In addition, you don’t have a lot of control over the composition of the shadows.</p>
<p>Here is where ’integrated’ texture shadows step in. Both of the texture shadow types above have alternative versions called SHADOWTYPE_TEXTURE_MODULATIVE_INTEGRATED and SHADOWTYPE_TEXTURE_ADDITIVE_INTEGRATED, where instead of rendering the shadows for you, it just creates the texture shadow and then expects you to use that shadow texture as you see fit when rendering receiver objects in the scene. The downside is that you have to take into account shadow receipt in every one of your materials if you use this option - the upside is that you have total control over how the shadow textures are used. The big advantage here is that you can can perform more complex shading, taking into account shadowing, than is possible using the generalised bolt-on approaches, AND you can probably write them in a smaller number of passes, since you know precisely what you need and can combine passes where possible. When you use one of these shadowing approaches, the only difference between additive and modulative is the colour of the casters in the shadow texture (the shadow colour for modulative, black for additive) - the actual calculation of how the texture affects the receivers is of course up to you. No separate modulative pass will be performed, and no splitting of your materials into ambient / per-light / decal etc will occur - absolutely everything is determined by your original material (which may have modulative passes or per-light iteration if you want of course, but it’s not required).</p>
<p>You reference a shadow texture in a material which implements this approach by using the ’<a href="#content_005ftype">content_type</a> shadow’ directive in your <a href="#Texture-Units">texture_unit</a>. It implicitly references a shadow texture based on the number of times you’ve used this directive in the same pass, and the light_start option or light-based pass iteration, which might start the light index higher than 0.</p>
<h1><a class="anchor" id="Modulative-Shadows"></a>
Modulative Shadows</h1>
<p>Modulative shadows work by darkening an already rendered scene with a fixed colour. First, the scene is rendered normally containing all the objects which will be shadowed, then a modulative pass is done per light, which darkens areas in shadow. Finally, objects which do not receive shadows are rendered.</p>
<p>There are 2 modulative shadow techniques; stencil-based (See <a href="#Stencil-Shadows">Stencil Shadows</a> : SHADOWTYPE_STENCIL_MODULATIVE) and texture-based (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a> : SHADOWTYPE_TEXTURE_MODULATIVE). Modulative shadows are an inaccurate lighting model, since they darken the areas of shadow uniformly, irrespective of the amount of light which would have fallen on the shadow area anyway. However, they can give fairly attractive results for a much lower overhead than more ’correct’ methods like <a href="#Additive-Light-Masking">Additive Light Masking</a>, and they also combine well with pre-baked static lighting (such as pre-calculated lightmaps), which additive lighting does not. The main thing to consider is that using multiple light sources can result in overly dark shadows (where shadows overlap, which intuitively looks right in fact, but it’s not physically correct) and artifacts when using stencil shadows (See <a href="#The-Silhouette-Edge">The Silhouette Edge</a>).</p>
<p><a class="anchor" id="Shadow-Colour"></a><a class="anchor" id="Shadow-Colour-1"></a></p>
<h2>Shadow Colour</h2>
<p>The colour which is used to darken the areas in shadow is set by SceneManager::setShadowColour; it defaults to a dark grey (so that the underlying colour still shows through a bit).</p>
<p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to have a separate pass of the scene to render shadows. In this case the ’modulative’ aspect of the shadow technique just affects the colour of the shadow texture.</p>
<h1><a class="anchor" id="Additive-Light-Masking"></a>
Additive Light Masking</h1>
<p>Additive light masking is about rendering the scene many times, each time representing a single light contribution whose influence is masked out in areas of shadow. Each pass is combined with (added to) the previous one such that when all the passes are complete, all the light contribution has correctly accumulated in the scene, and each light has been prevented from affecting areas which it should not be able to because of shadow casters. This is an effective technique which results in very realistic looking lighting, but it comes at a price: more rendering passes.</p>
<p>As many technical papers (and game marketing) will tell you, rendering realistic lighting like this requires multiple passes. Being a friendly sort of engine, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> frees you from most of the hard work though, and will let you use the exact same material definitions whether you use this lighting technique or not (for the most part, see <a href="#Pass-Classification-and-Vertex-Programs">Pass Classification and Vertex Programs</a>). In order to do this technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically categorises the <a href="#Passes">Passes</a> you define in your materials into 3 types:</p>
<ol type="1">
<li>ambient Passes categorised as ’ambient’ include any base pass which is not lit by any particular light, i.e. it occurs even if there is no ambient light in the scene. The ambient pass always happens first, and sets up the initial depth value of the fragments, and the ambient colour if applicable. It also includes any emissive / self illumination contribution. Only textures which affect ambient light (e.g. ambient occlusion maps) should be rendered in this pass.</li>
<li>diffuse/specular Passes categorised as ’diffuse/specular’ (or ’per-light’) are rendered once per light, and each pass contributes the diffuse and specular colour from that single light as reflected by the diffuse / specular terms in the pass. Areas in shadow from that light are masked and are thus not updated. The resulting masked colour is added to the existing colour in the scene. Again, no textures are used in this pass (except for textures used for lighting calculations such as normal maps).</li>
<li>decal Passes categorised as ’decal’ add the final texture colour to the scene, which is modulated by the accumulated light built up from all the ambient and diffuse/specular passes.</li>
</ol>
<p>In practice, <a href="#Passes">Passes</a> rarely fall nicely into just one of these categories. For each Technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> compiles a list of ’Illumination Passes’, which are derived from the user defined passes, but can be split, to ensure that the divisions between illumination pass categories can be maintained. For example, if we take a very simple material definition:</p>
<div class="fragment"><div class="line">material TestIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will split this into 3 illumination passes, which will be the equivalent of this:</p>
<div class="fragment"><div class="line">material TestIlluminationSplitIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 0 0 0</div><div class="line">            specular 0 0 0</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Diffuse / specular pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend add</div><div class="line">            iteration once_per_light</div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend modulate</div><div class="line">            lighting off</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>So as you can see, even a simple material requires a minimum of 3 passes when using this shadow technique, and in fact it requires (num_lights + 2) passes in the general sense. You can use more passes in your original material and <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will cope with that too, but be aware that each pass may turn into multiple ones if it uses more than one type of light contribution (ambient vs diffuse/specular) and / or has texture units. The main nice thing is that you get the full multipass lighting behaviour even if you don’t define your materials in terms of it, meaning that your material definitions can remain the same no matter what lighting approach you decide to use.</p>
<p><a class="anchor" id="Manually-Categorising-Illumination-Passes"></a><a class="anchor" id="Manually-Categorising-Illumination-Passes-1"></a></p>
<h2>Manually Categorising Illumination Passes</h2>
<p>Alternatively, if you want more direct control over the categorisation of your passes, you can use the <a href="#illumination_005fstage">illumination_stage</a> option in your pass to explicitly assign a pass unchanged to an illumination stage. This way you can make sure you know precisely how your material will be rendered under additive lighting conditions.</p>
<p><a class="anchor" id="Pass-Classification-and-Vertex-Programs"></a><a class="anchor" id="Pass-Classification-and-Vertex-Programs-1"></a></p>
<h2>Pass Classification and Vertex Programs</h2>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at classifying and splitting your passes to ensure that the multipass rendering approach required by additive lighting works correctly without you having to change your material definitions. However, there is one exception; when you use vertex programs, the normal lighting attributes ambient, diffuse, specular etc are not used, because all of that is determined by the vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has no way of knowing what you’re doing inside that vertex program, so you have to tell it.</p>
<p>In practice this is very easy. Even though your vertex program could be doing a lot of complex, highly customised processing, it can still be classified into one of the 3 types listed above. All you need to do to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> what you’re doing is to use the pass attributes ambient, diffuse, specular and self_illumination, just as if you were not using a vertex program. Sure, these attributes do nothing (as far as rendering is concerned) when you’re using vertex programs, but it’s the easiest way to indicate to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> which light components you’re using in your vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will then classify and potentially split your programmable pass based on this information - it will leave the vertex program as-is (so that any split passes will respect any vertex modification that is being done).</p>
<p>Note that when classifying a diffuse/specular programmable pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> checks to see whether you have indicated the pass can be run once per light (iteration once_per_light). If so, the pass is left intact, including it’s vertex and fragment programs. However, if this attribute is not included in the pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> tries to split off the per-light part, and in doing so it will disable the fragment program, since in the absence of the ’iteration once_per_light’ attribute it can only assume that the fragment program is performing decal work and hence must not be used per light.</p>
<p>So clearly, when you use additive light masking as a shadow technique, you need to make sure that programmable passes you use are properly set up so that they can be classified correctly. However, also note that the changes you have to make to ensure the classification is correct does not affect the way the material renders when you choose not to use additive lighting, so the principle that you should be able to use the same material definitions for all lighting scenarios still holds. Here is an example of a programmable material which will be classified correctly by the illumination pass classifier:</p>
<div class="fragment"><div class="line"><span class="comment">// Per-pixel normal mapping Any number of lights, diffuse and specular</span></div><div class="line">material Examples/BumpMapping/MultiLightSpecular</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Base ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// ambient only, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 1 1 1</div><div class="line">            diffuse 0 0 0 </div><div class="line">            specular 0 0 0 0</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named_auto ambient ambient_light_colour</div><div class="line">            }</div><div class="line">        }</div><div class="line">        <span class="comment">// Now do the lighting pass</span></div><div class="line">        <span class="comment">// NB we don&#39;t do decal texture here because this is repeated per light</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// set ambient off, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 0 0 0 </div><div class="line">            <span class="comment">// do this for each light</span></div><div class="line">            iteration once_per_light</div><div class="line">            scene_blend add</div><div class="line"></div><div class="line">            <span class="comment">// Vertex program reference</span></div><div class="line">            vertex_program_ref Examples/BumpMapVPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightPosition light_position_object_space 0</div><div class="line">                param_named_auto eyePosition camera_position_object_space</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Fragment program</span></div><div class="line">            fragment_program_ref Examples/BumpMapFPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Base bump map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture NMBumpsOut.png</div><div class="line">                colour_op replace</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map #2</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            lighting off</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named ambient float4 1 1 1 1</div><div class="line">            }</div><div class="line">            scene_blend dest_colour zero</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture RustedMetal.jpg </div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to use this explicit sequence - allowing you to compress the number of passes into a much smaller number at the expense of defining an upper number of shadow casting lights. In this case the ’additive’ aspect of the shadow technique just affects the colour of the shadow texture and it’s up to you to combine the shadow textures in your receivers however you like.</p>
<p><a class="anchor" id="Static-Lighting"></a></p>
<h2>Static Lighting</h2>
<p>Despite their power, additive lighting techniques have an additional limitation; they do not combine well with pre-calculated static lighting in the scene. This is because they are based on the principle that shadow is an absence of light, but since static lighting in the scene already includes areas of light and shadow, additive lighting cannot remove light to create new shadows. Therefore, if you use the additive lighting technique you must either use it exclusively as your lighting solution (and you can combine it with per-pixel lighting to create a very impressive dynamic lighting solution), or you must use <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> to combine the static lighting according to your chosen approach. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Shadows</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_shadows.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Shadows </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Enabling-shadows">Enabling shadows</a></li>
<li class="level1"><a href="#Opting-out-of-shadows">Opting out of shadows</a></li>
<li class="level1"><a href="#Stencil-Shadows">Stencil Shadows</a></li>
<li class="level1"><a href="#Texture_002dbased-Shadows">Texture-based Shadows</a></li>
<li class="level1"><a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a></li>
<li class="level1"><a href="#Modulative-Shadows">Modulative Shadows</a></li>
<li class="level1"><a href="#Additive-Light-Masking">Additive Light Masking</a></li>
</ul>
</div>
<div class="textblock"><p>Shadows are clearly an important part of rendering a believable scene - they provide a more tangible feel to the objects in the scene, and aid the viewer in understanding the spatial relationship between objects. Unfortunately, shadows are also one of the most challenging aspects of 3D rendering, and they are still very much an active area of research. Whilst there are many techniques to render shadows, none is perfect and they all come with advantages and disadvantages. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides multiple shadow implementations, with plenty of configuration settings, so you can choose which technique is most appropriate for your scene.</p>
<p>Shadow implementations fall into basically 2 broad categories: <a href="#Stencil-Shadows">Stencil Shadows</a> and <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>. This describes the method by which the shape of the shadow is generated. In addition, there is more than one way to render the shadow into the scene: <a href="#Modulative-Shadows">Modulative Shadows</a>, which darkens the scene in areas of shadow, and <a href="#Additive-Light-Masking">Additive Light Masking</a> which by contrast builds up light contribution in areas which are not in shadow. You also have the option of <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> which gives you complete control over texture shadow application, allowing for complex single-pass shadowing shaders. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports all these combinations.</p>
<h1><a class="anchor" id="Enabling-shadows"></a>
Enabling shadows</h1>
<p>Shadows are disabled by default, here’s how you turn them on and configure them in the general sense:</p>
<ol type="1">
<li><p class="startli">Enable a shadow technique on the SceneManager as the <b>first</b> thing you doing your scene setup. It is important that this is done first because the shadow technique can alter the way meshes are loaded. Here’s an example:</p>
<p class="startli"><code>mSceneMgr-&gt;setShadowTechnique(SHADOWTYPE_STENCIL_ADDITIVE);</code></p>
</li>
<li>Create one or more lights. Note that not all light types are necessarily supported by all shadow techniques, you should check the sections about each technique to check. Note that if certain lights should not cast shadows, you can turn that off by calling setCastShadows(false) on the light, the default is true.</li>
<li>Disable shadow casting on objects which should not cast shadows. Call setCastShadows(false) on objects you don’t want to cast shadows, the default for all objects is to cast shadows.</li>
<li>Configure shadow far distance. You can limit the distance at which shadows are considered for performance reasons, by calling SceneManager::setShadowFarDistance.</li>
<li>Turn off the receipt of shadows on materials that should not receive them. You can turn off the receipt of shadows (note, not the casting of shadows - that is done per-object) by calling Material::setReceiveShadows or using the receive_shadows material attribute. This is useful for materials which should be considered self-illuminated for example. Note that transparent materials are typically excluded from receiving and casting shadows, although see the <a href="#transparency_005fcasts_005fshadows">transparency_casts_shadows</a> option for exceptions.</li>
</ol>
<h1><a class="anchor" id="Opting-out-of-shadows"></a>
Opting out of shadows</h1>
<p>By default <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> treats all non-transparent objects as shadow casters and receivers (depending on the shadow technique they may not be able to be both at once, check the docs for your chosen technique first). You can disable shadows in various ways:</p>
<dl compact="compact">
<dt>Turning off shadow casting on the light </dt>
<dd><p class="startdd"></p>
<p>Calling Light::setCastsShadows(false) will mean this light casts no shadows at all.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow receipt on a material </dt>
<dd><p class="startdd"></p>
<p>Calling Material::setReceiveShadows(false) will prevent any objects using this material from receiving shadows.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow casting on individual objects </dt>
<dd><p class="startdd"></p>
<p>Calling MovableObject::setCastsShadows(false) will disable shadow casting for this object.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadows on an entire rendering queue group </dt>
<dd><p class="startdd"></p>
<p>Calling RenderQueueGroup::setShadowsEnabled(false) will turn off both shadow casting and receiving on an entire rendering queue group. This is useful because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has to do light setup tasks per group in order to preserve the inter-group ordering. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically disables shadows on a number of groups automatically, such as RENDER_QUEUE_BACKGROUND, RENDER_QUEUE_OVERLAY, RENDER_QUEUE_SKIES_EARLY and RENDER_QUEUE_SKIES_LATE. If you choose to use more rendering queues (and by default, you won’t be using any more than this plus the ’standard’ queue, so ignore this if you don’t know what it means!), be aware that each one can incur a light setup cost, and you should disable shadows on the additional ones you use if you can.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Stencil-Shadows"></a>
Stencil Shadows</h1>
<p>Stencil shadows are a method by which a ’mask’ is created for the screen using a feature called the stencil buffer. This mask can be used to exclude areas of the screen from subsequent renders, and thus it can be used to either include or exclude areas in shadow. They are enabled by calling SceneManager::setShadowTechnique with a parameter of either <code>SHADOWTYPE_STENCIL_ADDITIVE</code> or <code>SHADOWTYPE_STENCIL_MODULATIVE</code>. Because the stencil can only mask areas to be either ’enabled’ or ’disabled’, stencil shadows have ’hard’ edges, that is to say clear dividing lines between light and shadow - it is not possible to soften these edges.</p>
<p>In order to generate the stencil, ’shadow volumes’ are rendered by extruding the silhouette of the shadow caster away from the light. Where these shadow volumes intersect other objects (or the caster, since self-shadowing is supported using this technique), the stencil is updated, allowing subsequent operations to differentiate between light and shadow. How exactly this is used to render the shadows depends on whether <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> is being used. Objects can both cast and receive stencil shadows, so self-shadowing is inbuilt.</p>
<p>The advantage of stencil shadows is that they can do self-shadowing simply on low-end hardware, provided you keep your poly count under control. In contrast doing self-shadowing with texture shadows requires a fairly modern machine (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>). For this reason, you’re likely to pick stencil shadows if you need an accurate shadowing solution for an application aimed at older or lower-spec machines.</p>
<p>The disadvantages of stencil shadows are numerous though, especially on more modern hardware. Because stencil shadows are a geometric technique, they are inherently more costly the higher the number of polygons you use, meaning you are penalized the more detailed you make your meshes. The fillrate cost, which comes from having to render shadow volumes, also escalates the same way. Since more modern applications are likely to use higher polygon counts, stencil shadows can start to become a bottleneck. In addition, the visual aspects of stencil shadows are pretty primitive - your shadows will always be hard-edged, and you have no possibility of doing clever things with shaders since the stencil is not available for manipulation there. Therefore, if your application is aimed at higher-end machines you should definitely consider switching to texture shadows (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>).</p>
<p>There are a number of issues to consider which are specific to stencil shadows:</p>
<ul>
<li><a href="#CPU-Overhead">CPU Overhead</a></li>
<li><a href="#Extrusion-distance">Extrusion distance</a></li>
<li><a href="#Camera-far-plane-positioning">Camera far plane positioning</a></li>
<li><a href="#Mesh-edge-lists">Mesh edge lists</a></li>
<li><a href="#The-Silhouette-Edge">The Silhouette Edge</a></li>
<li><a href="#Be-realistic">Be realistic</a></li>
<li><a href="#Stencil-Optimisations-Performed-By-Ogre">Stencil Optimisations Performed By Ogre</a></li>
</ul>
<p><a class="anchor" id="CPU-Overhead"></a><a class="anchor" id="CPU-Overhead-1"></a></p>
<h2>CPU Overhead</h2>
<p>Calculating the shadow volume for a mesh can be expensive, and it has to be done on the CPU, it is not a hardware accelerated feature. Therefore, you can find that if you overuse this feature, you can create a CPU bottleneck for your application. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> quite aggressively eliminates objects which cannot be casting shadows on the frustum, but there are limits to how much it can do, and large, elongated shadows (e.g. representing a very low sun position) are very difficult to cull efficiently. Try to avoid having too many shadow casters around at once, and avoid long shadows if you can. Also, make use of the ’shadow far distance’ parameter on the SceneManager, this can eliminate distant shadow casters from the shadow volume construction and save you some time, at the expense of only having shadows for closer objects. Lastly, make use of Ogre’s Level-Of-Detail (LOD) features; you can generate automatically calculated LODs for your meshes in code (see the Mesh API docs) or when using the mesh tools such as OgreXMLConverter and OgreMeshUpgrader. Alternatively, you can assign your own manual LODs by providing alternative mesh files at lower detail levels. Both methods will cause the shadow volume complexity to decrease as the object gets further away, which saves you valuable volume calculation time.</p>
<p><a class="anchor" id="Extrusion-distance"></a><a class="anchor" id="Extrusion-distance-1"></a></p>
<h2>Extrusion distance</h2>
<p>When vertex programs are not available, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only extrude shadow volumes a finite distance from the object. If an object gets too close to a light, any finite extrusion distance will be inadequate to guarantee all objects will be shadowed properly by this object. Therefore, you are advised not to let shadow casters pass too close to light sources if you can avoid it, unless you can guarantee that your target audience will have vertex program capable hardware (in this case, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> extrudes the volume to infinity using a vertex program so the problem does not occur). When infinite extrusion is not possible, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses finite extrusion, either derived from the attenuation range of a light (in the case of a point light or spotlight), or a fixed extrusion distance set in the application in the case of directional lights. To change the directional light extrusion distance, use SceneManager::setShadowDirectionalLightExtrusionDistance.</p>
<p><a class="anchor" id="Camera-far-plane-positioning"></a><a class="anchor" id="Camera-far-plane-positioning-1"></a></p>
<h2>Camera far plane positioning</h2>
<p>Stencil shadow volumes rely very much on not being clipped by the far plane. When you enable stencil shadows, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> internally changes the far plane settings of your cameras such that there is no far plane - i.e. it is placed at infinity (Camera::setFarClipDistance(0)). This avoids artifacts caused by clipping the dark caps on shadow volumes, at the expense of a (very) small amount of depth precision.</p>
<p><a class="anchor" id="Mesh-edge-lists"></a><a class="anchor" id="Mesh-edge-lists-1"></a></p>
<h2>Mesh edge lists</h2>
<p>Stencil shadows can only be calculated when an ’edge list’ has been built for all the geometry in a mesh. The official exporters and tools automatically build this for you (or have an option to do so), but if you create your own meshes, you must remember to build edge lists for them before using them with stencil shadows - you can do that by using OgreMeshUpgrade or OgreXmlConverter, or by calling Mesh::buildEdgeList before you export or use the mesh. If a mesh doesn’t have edge lists, OGRE assumes that it is not supposed to cast stencil shadows.</p>
<p><a class="anchor" id="The-Silhouette-Edge"></a><a class="anchor" id="The-Silhouette-Edge-1"></a></p>
<h2>The Silhouette Edge</h2>
<p>Stencil shadowing is about finding a silhouette of the mesh, and projecting it away to form a volume. What this means is that there is a definite boundary on the shadow caster between light and shadow; a set of edges where where the triangle on one side is facing toward the light, and one is facing away. This produces a sharp edge around the mesh as the transition occurs. Provided there is little or no other light in the scene, and the mesh has smooth normals to produce a gradual light change in its underlying shading, the silhouette edge can be hidden - this works better the higher the tessellation of the mesh. However, if the scene includes ambient light, then the difference is far more marked. This is especially true when using <a href="#Modulative-Shadows">Modulative Shadows</a>, because the light contribution of each shadowed area is not taken into account by this simplified approach, and so using 2 or more lights in a scene using modulative stencil shadows is not advisable; the silhouette edges will be very marked. Additive lights do not suffer from this as badly because each light is masked individually, meaning that it is only ambient light which can show up the silhouette edges.</p>
<p><a class="anchor" id="Be-realistic"></a><a class="anchor" id="Be-realistic-1"></a></p>
<h2>Be realistic</h2>
<p>Don’t expect to be able to throw any scene using any hardware at the stencil shadow algorithm and expect to get perfect, optimum speed results. Shadows are a complex and expensive technique, so you should impose some reasonable limitations on your placing of lights and objects; they’re not really that restricting, but you should be aware that this is not a complete free-for-all.</p>
<ul>
<li>Try to avoid letting objects pass very close (or even through) lights - it might look nice but it’s one of the cases where artifacts can occur on machines not capable of running vertex programs.</li>
<li>Be aware that shadow volumes do not respect the ’solidity’ of the objects they pass through, and if those objects do not themselves cast shadows (which would hide the effect) then the result will be that you can see shadows on the other side of what should be an occluding object.</li>
<li>Make use of SceneManager::setShadowFarDistance to limit the number of shadow volumes constructed</li>
<li>Make use of LOD to reduce shadow volume complexity at distance</li>
<li><p class="startli">Avoid very long (dusk and dawn) shadows - they exacerbate other issues such as volume clipping, fillrate, and cause many more objects at a greater distance to require volume construction.</p>
<p class="startli"><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre"></a><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre-1"></a></p>
</li>
</ul>
<h2>Stencil Optimisations Performed By <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a></h2>
<p>Despite all that, stencil shadows can look very nice (especially with <a href="#Additive-Light-Masking">Additive Light Masking</a>) and can be fast if you respect the rules above. In addition, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> comes pre-packed with a lot of optimisations which help to make this as quick as possible. This section is more for developers or people interested in knowing something about the ’under the hood’ behaviour of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl compact="compact">
<dt>Vertex program extrusion </dt>
<dd><p class="startdd"></p>
<p>As previously mentioned, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> performs the extrusion of shadow volumes in hardware on vertex program-capable hardware (e.g. GeForce3, Radeon 8500 or better). This has 2 major benefits; the obvious one being speed, but secondly that vertex programs can extrude points to infinity, which the fixed-function pipeline cannot, at least not without performing all calculations in software. This leads to more robust volumes, and also eliminates more than half the volume triangles on directional lights since all points are projected to a single point at infinity.</p>
<p class="enddd"></p>
</dd>
<dt>Scissor test optimisation </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a scissor rectangle to limit the effect of point / spot lights when their range does not cover the entire viewport; that means we save fillrate when rendering stencil volumes, especially with distant lights</p>
<p class="enddd"></p>
</dd>
<dt>Z-Pass and Z-Fail algorithms </dt>
<dd><p class="startdd"></p>
<p>The Z-Fail algorithm, often attributed to John Carmack, is used in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to make sure shadows are robust when the camera passes through the shadow volume. However, the Z-Fail algorithm is more expensive than the traditional Z-Pass; so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> detects when Z-Fail is required and only uses it then, Z-Pass is used at all other times.</p>
<p class="enddd"></p>
</dd>
<dt>2-Sided stenciling and stencil wrapping </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports the 2-Sided stenciling / stencil wrapping extensions, which when supported allow volumes to be rendered in a single pass instead of having to do one pass for back facing tris and another for front-facing tris. This doesn’t save fillrate, since the same number of stencil updates are done, but it does save primitive setup and the overhead incurred in the driver every time a render call is made.</p>
<p class="enddd"></p>
</dd>
<dt>Aggressive shadow volume culling </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at detecting which lights could be affecting the frustum, and from that, which objects could be casting a shadow on the frustum. This means we don’t waste time constructing shadow geometry we don’t need. Setting the shadow far distance is another important way you can reduce stencil shadow overhead since it culls far away shadow volumes even if they are visible, which is beneficial in practice since you’re most interested in shadows for close-up objects.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Texture_002dbased-Shadows"></a>
Texture-based Shadows</h1>
<p>Texture shadows involve rendering shadow casters from the point of view of the light into a texture, which is then projected onto shadow receivers. The main advantage of texture shadows as opposed to <a href="#Stencil-Shadows">Stencil Shadows</a> is that the overhead of increasing the geometric detail is far lower, since there is no need to perform per-triangle calculations. Most of the work in rendering texture shadows is done by the graphics card, meaning the technique scales well when taking advantage of the latest cards, which are at present outpacing CPUs in terms of their speed of development. In addition, texture shadows are <b>much</b> more customisable - you can pull them into shaders to apply as you like (particularly with <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a>, you can perform filtering to create softer shadows or perform other special effects on them. Basically, most modern engines use texture shadows as their primary shadow technique simply because they are more powerful, and the increasing speed of GPUs is rapidly amortizing the fillrate / texture access costs of using them.</p>
<p>The main disadvantage to texture shadows is that, because they are simply a texture, they have a fixed resolution which means if stretched, the pixellation of the texture can become obvious. There are ways to combat this though:</p>
<dl compact="compact">
<dt>Choosing a projection basis </dt>
<dd><p class="startdd"></p>
<p>The simplest projection is just to render the shadow casters from the lights perspective using a regular camera setup. This can look bad though, so there are many other projections which can help to improve the quality from the main camera’s perspective. OGRE supports pluggable projection bases via it’s ShadowCameraSetup class, and comes with several existing options - <b>Uniform</b> (which is the simplest), <b>Uniform Focussed</b> (which is still a normal camera projection, except that the camera is focussed into the area that the main viewing camera is looking at), LiSPSM (Light Space Perspective Shadow Mapping - which both focusses and distorts the shadow frustum based on the main view camera) and Plan Optimal (which seeks to optimise the shadow fidelity for a single receiver plane).</p>
<p class="enddd"></p>
</dd>
<dt>Filtering </dt>
<dd><p class="startdd"></p>
<p>You can also sample the shadow texture multiple times rather than once to soften the shadow edges and improve the appearance. Percentage Closest Filtering (PCF) is the most popular approach, although there are multiple variants depending on the number and pattern of the samples you take. Our shadows demo includes a 5-tap PCF example combined with depth shadow mapping.</p>
<p class="enddd"></p>
</dd>
<dt>Using a larger texture </dt>
<dd><p class="startdd"></p>
<p>Again as GPUs get faster and gain more memory, you can scale up to take advantage of this.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If you combine all 3 of these techniques you can get a very high quality shadow solution.</p>
<p>The other issue is with point lights. Because texture shadows require a render to texture in the direction of the light, omnidirectional lights (point lights) would require 6 renders to totally cover all the directions shadows might be cast. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> primarily supports directional lights and spotlights for generating texture shadows; you can use point lights but they will only work if off-camera since they are essentially turned into a spotlight shining into your camera frustum for the purposes of texture shadows.</p>
<p><a class="anchor" id="Directional-Lights"></a></p>
<h2>Directional Lights</h2>
<p>Directional lights in theory shadow the entire scene from an infinitely distant light. Now, since we only have a finite texture which will look very poor quality if stretched over the entire scene, clearly a simplification is required. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> places a shadow texture over the area immediately in front of the camera, and moves it as the camera moves (although it rounds this movement to multiples of texels so that the slight ’swimming shadow’ effect caused by moving the texture is minimised). The range to which this shadow extends, and the offset used to move it in front of the camera, are configurable (See <a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a>). At the far edge of the shadow, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> fades out the shadow based on other configurable parameters so that the termination of the shadow is softened.</p>
<p><a class="anchor" id="Spotlights"></a></p>
<h2>Spotlights</h2>
<p>Spotlights are much easier to represent as renderable shadow textures than directional lights, since they are naturally a frustum. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> represents spotlight directly by rendering the shadow from the light position, in the direction of the light cone; the field-of-view of the texture camera is adjusted based on the spotlight falloff angles. In addition, to hide the fact that the shadow texture is square and has definite edges which could show up outside the spotlight, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a second texture unit when projecting the shadow onto the scene which fades out the shadow gradually in a projected circle around the spotlight.</p>
<p><a class="anchor" id="Point-Lights"></a></p>
<h2>Point Lights</h2>
<p>As mentioned above, to support point lights properly would require multiple renders (either 6 for a cubic render or perhaps 2 for a less precise parabolic mapping), so rather than do that we approximate point lights as spotlights, where the configuration is changed on the fly to make the light shine from its position over the whole of the viewing frustum. This is not an ideal setup since it means it can only really work if the point light’s position is out of view, and in addition the changing parameterisation can cause some ’swimming’ of the texture. Generally we recommend avoiding making point lights cast texture shadows.</p>
<p><a class="anchor" id="Shadow-Casters-and-Shadow-Receivers"></a></p>
<h2>Shadow Casters and Shadow Receivers</h2>
<p>To enable texture shadows, use the shadow technique SHADOWTYPE_TEXTURE_MODULATIVE or SHADOWTYPE_TEXTURE_ADDITIVE; as the name suggests this produces <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> respectively. The cheapest and simplest texture shadow techniques do not use depth information, they merely render casters to a texture and render this onto receivers as plain colour - this means self-shadowing is not possible using these methods. This is the default behaviour if you use the automatic, fixed-function compatible (and thus usable on lower end hardware) texture shadow techniques. You can however use shaders-based techniques through custom shadow materials for casters and receivers to perform more complex shadow algorithms, such as depth shadow mapping which does allow self-shadowing. OGRE comes with an example of this in its shadows demo, although it’s only usable on Shader Model 2 cards or better. Whilst fixed-function depth shadow mapping is available in OpenGL, it was never standardised in Direct3D so using shaders in custom caster &amp; receiver materials is the only portable way to do it. If you use this approach, call SceneManager::setShadowTextureSelfShadow with a parameter of ’true’ to allow texture shadow casters to also be receivers. If you’re not using depth shadow mapping, OGRE divides shadow casters and receivers into 2 disjoint groups. Simply by turning off shadow casting on an object, you automatically make it a shadow receiver (although this can be disabled by setting the ’receive_shadows’ option to ’false’ in a material script. Similarly, if an object is set as a shadow caster, it cannot receive shadows.</p>
<h1><a class="anchor" id="Configuring-Texture-Shadows"></a>
Configuring Texture Shadows</h1>
<p>There are a number of settings which will help you configure your texture-based shadows so that they match your requirements.</p>
<ul>
<li><a href="#Maximum-number-of-shadow-textures">Maximum number of shadow textures</a></li>
<li><a href="#Shadow-texture-size">Shadow texture size</a></li>
<li><a href="#Shadow-far-distance">Shadow far distance</a></li>
<li><a href="#Shadow-texture-offset-Directional-Lights_0029">Shadow texture offset (Directional Lights)</a></li>
<li><a href="#Shadow-fade-settings">Shadow fade settings</a></li>
<li><a href="#Custom-shadow-camera-setups">Custom shadow camera setups</a></li>
<li><a href="#Shadow-texture-Depth-Buffer-sharing">Shadow texture Depth Buffer sharing</a></li>
<li><a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a></li>
</ul>
<p><a class="anchor" id="Maximum-number-of-shadow-textures"></a><a class="anchor" id="Maximum-number-of-shadow-textures-1"></a></p>
<h2>Maximum number of shadow textures</h2>
<p>Shadow textures take up texture memory, and to avoid stalling the rendering pipeline <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> does not reuse the same shadow texture for multiple lights within the same frame. This means that each light which is to cast shadows must have its own shadow texture. In practice, if you have a lot of lights in your scene you would not wish to incur that sort of texture overhead. You can adjust this manually by simply turning off shadow casting for lights you do not wish to cast shadows. In addition, you can set a maximum limit on the number of shadow textures <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is allowed to use by calling SceneManager::setShadowTextureCount. Each frame, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> determines the lights which could be affecting the frustum, and then allocates the number of shadow textures it is allowed to use to the lights on a first-come-first-served basis. Any additional lights will not cast shadows that frame. Note that you can set the number of shadow textures and their size at the same time by using the SceneManager::setShadowTextureSettings method; this is useful because both the individual calls require the potential creation / destruction of texture resources.</p>
<p><a class="anchor" id="Shadow-texture-size"></a><a class="anchor" id="Shadow-texture-size-1"></a></p>
<h2>Shadow texture size</h2>
<p>The size of the textures used for rendering the shadow casters into can be altered; clearly using larger textures will give you better quality shadows, but at the expense of greater memory usage. Changing the texture size is done by calling SceneManager::setShadowTextureSize - textures are assumed to be square and you must specify a texture size that is a power of 2. Be aware that each modulative shadow texture will take size*size*3 bytes of texture memory. <b>Important</b>: if you use the GL render system your shadow texture size can only be larger (in either dimension) than the size of your primary window surface if the hardware supports the Frame Buffer Object (FBO) or Pixel Buffer Object (PBO) extensions. Most modern cards support this now, but be careful of older cards - you can check the ability of the hardware to manage this through ogreRoot-&gt;getRenderSystem()-&gt;getCapabilities()-&gt;hasCapability(RSC_HWRENDER_TO_TEXTURE). If this returns false, if you create a shadow texture larger in any dimension than the primary surface, the rest of the shadow texture will be blank.</p>
<p><a class="anchor" id="Shadow-far-distance"></a><a class="anchor" id="Shadow-far-distance-1"></a></p>
<h2>Shadow far distance</h2>
<p>This determines the distance at which shadows are terminated; it also determines how far into the distance the texture shadows for directional lights are stretched - by reducing this value, or increasing the texture size, you can improve the quality of shadows from directional lights at the expense of closer shadow termination or increased memory usage, respectively.</p>
<p><a class="anchor" id="Shadow-texture-offset-Directional-Lights_0029"></a></p>
<h2>Shadow texture offset (Directional Lights)</h2>
<p>As mentioned above in the directional lights section, the rendering of shadows for directional lights is an approximation that allows us to use a single render to cover a largish area with shadows. This offset parameter affects how far from the camera position the center of the shadow texture is offset, as a proportion of the shadow far distance. The greater this value, the more of the shadow texture is ’useful’ to you since it’s ahead of the camera, but also the further you offset it, the more chance there is of accidentally seeing the edge of the shadow texture at more extreme angles. You change this value by calling SceneManager::setShadowDirLightTextureOffset, the default is 0.6.</p>
<p><a class="anchor" id="Shadow-fade-settings"></a><a class="anchor" id="Shadow-fade-settings-1"></a></p>
<h2>Shadow fade settings</h2>
<p>Shadows fade out before the shadow far distance so that the termination of shadow is not abrupt. You can configure the start and end points of this fade by calling the SceneManager::setShadowTextureFadeStart and SceneManager::setShadowTextureFadeEnd methods, both take distances as a proportion of the shadow far distance. Because of the inaccuracies caused by using a square texture and a radial fade distance, you cannot use 1.0 as the fade end, if you do you’ll see artifacts at the extreme edges. The default values are 0.7 and 0.9, which serve most purposes but you can change them if you like.</p>
<h1>Texture shadows and vertex / fragment programs</h1>
<p>When rendering shadow casters into a modulative shadow texture, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> turns off all textures, and all lighting contributions except for ambient light, which it sets to the colour of the shadow (<a href="#Shadow-Colour">Shadow Colour</a>). For additive shadows, it render the casters into a black &amp; white texture instead. This is enough to render shadow casters for fixed-function material techniques, however where a vertex program is used <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> doesn’t have so much control. If you use a vertex program in the <b>first pass</b> of your technique, then you must also tell ogre which vertex program you want it to use when rendering the shadow caster; see <a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a> for full details.</p>
<p><a class="anchor" id="Custom-shadow-camera-setups"></a><a class="anchor" id="Custom-shadow-camera-setups-1"></a></p>
<h2>Custom shadow camera setups</h2>
<p>As previously mentioned, one of the downsides of texture shadows is that the texture resolution is finite, and it’s possible to get aliasing when the size of the shadow texel is larger than a screen pixel, due to the projection of the texture. In order to address this, you can specify alternative projection bases by using or creating subclasses of the ShadowCameraSetup class. The default version is called DefaultShadowCameraSetup and this sets up a simple regular frustum for point and spotlights, and an orthographic frustum for directional lights. There is also a PlaneOptimalShadowCameraSetup class which specialises the projection to a plane, thus giving you much better definition provided your shadow receivers exist mostly in a single plane. Other setup classes (e.g. you might create a perspective or trapezoid shadow mapping version) can be created and plugged in at runtime, either on individual lights or on the SceneManager as a whole.</p>
<p><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing"></a><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing-1"></a></p>
<h2>Shadow texture Depth Buffer sharing</h2>
<p>Shadow textures need a depth buffer like many other RTs (Render Textures). Prior to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8, the depth buffer behavior was left undefined leaving a very small possibility of causing inconsistencies across different window resolutions and render systems. Depending on the render window’s resolutions and/or rendersystem being used, the depth buffer might been shared with the render window or a new one could get created to suite the shadow textures. If the application was depending on the depth buffer contents from the previous scene render (that is, no clear was performed) where a shadow texture render pass was in the middle; then the depth buffer would’ve contained garbage (but not consistent on all machines) causing graphical glitches hard to spot.</p>
<p>From <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8 onwards the depth buffer usage &amp; sharing can be flexible controlled through the use of depth pool IDs. These pool IDs are not specifically part of shadow textures, but rather anything involving RTs. All RTs with the same pool ID share the same depth buffers when possible (following RenderSystem API rules, check RenderSystemCapabilities flags to find out what the behavior will be). The default ID for shadow textures is 1; which is the same default value for render windows, and RTTs; thus maintaining the same behavior with older applications while achieving maximum memory saving and performance efficiency because the number of created depth buffers is as lowest as possible.</p>
<p>However there are some reasons to put shadow textures in a separate pool. This holds specially true if the application depends on the previous contents from the depth buffer before the shadow pass, instead of doing a clear:</p>
<ul>
<li>In Direct3D9, the shadow texture is more likely to share the depth buffer with the render window at high resolutions (when the window is bigger than the shadow texture resolution), but at low resolutions it won’t be shared, thus causing two different behaviors. Also probably the shadow texture will share the depth buffers with most other RTTs (i.e. compositors)</li>
<li>In OpenGL 2.1, the shadow texture can’t be shared with the main render window; and most likely will NOT be shared with many other RTTs (i.e. compositors) since OGL 2.1 has a requirement that texture resolutions should exactly match, while D3D9 specifies depth buffers can be shared as long as the resolutions are equal or less.</li>
</ul>
<p>For example, the DeferredShading sample suffers from this problem. If this is a problem for a particular effect you’re trying to achieve, you can specify a custom pool ID so that shadow textures get their own depth buffer(s), ensuring they aren’t shared with other RTs. You can set the poolId parameter from either SceneManager::setShadowTextureSettings or setShadowTextureConfig</p>
<div class="fragment"><div class="line">mSceneMgr-&gt;setShadowTextureSettings( size, count, format, PoolId );</div><div class="line">mSceneMgr-&gt;setShadowTextureConfig( 0, 512, 512, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39cab63dbcddd53d1f766d742fd557c26be4">PF_FLOAT16_R</a>, 50 );</div></div><!-- fragment --><p>Note a poolId of 0 will make the shadow textures not to use a depth buffer, which isn’t usually a desired behavior.</p>
<p><a class="anchor" id="Integrated-Texture-Shadows"></a><a class="anchor" id="Integrated-Texture-Shadows-1"></a></p>
<h2>Integrated Texture Shadows</h2>
<p>Texture shadows have one major advantage over stencil shadows - the data used to represent them can be referenced in regular shaders. Whilst the default texture shadow modes (SHADOWTYPE_TEXTURE_MODULATIVE and SHADOWTYPE_TEXTURE_ADDITIVE) automatically render shadows for you, their disadvantage is that because they are generalised add-ons to your own materials, they tend to take more passes of the scene to use. In addition, you don’t have a lot of control over the composition of the shadows.</p>
<p>Here is where ’integrated’ texture shadows step in. Both of the texture shadow types above have alternative versions called SHADOWTYPE_TEXTURE_MODULATIVE_INTEGRATED and SHADOWTYPE_TEXTURE_ADDITIVE_INTEGRATED, where instead of rendering the shadows for you, it just creates the texture shadow and then expects you to use that shadow texture as you see fit when rendering receiver objects in the scene. The downside is that you have to take into account shadow receipt in every one of your materials if you use this option - the upside is that you have total control over how the shadow textures are used. The big advantage here is that you can can perform more complex shading, taking into account shadowing, than is possible using the generalised bolt-on approaches, AND you can probably write them in a smaller number of passes, since you know precisely what you need and can combine passes where possible. When you use one of these shadowing approaches, the only difference between additive and modulative is the colour of the casters in the shadow texture (the shadow colour for modulative, black for additive) - the actual calculation of how the texture affects the receivers is of course up to you. No separate modulative pass will be performed, and no splitting of your materials into ambient / per-light / decal etc will occur - absolutely everything is determined by your original material (which may have modulative passes or per-light iteration if you want of course, but it’s not required).</p>
<p>You reference a shadow texture in a material which implements this approach by using the ’<a href="#content_005ftype">content_type</a> shadow’ directive in your <a href="#Texture-Units">texture_unit</a>. It implicitly references a shadow texture based on the number of times you’ve used this directive in the same pass, and the light_start option or light-based pass iteration, which might start the light index higher than 0.</p>
<h1><a class="anchor" id="Modulative-Shadows"></a>
Modulative Shadows</h1>
<p>Modulative shadows work by darkening an already rendered scene with a fixed colour. First, the scene is rendered normally containing all the objects which will be shadowed, then a modulative pass is done per light, which darkens areas in shadow. Finally, objects which do not receive shadows are rendered.</p>
<p>There are 2 modulative shadow techniques; stencil-based (See <a href="#Stencil-Shadows">Stencil Shadows</a> : SHADOWTYPE_STENCIL_MODULATIVE) and texture-based (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a> : SHADOWTYPE_TEXTURE_MODULATIVE). Modulative shadows are an inaccurate lighting model, since they darken the areas of shadow uniformly, irrespective of the amount of light which would have fallen on the shadow area anyway. However, they can give fairly attractive results for a much lower overhead than more ’correct’ methods like <a href="#Additive-Light-Masking">Additive Light Masking</a>, and they also combine well with pre-baked static lighting (such as pre-calculated lightmaps), which additive lighting does not. The main thing to consider is that using multiple light sources can result in overly dark shadows (where shadows overlap, which intuitively looks right in fact, but it’s not physically correct) and artifacts when using stencil shadows (See <a href="#The-Silhouette-Edge">The Silhouette Edge</a>).</p>
<p><a class="anchor" id="Shadow-Colour"></a><a class="anchor" id="Shadow-Colour-1"></a></p>
<h2>Shadow Colour</h2>
<p>The colour which is used to darken the areas in shadow is set by SceneManager::setShadowColour; it defaults to a dark grey (so that the underlying colour still shows through a bit).</p>
<p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to have a separate pass of the scene to render shadows. In this case the ’modulative’ aspect of the shadow technique just affects the colour of the shadow texture.</p>
<h1><a class="anchor" id="Additive-Light-Masking"></a>
Additive Light Masking</h1>
<p>Additive light masking is about rendering the scene many times, each time representing a single light contribution whose influence is masked out in areas of shadow. Each pass is combined with (added to) the previous one such that when all the passes are complete, all the light contribution has correctly accumulated in the scene, and each light has been prevented from affecting areas which it should not be able to because of shadow casters. This is an effective technique which results in very realistic looking lighting, but it comes at a price: more rendering passes.</p>
<p>As many technical papers (and game marketing) will tell you, rendering realistic lighting like this requires multiple passes. Being a friendly sort of engine, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> frees you from most of the hard work though, and will let you use the exact same material definitions whether you use this lighting technique or not (for the most part, see <a href="#Pass-Classification-and-Vertex-Programs">Pass Classification and Vertex Programs</a>). In order to do this technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically categorises the <a href="#Passes">Passes</a> you define in your materials into 3 types:</p>
<ol type="1">
<li>ambient Passes categorised as ’ambient’ include any base pass which is not lit by any particular light, i.e. it occurs even if there is no ambient light in the scene. The ambient pass always happens first, and sets up the initial depth value of the fragments, and the ambient colour if applicable. It also includes any emissive / self illumination contribution. Only textures which affect ambient light (e.g. ambient occlusion maps) should be rendered in this pass.</li>
<li>diffuse/specular Passes categorised as ’diffuse/specular’ (or ’per-light’) are rendered once per light, and each pass contributes the diffuse and specular colour from that single light as reflected by the diffuse / specular terms in the pass. Areas in shadow from that light are masked and are thus not updated. The resulting masked colour is added to the existing colour in the scene. Again, no textures are used in this pass (except for textures used for lighting calculations such as normal maps).</li>
<li>decal Passes categorised as ’decal’ add the final texture colour to the scene, which is modulated by the accumulated light built up from all the ambient and diffuse/specular passes.</li>
</ol>
<p>In practice, <a href="#Passes">Passes</a> rarely fall nicely into just one of these categories. For each Technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> compiles a list of ’Illumination Passes’, which are derived from the user defined passes, but can be split, to ensure that the divisions between illumination pass categories can be maintained. For example, if we take a very simple material definition:</p>
<div class="fragment"><div class="line">material TestIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will split this into 3 illumination passes, which will be the equivalent of this:</p>
<div class="fragment"><div class="line">material TestIlluminationSplitIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 0 0 0</div><div class="line">            specular 0 0 0</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Diffuse / specular pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend add</div><div class="line">            iteration once_per_light</div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend modulate</div><div class="line">            lighting off</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>So as you can see, even a simple material requires a minimum of 3 passes when using this shadow technique, and in fact it requires (num_lights + 2) passes in the general sense. You can use more passes in your original material and <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will cope with that too, but be aware that each pass may turn into multiple ones if it uses more than one type of light contribution (ambient vs diffuse/specular) and / or has texture units. The main nice thing is that you get the full multipass lighting behaviour even if you don’t define your materials in terms of it, meaning that your material definitions can remain the same no matter what lighting approach you decide to use.</p>
<p><a class="anchor" id="Manually-Categorising-Illumination-Passes"></a><a class="anchor" id="Manually-Categorising-Illumination-Passes-1"></a></p>
<h2>Manually Categorising Illumination Passes</h2>
<p>Alternatively, if you want more direct control over the categorisation of your passes, you can use the <a href="#illumination_005fstage">illumination_stage</a> option in your pass to explicitly assign a pass unchanged to an illumination stage. This way you can make sure you know precisely how your material will be rendered under additive lighting conditions.</p>
<p><a class="anchor" id="Pass-Classification-and-Vertex-Programs"></a><a class="anchor" id="Pass-Classification-and-Vertex-Programs-1"></a></p>
<h2>Pass Classification and Vertex Programs</h2>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at classifying and splitting your passes to ensure that the multipass rendering approach required by additive lighting works correctly without you having to change your material definitions. However, there is one exception; when you use vertex programs, the normal lighting attributes ambient, diffuse, specular etc are not used, because all of that is determined by the vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has no way of knowing what you’re doing inside that vertex program, so you have to tell it.</p>
<p>In practice this is very easy. Even though your vertex program could be doing a lot of complex, highly customised processing, it can still be classified into one of the 3 types listed above. All you need to do to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> what you’re doing is to use the pass attributes ambient, diffuse, specular and self_illumination, just as if you were not using a vertex program. Sure, these attributes do nothing (as far as rendering is concerned) when you’re using vertex programs, but it’s the easiest way to indicate to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> which light components you’re using in your vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will then classify and potentially split your programmable pass based on this information - it will leave the vertex program as-is (so that any split passes will respect any vertex modification that is being done).</p>
<p>Note that when classifying a diffuse/specular programmable pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> checks to see whether you have indicated the pass can be run once per light (iteration once_per_light). If so, the pass is left intact, including it’s vertex and fragment programs. However, if this attribute is not included in the pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> tries to split off the per-light part, and in doing so it will disable the fragment program, since in the absence of the ’iteration once_per_light’ attribute it can only assume that the fragment program is performing decal work and hence must not be used per light.</p>
<p>So clearly, when you use additive light masking as a shadow technique, you need to make sure that programmable passes you use are properly set up so that they can be classified correctly. However, also note that the changes you have to make to ensure the classification is correct does not affect the way the material renders when you choose not to use additive lighting, so the principle that you should be able to use the same material definitions for all lighting scenarios still holds. Here is an example of a programmable material which will be classified correctly by the illumination pass classifier:</p>
<div class="fragment"><div class="line"><span class="comment">// Per-pixel normal mapping Any number of lights, diffuse and specular</span></div><div class="line">material Examples/BumpMapping/MultiLightSpecular</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Base ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// ambient only, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 1 1 1</div><div class="line">            diffuse 0 0 0 </div><div class="line">            specular 0 0 0 0</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named_auto ambient ambient_light_colour</div><div class="line">            }</div><div class="line">        }</div><div class="line">        <span class="comment">// Now do the lighting pass</span></div><div class="line">        <span class="comment">// NB we don&#39;t do decal texture here because this is repeated per light</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// set ambient off, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 0 0 0 </div><div class="line">            <span class="comment">// do this for each light</span></div><div class="line">            iteration once_per_light</div><div class="line">            scene_blend add</div><div class="line"></div><div class="line">            <span class="comment">// Vertex program reference</span></div><div class="line">            vertex_program_ref Examples/BumpMapVPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightPosition light_position_object_space 0</div><div class="line">                param_named_auto eyePosition camera_position_object_space</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Fragment program</span></div><div class="line">            fragment_program_ref Examples/BumpMapFPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Base bump map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture NMBumpsOut.png</div><div class="line">                colour_op replace</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map #2</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            lighting off</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named ambient float4 1 1 1 1</div><div class="line">            }</div><div class="line">            scene_blend dest_colour zero</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture RustedMetal.jpg </div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to use this explicit sequence - allowing you to compress the number of passes into a much smaller number at the expense of defining an upper number of shadow casting lights. In this case the ’additive’ aspect of the shadow technique just affects the colour of the shadow texture and it’s up to you to combine the shadow textures in your receivers however you like.</p>
<p><a class="anchor" id="Static-Lighting"></a></p>
<h2>Static Lighting</h2>
<p>Despite their power, additive lighting techniques have an additional limitation; they do not combine well with pre-calculated static lighting in the scene. This is because they are based on the principle that shadow is an absence of light, but since static lighting in the scene already includes areas of light and shadow, additive lighting cannot remove light to create new shadows. Therefore, if you use the additive lighting technique you must either use it exclusively as your lighting solution (and you can combine it with per-pixel lighting to create a very impressive dynamic lighting solution), or you must use <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> to combine the static lighting according to your chosen approach. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Shadows</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_shadows.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Shadows </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#Enabling-shadows">Enabling shadows</a></li>
<li class="level1"><a href="#Opting-out-of-shadows">Opting out of shadows</a></li>
<li class="level1"><a href="#Stencil-Shadows">Stencil Shadows</a></li>
<li class="level1"><a href="#Texture_002dbased-Shadows">Texture-based Shadows</a></li>
<li class="level1"><a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a></li>
<li class="level1"><a href="#Modulative-Shadows">Modulative Shadows</a></li>
<li class="level1"><a href="#Additive-Light-Masking">Additive Light Masking</a></li>
</ul>
</div>
<div class="textblock"><p>Shadows are clearly an important part of rendering a believable scene - they provide a more tangible feel to the objects in the scene, and aid the viewer in understanding the spatial relationship between objects. Unfortunately, shadows are also one of the most challenging aspects of 3D rendering, and they are still very much an active area of research. Whilst there are many techniques to render shadows, none is perfect and they all come with advantages and disadvantages. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> provides multiple shadow implementations, with plenty of configuration settings, so you can choose which technique is most appropriate for your scene.</p>
<p>Shadow implementations fall into basically 2 broad categories: <a href="#Stencil-Shadows">Stencil Shadows</a> and <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>. This describes the method by which the shape of the shadow is generated. In addition, there is more than one way to render the shadow into the scene: <a href="#Modulative-Shadows">Modulative Shadows</a>, which darkens the scene in areas of shadow, and <a href="#Additive-Light-Masking">Additive Light Masking</a> which by contrast builds up light contribution in areas which are not in shadow. You also have the option of <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> which gives you complete control over texture shadow application, allowing for complex single-pass shadowing shaders. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports all these combinations.</p>
<h1><a class="anchor" id="Enabling-shadows"></a>
Enabling shadows</h1>
<p>Shadows are disabled by default, here’s how you turn them on and configure them in the general sense:</p>
<ol type="1">
<li><p class="startli">Enable a shadow technique on the SceneManager as the <b>first</b> thing you doing your scene setup. It is important that this is done first because the shadow technique can alter the way meshes are loaded. Here’s an example:</p>
<p class="startli"><code>mSceneMgr-&gt;setShadowTechnique(SHADOWTYPE_STENCIL_ADDITIVE);</code></p>
</li>
<li>Create one or more lights. Note that not all light types are necessarily supported by all shadow techniques, you should check the sections about each technique to check. Note that if certain lights should not cast shadows, you can turn that off by calling setCastShadows(false) on the light, the default is true.</li>
<li>Disable shadow casting on objects which should not cast shadows. Call setCastShadows(false) on objects you don’t want to cast shadows, the default for all objects is to cast shadows.</li>
<li>Configure shadow far distance. You can limit the distance at which shadows are considered for performance reasons, by calling SceneManager::setShadowFarDistance.</li>
<li>Turn off the receipt of shadows on materials that should not receive them. You can turn off the receipt of shadows (note, not the casting of shadows - that is done per-object) by calling Material::setReceiveShadows or using the receive_shadows material attribute. This is useful for materials which should be considered self-illuminated for example. Note that transparent materials are typically excluded from receiving and casting shadows, although see the <a href="#transparency_005fcasts_005fshadows">transparency_casts_shadows</a> option for exceptions.</li>
</ol>
<h1><a class="anchor" id="Opting-out-of-shadows"></a>
Opting out of shadows</h1>
<p>By default <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> treats all non-transparent objects as shadow casters and receivers (depending on the shadow technique they may not be able to be both at once, check the docs for your chosen technique first). You can disable shadows in various ways:</p>
<dl compact="compact">
<dt>Turning off shadow casting on the light </dt>
<dd><p class="startdd"></p>
<p>Calling Light::setCastsShadows(false) will mean this light casts no shadows at all.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow receipt on a material </dt>
<dd><p class="startdd"></p>
<p>Calling Material::setReceiveShadows(false) will prevent any objects using this material from receiving shadows.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadow casting on individual objects </dt>
<dd><p class="startdd"></p>
<p>Calling MovableObject::setCastsShadows(false) will disable shadow casting for this object.</p>
<p class="enddd"></p>
</dd>
<dt>Turn off shadows on an entire rendering queue group </dt>
<dd><p class="startdd"></p>
<p>Calling RenderQueueGroup::setShadowsEnabled(false) will turn off both shadow casting and receiving on an entire rendering queue group. This is useful because <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has to do light setup tasks per group in order to preserve the inter-group ordering. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically disables shadows on a number of groups automatically, such as RENDER_QUEUE_BACKGROUND, RENDER_QUEUE_OVERLAY, RENDER_QUEUE_SKIES_EARLY and RENDER_QUEUE_SKIES_LATE. If you choose to use more rendering queues (and by default, you won’t be using any more than this plus the ’standard’ queue, so ignore this if you don’t know what it means!), be aware that each one can incur a light setup cost, and you should disable shadows on the additional ones you use if you can.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Stencil-Shadows"></a>
Stencil Shadows</h1>
<p>Stencil shadows are a method by which a ’mask’ is created for the screen using a feature called the stencil buffer. This mask can be used to exclude areas of the screen from subsequent renders, and thus it can be used to either include or exclude areas in shadow. They are enabled by calling SceneManager::setShadowTechnique with a parameter of either <code>SHADOWTYPE_STENCIL_ADDITIVE</code> or <code>SHADOWTYPE_STENCIL_MODULATIVE</code>. Because the stencil can only mask areas to be either ’enabled’ or ’disabled’, stencil shadows have ’hard’ edges, that is to say clear dividing lines between light and shadow - it is not possible to soften these edges.</p>
<p>In order to generate the stencil, ’shadow volumes’ are rendered by extruding the silhouette of the shadow caster away from the light. Where these shadow volumes intersect other objects (or the caster, since self-shadowing is supported using this technique), the stencil is updated, allowing subsequent operations to differentiate between light and shadow. How exactly this is used to render the shadows depends on whether <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> is being used. Objects can both cast and receive stencil shadows, so self-shadowing is inbuilt.</p>
<p>The advantage of stencil shadows is that they can do self-shadowing simply on low-end hardware, provided you keep your poly count under control. In contrast doing self-shadowing with texture shadows requires a fairly modern machine (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>). For this reason, you’re likely to pick stencil shadows if you need an accurate shadowing solution for an application aimed at older or lower-spec machines.</p>
<p>The disadvantages of stencil shadows are numerous though, especially on more modern hardware. Because stencil shadows are a geometric technique, they are inherently more costly the higher the number of polygons you use, meaning you are penalized the more detailed you make your meshes. The fillrate cost, which comes from having to render shadow volumes, also escalates the same way. Since more modern applications are likely to use higher polygon counts, stencil shadows can start to become a bottleneck. In addition, the visual aspects of stencil shadows are pretty primitive - your shadows will always be hard-edged, and you have no possibility of doing clever things with shaders since the stencil is not available for manipulation there. Therefore, if your application is aimed at higher-end machines you should definitely consider switching to texture shadows (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a>).</p>
<p>There are a number of issues to consider which are specific to stencil shadows:</p>
<ul>
<li><a href="#CPU-Overhead">CPU Overhead</a></li>
<li><a href="#Extrusion-distance">Extrusion distance</a></li>
<li><a href="#Camera-far-plane-positioning">Camera far plane positioning</a></li>
<li><a href="#Mesh-edge-lists">Mesh edge lists</a></li>
<li><a href="#The-Silhouette-Edge">The Silhouette Edge</a></li>
<li><a href="#Be-realistic">Be realistic</a></li>
<li><a href="#Stencil-Optimisations-Performed-By-Ogre">Stencil Optimisations Performed By Ogre</a></li>
</ul>
<p><a class="anchor" id="CPU-Overhead"></a><a class="anchor" id="CPU-Overhead-1"></a></p>
<h2>CPU Overhead</h2>
<p>Calculating the shadow volume for a mesh can be expensive, and it has to be done on the CPU, it is not a hardware accelerated feature. Therefore, you can find that if you overuse this feature, you can create a CPU bottleneck for your application. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> quite aggressively eliminates objects which cannot be casting shadows on the frustum, but there are limits to how much it can do, and large, elongated shadows (e.g. representing a very low sun position) are very difficult to cull efficiently. Try to avoid having too many shadow casters around at once, and avoid long shadows if you can. Also, make use of the ’shadow far distance’ parameter on the SceneManager, this can eliminate distant shadow casters from the shadow volume construction and save you some time, at the expense of only having shadows for closer objects. Lastly, make use of Ogre’s Level-Of-Detail (LOD) features; you can generate automatically calculated LODs for your meshes in code (see the Mesh API docs) or when using the mesh tools such as OgreXMLConverter and OgreMeshUpgrader. Alternatively, you can assign your own manual LODs by providing alternative mesh files at lower detail levels. Both methods will cause the shadow volume complexity to decrease as the object gets further away, which saves you valuable volume calculation time.</p>
<p><a class="anchor" id="Extrusion-distance"></a><a class="anchor" id="Extrusion-distance-1"></a></p>
<h2>Extrusion distance</h2>
<p>When vertex programs are not available, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> can only extrude shadow volumes a finite distance from the object. If an object gets too close to a light, any finite extrusion distance will be inadequate to guarantee all objects will be shadowed properly by this object. Therefore, you are advised not to let shadow casters pass too close to light sources if you can avoid it, unless you can guarantee that your target audience will have vertex program capable hardware (in this case, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> extrudes the volume to infinity using a vertex program so the problem does not occur). When infinite extrusion is not possible, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses finite extrusion, either derived from the attenuation range of a light (in the case of a point light or spotlight), or a fixed extrusion distance set in the application in the case of directional lights. To change the directional light extrusion distance, use SceneManager::setShadowDirectionalLightExtrusionDistance.</p>
<p><a class="anchor" id="Camera-far-plane-positioning"></a><a class="anchor" id="Camera-far-plane-positioning-1"></a></p>
<h2>Camera far plane positioning</h2>
<p>Stencil shadow volumes rely very much on not being clipped by the far plane. When you enable stencil shadows, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> internally changes the far plane settings of your cameras such that there is no far plane - i.e. it is placed at infinity (Camera::setFarClipDistance(0)). This avoids artifacts caused by clipping the dark caps on shadow volumes, at the expense of a (very) small amount of depth precision.</p>
<p><a class="anchor" id="Mesh-edge-lists"></a><a class="anchor" id="Mesh-edge-lists-1"></a></p>
<h2>Mesh edge lists</h2>
<p>Stencil shadows can only be calculated when an ’edge list’ has been built for all the geometry in a mesh. The official exporters and tools automatically build this for you (or have an option to do so), but if you create your own meshes, you must remember to build edge lists for them before using them with stencil shadows - you can do that by using OgreMeshUpgrade or OgreXmlConverter, or by calling Mesh::buildEdgeList before you export or use the mesh. If a mesh doesn’t have edge lists, OGRE assumes that it is not supposed to cast stencil shadows.</p>
<p><a class="anchor" id="The-Silhouette-Edge"></a><a class="anchor" id="The-Silhouette-Edge-1"></a></p>
<h2>The Silhouette Edge</h2>
<p>Stencil shadowing is about finding a silhouette of the mesh, and projecting it away to form a volume. What this means is that there is a definite boundary on the shadow caster between light and shadow; a set of edges where where the triangle on one side is facing toward the light, and one is facing away. This produces a sharp edge around the mesh as the transition occurs. Provided there is little or no other light in the scene, and the mesh has smooth normals to produce a gradual light change in its underlying shading, the silhouette edge can be hidden - this works better the higher the tessellation of the mesh. However, if the scene includes ambient light, then the difference is far more marked. This is especially true when using <a href="#Modulative-Shadows">Modulative Shadows</a>, because the light contribution of each shadowed area is not taken into account by this simplified approach, and so using 2 or more lights in a scene using modulative stencil shadows is not advisable; the silhouette edges will be very marked. Additive lights do not suffer from this as badly because each light is masked individually, meaning that it is only ambient light which can show up the silhouette edges.</p>
<p><a class="anchor" id="Be-realistic"></a><a class="anchor" id="Be-realistic-1"></a></p>
<h2>Be realistic</h2>
<p>Don’t expect to be able to throw any scene using any hardware at the stencil shadow algorithm and expect to get perfect, optimum speed results. Shadows are a complex and expensive technique, so you should impose some reasonable limitations on your placing of lights and objects; they’re not really that restricting, but you should be aware that this is not a complete free-for-all.</p>
<ul>
<li>Try to avoid letting objects pass very close (or even through) lights - it might look nice but it’s one of the cases where artifacts can occur on machines not capable of running vertex programs.</li>
<li>Be aware that shadow volumes do not respect the ’solidity’ of the objects they pass through, and if those objects do not themselves cast shadows (which would hide the effect) then the result will be that you can see shadows on the other side of what should be an occluding object.</li>
<li>Make use of SceneManager::setShadowFarDistance to limit the number of shadow volumes constructed</li>
<li>Make use of LOD to reduce shadow volume complexity at distance</li>
<li><p class="startli">Avoid very long (dusk and dawn) shadows - they exacerbate other issues such as volume clipping, fillrate, and cause many more objects at a greater distance to require volume construction.</p>
<p class="startli"><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre"></a><a class="anchor" id="Stencil-Optimisations-Performed-By-Ogre-1"></a></p>
</li>
</ul>
<h2>Stencil Optimisations Performed By <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a></h2>
<p>Despite all that, stencil shadows can look very nice (especially with <a href="#Additive-Light-Masking">Additive Light Masking</a>) and can be fast if you respect the rules above. In addition, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> comes pre-packed with a lot of optimisations which help to make this as quick as possible. This section is more for developers or people interested in knowing something about the ’under the hood’ behaviour of <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a>.</p>
<dl compact="compact">
<dt>Vertex program extrusion </dt>
<dd><p class="startdd"></p>
<p>As previously mentioned, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> performs the extrusion of shadow volumes in hardware on vertex program-capable hardware (e.g. GeForce3, Radeon 8500 or better). This has 2 major benefits; the obvious one being speed, but secondly that vertex programs can extrude points to infinity, which the fixed-function pipeline cannot, at least not without performing all calculations in software. This leads to more robust volumes, and also eliminates more than half the volume triangles on directional lights since all points are projected to a single point at infinity.</p>
<p class="enddd"></p>
</dd>
<dt>Scissor test optimisation </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a scissor rectangle to limit the effect of point / spot lights when their range does not cover the entire viewport; that means we save fillrate when rendering stencil volumes, especially with distant lights</p>
<p class="enddd"></p>
</dd>
<dt>Z-Pass and Z-Fail algorithms </dt>
<dd><p class="startdd"></p>
<p>The Z-Fail algorithm, often attributed to John Carmack, is used in <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> to make sure shadows are robust when the camera passes through the shadow volume. However, the Z-Fail algorithm is more expensive than the traditional Z-Pass; so <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> detects when Z-Fail is required and only uses it then, Z-Pass is used at all other times.</p>
<p class="enddd"></p>
</dd>
<dt>2-Sided stenciling and stencil wrapping </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> supports the 2-Sided stenciling / stencil wrapping extensions, which when supported allow volumes to be rendered in a single pass instead of having to do one pass for back facing tris and another for front-facing tris. This doesn’t save fillrate, since the same number of stencil updates are done, but it does save primitive setup and the overhead incurred in the driver every time a render call is made.</p>
<p class="enddd"></p>
</dd>
<dt>Aggressive shadow volume culling </dt>
<dd><p class="startdd"></p>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at detecting which lights could be affecting the frustum, and from that, which objects could be casting a shadow on the frustum. This means we don’t waste time constructing shadow geometry we don’t need. Setting the shadow far distance is another important way you can reduce stencil shadow overhead since it culls far away shadow volumes even if they are visible, which is beneficial in practice since you’re most interested in shadows for close-up objects.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Texture_002dbased-Shadows"></a>
Texture-based Shadows</h1>
<p>Texture shadows involve rendering shadow casters from the point of view of the light into a texture, which is then projected onto shadow receivers. The main advantage of texture shadows as opposed to <a href="#Stencil-Shadows">Stencil Shadows</a> is that the overhead of increasing the geometric detail is far lower, since there is no need to perform per-triangle calculations. Most of the work in rendering texture shadows is done by the graphics card, meaning the technique scales well when taking advantage of the latest cards, which are at present outpacing CPUs in terms of their speed of development. In addition, texture shadows are <b>much</b> more customisable - you can pull them into shaders to apply as you like (particularly with <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a>, you can perform filtering to create softer shadows or perform other special effects on them. Basically, most modern engines use texture shadows as their primary shadow technique simply because they are more powerful, and the increasing speed of GPUs is rapidly amortizing the fillrate / texture access costs of using them.</p>
<p>The main disadvantage to texture shadows is that, because they are simply a texture, they have a fixed resolution which means if stretched, the pixellation of the texture can become obvious. There are ways to combat this though:</p>
<dl compact="compact">
<dt>Choosing a projection basis </dt>
<dd><p class="startdd"></p>
<p>The simplest projection is just to render the shadow casters from the lights perspective using a regular camera setup. This can look bad though, so there are many other projections which can help to improve the quality from the main camera’s perspective. OGRE supports pluggable projection bases via it’s ShadowCameraSetup class, and comes with several existing options - <b>Uniform</b> (which is the simplest), <b>Uniform Focussed</b> (which is still a normal camera projection, except that the camera is focussed into the area that the main viewing camera is looking at), LiSPSM (Light Space Perspective Shadow Mapping - which both focusses and distorts the shadow frustum based on the main view camera) and Plan Optimal (which seeks to optimise the shadow fidelity for a single receiver plane).</p>
<p class="enddd"></p>
</dd>
<dt>Filtering </dt>
<dd><p class="startdd"></p>
<p>You can also sample the shadow texture multiple times rather than once to soften the shadow edges and improve the appearance. Percentage Closest Filtering (PCF) is the most popular approach, although there are multiple variants depending on the number and pattern of the samples you take. Our shadows demo includes a 5-tap PCF example combined with depth shadow mapping.</p>
<p class="enddd"></p>
</dd>
<dt>Using a larger texture </dt>
<dd><p class="startdd"></p>
<p>Again as GPUs get faster and gain more memory, you can scale up to take advantage of this.</p>
<p class="enddd"></p>
</dd>
</dl>
<p>If you combine all 3 of these techniques you can get a very high quality shadow solution.</p>
<p>The other issue is with point lights. Because texture shadows require a render to texture in the direction of the light, omnidirectional lights (point lights) would require 6 renders to totally cover all the directions shadows might be cast. For this reason, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> primarily supports directional lights and spotlights for generating texture shadows; you can use point lights but they will only work if off-camera since they are essentially turned into a spotlight shining into your camera frustum for the purposes of texture shadows.</p>
<p><a class="anchor" id="Directional-Lights"></a></p>
<h2>Directional Lights</h2>
<p>Directional lights in theory shadow the entire scene from an infinitely distant light. Now, since we only have a finite texture which will look very poor quality if stretched over the entire scene, clearly a simplification is required. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> places a shadow texture over the area immediately in front of the camera, and moves it as the camera moves (although it rounds this movement to multiples of texels so that the slight ’swimming shadow’ effect caused by moving the texture is minimised). The range to which this shadow extends, and the offset used to move it in front of the camera, are configurable (See <a href="#Configuring-Texture-Shadows">Configuring Texture Shadows</a>). At the far edge of the shadow, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> fades out the shadow based on other configurable parameters so that the termination of the shadow is softened.</p>
<p><a class="anchor" id="Spotlights"></a></p>
<h2>Spotlights</h2>
<p>Spotlights are much easier to represent as renderable shadow textures than directional lights, since they are naturally a frustum. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> represents spotlight directly by rendering the shadow from the light position, in the direction of the light cone; the field-of-view of the texture camera is adjusted based on the spotlight falloff angles. In addition, to hide the fact that the shadow texture is square and has definite edges which could show up outside the spotlight, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> uses a second texture unit when projecting the shadow onto the scene which fades out the shadow gradually in a projected circle around the spotlight.</p>
<p><a class="anchor" id="Point-Lights"></a></p>
<h2>Point Lights</h2>
<p>As mentioned above, to support point lights properly would require multiple renders (either 6 for a cubic render or perhaps 2 for a less precise parabolic mapping), so rather than do that we approximate point lights as spotlights, where the configuration is changed on the fly to make the light shine from its position over the whole of the viewing frustum. This is not an ideal setup since it means it can only really work if the point light’s position is out of view, and in addition the changing parameterisation can cause some ’swimming’ of the texture. Generally we recommend avoiding making point lights cast texture shadows.</p>
<p><a class="anchor" id="Shadow-Casters-and-Shadow-Receivers"></a></p>
<h2>Shadow Casters and Shadow Receivers</h2>
<p>To enable texture shadows, use the shadow technique SHADOWTYPE_TEXTURE_MODULATIVE or SHADOWTYPE_TEXTURE_ADDITIVE; as the name suggests this produces <a href="#Modulative-Shadows">Modulative Shadows</a> or <a href="#Additive-Light-Masking">Additive Light Masking</a> respectively. The cheapest and simplest texture shadow techniques do not use depth information, they merely render casters to a texture and render this onto receivers as plain colour - this means self-shadowing is not possible using these methods. This is the default behaviour if you use the automatic, fixed-function compatible (and thus usable on lower end hardware) texture shadow techniques. You can however use shaders-based techniques through custom shadow materials for casters and receivers to perform more complex shadow algorithms, such as depth shadow mapping which does allow self-shadowing. OGRE comes with an example of this in its shadows demo, although it’s only usable on Shader Model 2 cards or better. Whilst fixed-function depth shadow mapping is available in OpenGL, it was never standardised in Direct3D so using shaders in custom caster &amp; receiver materials is the only portable way to do it. If you use this approach, call SceneManager::setShadowTextureSelfShadow with a parameter of ’true’ to allow texture shadow casters to also be receivers. If you’re not using depth shadow mapping, OGRE divides shadow casters and receivers into 2 disjoint groups. Simply by turning off shadow casting on an object, you automatically make it a shadow receiver (although this can be disabled by setting the ’receive_shadows’ option to ’false’ in a material script. Similarly, if an object is set as a shadow caster, it cannot receive shadows.</p>
<h1><a class="anchor" id="Configuring-Texture-Shadows"></a>
Configuring Texture Shadows</h1>
<p>There are a number of settings which will help you configure your texture-based shadows so that they match your requirements.</p>
<ul>
<li><a href="#Maximum-number-of-shadow-textures">Maximum number of shadow textures</a></li>
<li><a href="#Shadow-texture-size">Shadow texture size</a></li>
<li><a href="#Shadow-far-distance">Shadow far distance</a></li>
<li><a href="#Shadow-texture-offset-Directional-Lights_0029">Shadow texture offset (Directional Lights)</a></li>
<li><a href="#Shadow-fade-settings">Shadow fade settings</a></li>
<li><a href="#Custom-shadow-camera-setups">Custom shadow camera setups</a></li>
<li><a href="#Shadow-texture-Depth-Buffer-sharing">Shadow texture Depth Buffer sharing</a></li>
<li><a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a></li>
</ul>
<p><a class="anchor" id="Maximum-number-of-shadow-textures"></a><a class="anchor" id="Maximum-number-of-shadow-textures-1"></a></p>
<h2>Maximum number of shadow textures</h2>
<p>Shadow textures take up texture memory, and to avoid stalling the rendering pipeline <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> does not reuse the same shadow texture for multiple lights within the same frame. This means that each light which is to cast shadows must have its own shadow texture. In practice, if you have a lot of lights in your scene you would not wish to incur that sort of texture overhead. You can adjust this manually by simply turning off shadow casting for lights you do not wish to cast shadows. In addition, you can set a maximum limit on the number of shadow textures <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is allowed to use by calling SceneManager::setShadowTextureCount. Each frame, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> determines the lights which could be affecting the frustum, and then allocates the number of shadow textures it is allowed to use to the lights on a first-come-first-served basis. Any additional lights will not cast shadows that frame. Note that you can set the number of shadow textures and their size at the same time by using the SceneManager::setShadowTextureSettings method; this is useful because both the individual calls require the potential creation / destruction of texture resources.</p>
<p><a class="anchor" id="Shadow-texture-size"></a><a class="anchor" id="Shadow-texture-size-1"></a></p>
<h2>Shadow texture size</h2>
<p>The size of the textures used for rendering the shadow casters into can be altered; clearly using larger textures will give you better quality shadows, but at the expense of greater memory usage. Changing the texture size is done by calling SceneManager::setShadowTextureSize - textures are assumed to be square and you must specify a texture size that is a power of 2. Be aware that each modulative shadow texture will take size*size*3 bytes of texture memory. <b>Important</b>: if you use the GL render system your shadow texture size can only be larger (in either dimension) than the size of your primary window surface if the hardware supports the Frame Buffer Object (FBO) or Pixel Buffer Object (PBO) extensions. Most modern cards support this now, but be careful of older cards - you can check the ability of the hardware to manage this through ogreRoot-&gt;getRenderSystem()-&gt;getCapabilities()-&gt;hasCapability(RSC_HWRENDER_TO_TEXTURE). If this returns false, if you create a shadow texture larger in any dimension than the primary surface, the rest of the shadow texture will be blank.</p>
<p><a class="anchor" id="Shadow-far-distance"></a><a class="anchor" id="Shadow-far-distance-1"></a></p>
<h2>Shadow far distance</h2>
<p>This determines the distance at which shadows are terminated; it also determines how far into the distance the texture shadows for directional lights are stretched - by reducing this value, or increasing the texture size, you can improve the quality of shadows from directional lights at the expense of closer shadow termination or increased memory usage, respectively.</p>
<p><a class="anchor" id="Shadow-texture-offset-Directional-Lights_0029"></a></p>
<h2>Shadow texture offset (Directional Lights)</h2>
<p>As mentioned above in the directional lights section, the rendering of shadows for directional lights is an approximation that allows us to use a single render to cover a largish area with shadows. This offset parameter affects how far from the camera position the center of the shadow texture is offset, as a proportion of the shadow far distance. The greater this value, the more of the shadow texture is ’useful’ to you since it’s ahead of the camera, but also the further you offset it, the more chance there is of accidentally seeing the edge of the shadow texture at more extreme angles. You change this value by calling SceneManager::setShadowDirLightTextureOffset, the default is 0.6.</p>
<p><a class="anchor" id="Shadow-fade-settings"></a><a class="anchor" id="Shadow-fade-settings-1"></a></p>
<h2>Shadow fade settings</h2>
<p>Shadows fade out before the shadow far distance so that the termination of shadow is not abrupt. You can configure the start and end points of this fade by calling the SceneManager::setShadowTextureFadeStart and SceneManager::setShadowTextureFadeEnd methods, both take distances as a proportion of the shadow far distance. Because of the inaccuracies caused by using a square texture and a radial fade distance, you cannot use 1.0 as the fade end, if you do you’ll see artifacts at the extreme edges. The default values are 0.7 and 0.9, which serve most purposes but you can change them if you like.</p>
<h1>Texture shadows and vertex / fragment programs</h1>
<p>When rendering shadow casters into a modulative shadow texture, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> turns off all textures, and all lighting contributions except for ambient light, which it sets to the colour of the shadow (<a href="#Shadow-Colour">Shadow Colour</a>). For additive shadows, it render the casters into a black &amp; white texture instead. This is enough to render shadow casters for fixed-function material techniques, however where a vertex program is used <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> doesn’t have so much control. If you use a vertex program in the <b>first pass</b> of your technique, then you must also tell ogre which vertex program you want it to use when rendering the shadow caster; see <a href="#Shadows-and-Vertex-Programs">Shadows and Vertex Programs</a> for full details.</p>
<p><a class="anchor" id="Custom-shadow-camera-setups"></a><a class="anchor" id="Custom-shadow-camera-setups-1"></a></p>
<h2>Custom shadow camera setups</h2>
<p>As previously mentioned, one of the downsides of texture shadows is that the texture resolution is finite, and it’s possible to get aliasing when the size of the shadow texel is larger than a screen pixel, due to the projection of the texture. In order to address this, you can specify alternative projection bases by using or creating subclasses of the ShadowCameraSetup class. The default version is called DefaultShadowCameraSetup and this sets up a simple regular frustum for point and spotlights, and an orthographic frustum for directional lights. There is also a PlaneOptimalShadowCameraSetup class which specialises the projection to a plane, thus giving you much better definition provided your shadow receivers exist mostly in a single plane. Other setup classes (e.g. you might create a perspective or trapezoid shadow mapping version) can be created and plugged in at runtime, either on individual lights or on the SceneManager as a whole.</p>
<p><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing"></a><a class="anchor" id="Shadow-texture-Depth-Buffer-sharing-1"></a></p>
<h2>Shadow texture Depth Buffer sharing</h2>
<p>Shadow textures need a depth buffer like many other RTs (Render Textures). Prior to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8, the depth buffer behavior was left undefined leaving a very small possibility of causing inconsistencies across different window resolutions and render systems. Depending on the render window’s resolutions and/or rendersystem being used, the depth buffer might been shared with the render window or a new one could get created to suite the shadow textures. If the application was depending on the depth buffer contents from the previous scene render (that is, no clear was performed) where a shadow texture render pass was in the middle; then the depth buffer would’ve contained garbage (but not consistent on all machines) causing graphical glitches hard to spot.</p>
<p>From <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> 1.8 onwards the depth buffer usage &amp; sharing can be flexible controlled through the use of depth pool IDs. These pool IDs are not specifically part of shadow textures, but rather anything involving RTs. All RTs with the same pool ID share the same depth buffers when possible (following RenderSystem API rules, check RenderSystemCapabilities flags to find out what the behavior will be). The default ID for shadow textures is 1; which is the same default value for render windows, and RTTs; thus maintaining the same behavior with older applications while achieving maximum memory saving and performance efficiency because the number of created depth buffers is as lowest as possible.</p>
<p>However there are some reasons to put shadow textures in a separate pool. This holds specially true if the application depends on the previous contents from the depth buffer before the shadow pass, instead of doing a clear:</p>
<ul>
<li>In Direct3D9, the shadow texture is more likely to share the depth buffer with the render window at high resolutions (when the window is bigger than the shadow texture resolution), but at low resolutions it won’t be shared, thus causing two different behaviors. Also probably the shadow texture will share the depth buffers with most other RTTs (i.e. compositors)</li>
<li>In OpenGL 2.1, the shadow texture can’t be shared with the main render window; and most likely will NOT be shared with many other RTTs (i.e. compositors) since OGL 2.1 has a requirement that texture resolutions should exactly match, while D3D9 specifies depth buffers can be shared as long as the resolutions are equal or less.</li>
</ul>
<p>For example, the DeferredShading sample suffers from this problem. If this is a problem for a particular effect you’re trying to achieve, you can specify a custom pool ID so that shadow textures get their own depth buffer(s), ensuring they aren’t shared with other RTs. You can set the poolId parameter from either SceneManager::setShadowTextureSettings or setShadowTextureConfig</p>
<div class="fragment"><div class="line">mSceneMgr-&gt;setShadowTextureSettings( size, count, format, PoolId );</div><div class="line">mSceneMgr-&gt;setShadowTextureConfig( 0, 512, 512, <a class="code" href="group___image.html#gga7e0353e7d36d4c2e8468641b7303d39cab63dbcddd53d1f766d742fd557c26be4">PF_FLOAT16_R</a>, 50 );</div></div><!-- fragment --><p>Note a poolId of 0 will make the shadow textures not to use a depth buffer, which isn’t usually a desired behavior.</p>
<p><a class="anchor" id="Integrated-Texture-Shadows"></a><a class="anchor" id="Integrated-Texture-Shadows-1"></a></p>
<h2>Integrated Texture Shadows</h2>
<p>Texture shadows have one major advantage over stencil shadows - the data used to represent them can be referenced in regular shaders. Whilst the default texture shadow modes (SHADOWTYPE_TEXTURE_MODULATIVE and SHADOWTYPE_TEXTURE_ADDITIVE) automatically render shadows for you, their disadvantage is that because they are generalised add-ons to your own materials, they tend to take more passes of the scene to use. In addition, you don’t have a lot of control over the composition of the shadows.</p>
<p>Here is where ’integrated’ texture shadows step in. Both of the texture shadow types above have alternative versions called SHADOWTYPE_TEXTURE_MODULATIVE_INTEGRATED and SHADOWTYPE_TEXTURE_ADDITIVE_INTEGRATED, where instead of rendering the shadows for you, it just creates the texture shadow and then expects you to use that shadow texture as you see fit when rendering receiver objects in the scene. The downside is that you have to take into account shadow receipt in every one of your materials if you use this option - the upside is that you have total control over how the shadow textures are used. The big advantage here is that you can can perform more complex shading, taking into account shadowing, than is possible using the generalised bolt-on approaches, AND you can probably write them in a smaller number of passes, since you know precisely what you need and can combine passes where possible. When you use one of these shadowing approaches, the only difference between additive and modulative is the colour of the casters in the shadow texture (the shadow colour for modulative, black for additive) - the actual calculation of how the texture affects the receivers is of course up to you. No separate modulative pass will be performed, and no splitting of your materials into ambient / per-light / decal etc will occur - absolutely everything is determined by your original material (which may have modulative passes or per-light iteration if you want of course, but it’s not required).</p>
<p>You reference a shadow texture in a material which implements this approach by using the ’<a href="#content_005ftype">content_type</a> shadow’ directive in your <a href="#Texture-Units">texture_unit</a>. It implicitly references a shadow texture based on the number of times you’ve used this directive in the same pass, and the light_start option or light-based pass iteration, which might start the light index higher than 0.</p>
<h1><a class="anchor" id="Modulative-Shadows"></a>
Modulative Shadows</h1>
<p>Modulative shadows work by darkening an already rendered scene with a fixed colour. First, the scene is rendered normally containing all the objects which will be shadowed, then a modulative pass is done per light, which darkens areas in shadow. Finally, objects which do not receive shadows are rendered.</p>
<p>There are 2 modulative shadow techniques; stencil-based (See <a href="#Stencil-Shadows">Stencil Shadows</a> : SHADOWTYPE_STENCIL_MODULATIVE) and texture-based (See <a href="#Texture_002dbased-Shadows">Texture-based Shadows</a> : SHADOWTYPE_TEXTURE_MODULATIVE). Modulative shadows are an inaccurate lighting model, since they darken the areas of shadow uniformly, irrespective of the amount of light which would have fallen on the shadow area anyway. However, they can give fairly attractive results for a much lower overhead than more ’correct’ methods like <a href="#Additive-Light-Masking">Additive Light Masking</a>, and they also combine well with pre-baked static lighting (such as pre-calculated lightmaps), which additive lighting does not. The main thing to consider is that using multiple light sources can result in overly dark shadows (where shadows overlap, which intuitively looks right in fact, but it’s not physically correct) and artifacts when using stencil shadows (See <a href="#The-Silhouette-Edge">The Silhouette Edge</a>).</p>
<p><a class="anchor" id="Shadow-Colour"></a><a class="anchor" id="Shadow-Colour-1"></a></p>
<h2>Shadow Colour</h2>
<p>The colour which is used to darken the areas in shadow is set by SceneManager::setShadowColour; it defaults to a dark grey (so that the underlying colour still shows through a bit).</p>
<p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to have a separate pass of the scene to render shadows. In this case the ’modulative’ aspect of the shadow technique just affects the colour of the shadow texture.</p>
<h1><a class="anchor" id="Additive-Light-Masking"></a>
Additive Light Masking</h1>
<p>Additive light masking is about rendering the scene many times, each time representing a single light contribution whose influence is masked out in areas of shadow. Each pass is combined with (added to) the previous one such that when all the passes are complete, all the light contribution has correctly accumulated in the scene, and each light has been prevented from affecting areas which it should not be able to because of shadow casters. This is an effective technique which results in very realistic looking lighting, but it comes at a price: more rendering passes.</p>
<p>As many technical papers (and game marketing) will tell you, rendering realistic lighting like this requires multiple passes. Being a friendly sort of engine, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> frees you from most of the hard work though, and will let you use the exact same material definitions whether you use this lighting technique or not (for the most part, see <a href="#Pass-Classification-and-Vertex-Programs">Pass Classification and Vertex Programs</a>). In order to do this technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> automatically categorises the <a href="#Passes">Passes</a> you define in your materials into 3 types:</p>
<ol type="1">
<li>ambient Passes categorised as ’ambient’ include any base pass which is not lit by any particular light, i.e. it occurs even if there is no ambient light in the scene. The ambient pass always happens first, and sets up the initial depth value of the fragments, and the ambient colour if applicable. It also includes any emissive / self illumination contribution. Only textures which affect ambient light (e.g. ambient occlusion maps) should be rendered in this pass.</li>
<li>diffuse/specular Passes categorised as ’diffuse/specular’ (or ’per-light’) are rendered once per light, and each pass contributes the diffuse and specular colour from that single light as reflected by the diffuse / specular terms in the pass. Areas in shadow from that light are masked and are thus not updated. The resulting masked colour is added to the existing colour in the scene. Again, no textures are used in this pass (except for textures used for lighting calculations such as normal maps).</li>
<li>decal Passes categorised as ’decal’ add the final texture colour to the scene, which is modulated by the accumulated light built up from all the ambient and diffuse/specular passes.</li>
</ol>
<p>In practice, <a href="#Passes">Passes</a> rarely fall nicely into just one of these categories. For each Technique, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> compiles a list of ’Illumination Passes’, which are derived from the user defined passes, but can be split, to ensure that the divisions between illumination pass categories can be maintained. For example, if we take a very simple material definition:</p>
<div class="fragment"><div class="line">material TestIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will split this into 3 illumination passes, which will be the equivalent of this:</p>
<div class="fragment"><div class="line">material TestIlluminationSplitIllumination</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            ambient 0.5 0.2 0.2 </div><div class="line">            diffuse 0 0 0</div><div class="line">            specular 0 0 0</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Diffuse / specular pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend add</div><div class="line">            iteration once_per_light</div><div class="line">            diffuse 1 0 0  </div><div class="line">            specular 1 0.8 0.8 15</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            scene_blend modulate</div><div class="line">            lighting off</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture grass.png</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>So as you can see, even a simple material requires a minimum of 3 passes when using this shadow technique, and in fact it requires (num_lights + 2) passes in the general sense. You can use more passes in your original material and <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will cope with that too, but be aware that each pass may turn into multiple ones if it uses more than one type of light contribution (ambient vs diffuse/specular) and / or has texture units. The main nice thing is that you get the full multipass lighting behaviour even if you don’t define your materials in terms of it, meaning that your material definitions can remain the same no matter what lighting approach you decide to use.</p>
<p><a class="anchor" id="Manually-Categorising-Illumination-Passes"></a><a class="anchor" id="Manually-Categorising-Illumination-Passes-1"></a></p>
<h2>Manually Categorising Illumination Passes</h2>
<p>Alternatively, if you want more direct control over the categorisation of your passes, you can use the <a href="#illumination_005fstage">illumination_stage</a> option in your pass to explicitly assign a pass unchanged to an illumination stage. This way you can make sure you know precisely how your material will be rendered under additive lighting conditions.</p>
<p><a class="anchor" id="Pass-Classification-and-Vertex-Programs"></a><a class="anchor" id="Pass-Classification-and-Vertex-Programs-1"></a></p>
<h2>Pass Classification and Vertex Programs</h2>
<p><a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> is pretty good at classifying and splitting your passes to ensure that the multipass rendering approach required by additive lighting works correctly without you having to change your material definitions. However, there is one exception; when you use vertex programs, the normal lighting attributes ambient, diffuse, specular etc are not used, because all of that is determined by the vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> has no way of knowing what you’re doing inside that vertex program, so you have to tell it.</p>
<p>In practice this is very easy. Even though your vertex program could be doing a lot of complex, highly customised processing, it can still be classified into one of the 3 types listed above. All you need to do to tell <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> what you’re doing is to use the pass attributes ambient, diffuse, specular and self_illumination, just as if you were not using a vertex program. Sure, these attributes do nothing (as far as rendering is concerned) when you’re using vertex programs, but it’s the easiest way to indicate to <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> which light components you’re using in your vertex program. <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> will then classify and potentially split your programmable pass based on this information - it will leave the vertex program as-is (so that any split passes will respect any vertex modification that is being done).</p>
<p>Note that when classifying a diffuse/specular programmable pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> checks to see whether you have indicated the pass can be run once per light (iteration once_per_light). If so, the pass is left intact, including it’s vertex and fragment programs. However, if this attribute is not included in the pass, <a class="el" href="namespace_ogre.html" title=" This source file is part of OGRE (Object-oriented Graphics Rendering Engine) For the latest info...">Ogre</a> tries to split off the per-light part, and in doing so it will disable the fragment program, since in the absence of the ’iteration once_per_light’ attribute it can only assume that the fragment program is performing decal work and hence must not be used per light.</p>
<p>So clearly, when you use additive light masking as a shadow technique, you need to make sure that programmable passes you use are properly set up so that they can be classified correctly. However, also note that the changes you have to make to ensure the classification is correct does not affect the way the material renders when you choose not to use additive lighting, so the principle that you should be able to use the same material definitions for all lighting scenarios still holds. Here is an example of a programmable material which will be classified correctly by the illumination pass classifier:</p>
<div class="fragment"><div class="line"><span class="comment">// Per-pixel normal mapping Any number of lights, diffuse and specular</span></div><div class="line">material Examples/BumpMapping/MultiLightSpecular</div><div class="line">{</div><div class="line">    technique</div><div class="line">    {</div><div class="line">        <span class="comment">// Base ambient pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// ambient only, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 1 1 1</div><div class="line">            diffuse 0 0 0 </div><div class="line">            specular 0 0 0 0</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named_auto ambient ambient_light_colour</div><div class="line">            }</div><div class="line">        }</div><div class="line">        <span class="comment">// Now do the lighting pass</span></div><div class="line">        <span class="comment">// NB we don&#39;t do decal texture here because this is repeated per light</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            <span class="comment">// set ambient off, not needed for rendering, but as information</span></div><div class="line">            <span class="comment">// to lighting pass categorisation routine</span></div><div class="line">            ambient 0 0 0 </div><div class="line">            <span class="comment">// do this for each light</span></div><div class="line">            iteration once_per_light</div><div class="line">            scene_blend add</div><div class="line"></div><div class="line">            <span class="comment">// Vertex program reference</span></div><div class="line">            vertex_program_ref Examples/BumpMapVPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightPosition light_position_object_space 0</div><div class="line">                param_named_auto eyePosition camera_position_object_space</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Fragment program</span></div><div class="line">            fragment_program_ref Examples/BumpMapFPSpecular</div><div class="line">            {</div><div class="line">                param_named_auto lightDiffuse light_diffuse_colour 0 </div><div class="line">                param_named_auto lightSpecular light_specular_colour 0</div><div class="line">            }</div><div class="line"></div><div class="line">            <span class="comment">// Base bump map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture NMBumpsOut.png</div><div class="line">                colour_op replace</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">            <span class="comment">// Normalisation cube map #2</span></div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                cubic_texture nm.png combinedUVW</div><div class="line">                tex_coord_set 1</div><div class="line">                tex_address_mode clamp</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Decal pass</span></div><div class="line">        pass</div><div class="line">        {</div><div class="line">            lighting off</div><div class="line">            <span class="comment">// Really basic vertex program</span></div><div class="line">            vertex_program_ref <a class="code" href="namespace_ogre.html">Ogre</a>/BasicVertexPrograms/AmbientOneTexture</div><div class="line">            {</div><div class="line">                param_named_auto worldViewProj worldviewproj_matrix</div><div class="line">                param_named ambient float4 1 1 1 1</div><div class="line">            }</div><div class="line">            scene_blend dest_colour zero</div><div class="line">            texture_unit</div><div class="line">            {</div><div class="line">                texture RustedMetal.jpg </div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p>Note that if you’re using texture shadows you have the additional option of using <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> rather than being forced to use this explicit sequence - allowing you to compress the number of passes into a much smaller number at the expense of defining an upper number of shadow casting lights. In this case the ’additive’ aspect of the shadow technique just affects the colour of the shadow texture and it’s up to you to combine the shadow textures in your receivers however you like.</p>
<p><a class="anchor" id="Static-Lighting"></a></p>
<h2>Static Lighting</h2>
<p>Despite their power, additive lighting techniques have an additional limitation; they do not combine well with pre-calculated static lighting in the scene. This is because they are based on the principle that shadow is an absence of light, but since static lighting in the scene already includes areas of light and shadow, additive lighting cannot remove light to create new shadows. Therefore, if you use the additive lighting technique you must either use it exclusively as your lighting solution (and you can combine it with per-pixel lighting to create a very impressive dynamic lighting solution), or you must use <a href="#Integrated-Texture-Shadows">Integrated Texture Shadows</a> to combine the static lighting according to your chosen approach. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Animation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_animation.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Animation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>OGRE supports a pretty flexible animation system that allows you to script animation for several different purposes:</p>
<dl compact="compact">
<dt><a href="#Skeletal-Animation">Skeletal Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Mesh animation using a skeletal structure to determine how the mesh deforms. <br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Vertex-Animation">Vertex Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Mesh animation using snapshots of vertex data to determine how the shape of the mesh changes.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#SceneNode-Animation">SceneNode Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Animating SceneNodes automatically to create effects like camera sweeps, objects following predefined paths, etc.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Numeric-Value-Animation">Numeric Value Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Using OGRE’s extensible class structure to animate any value.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Skeletal-Animation"></a>
Skeletal Animation</h1>
<p>Skeletal animation is a process of animating a mesh by moving a set of hierarchical bones within the mesh, which in turn moves the vertices of the model according to the bone assignments stored in each vertex. An alternative term for this approach is ’skinning’. The usual way of creating these animations is with a modelling tool such as Softimage XSI, Milkshape 3D, Blender, 3D Studio or Maya among others. OGRE provides exporters to allow you to get the data out of these modellers and into the engine See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a>.</p>
<p>There are many grades of skeletal animation, and not all engines (or modellers for that matter) support all of them. OGRE supports the following features:</p>
<ul>
<li>Each mesh can be linked to a single skeleton</li>
<li>Unlimited bones per skeleton</li>
<li>Hierarchical forward-kinematics on bones</li>
<li>Multiple named animations per skeleton (e.g. ’Walk’, ’Run’, ’Jump’, ’Shoot’ etc)</li>
<li>Unlimited keyframes per animation</li>
<li>Linear or spline-based interpolation between keyframes</li>
<li>A vertex can be assigned to multiple bones and assigned weightings for smoother skinning</li>
<li>Multiple animations can be applied to a mesh at the same time, again with a blend weighting</li>
</ul>
<p><br />
</p>
<p>Skeletons and the animations which go with them are held in .skeleton files, which are produced by the OGRE exporters. These files are loaded automatically when you create an Entity based on a Mesh which is linked to the skeleton in question. You then use <a href="#Animation-State">Animation State</a> to set the use of animation on the entity in question.</p>
<p>Skeletal animation can be performed in software, or implemented in shaders (hardware skinning). Clearly the latter is preferable, since it takes some of the work away from the CPU and gives it to the graphics card, and also means that the vertex data does not need to be re-uploaded every frame. This is especially important for large, detailed models. You should try to use hardware skinning wherever possible; this basically means assigning a material which has a vertex program powered technique. See <a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a> for more details. Skeletal animation can be combined with vertex animation, See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>.</p>
<p><a class="anchor" id="Animation-State"></a> </p><h2>Animation State</h2>
<p>When an entity containing animation of any type is created, it is given an ’animation state’ object per animation to allow you to specify the animation state of that single entity (you can animate multiple entities using the same animation definitions, OGRE sorts the reuse out internally).</p>
<p>You can retrieve a pointer to the AnimationState object by calling Entity::getAnimationState. You can then call methods on this returned object to update the animation, probably in the frameStarted event. Each AnimationState needs to be enabled using the setEnabled method before the animation it refers to will take effect, and you can set both the weight and the time position (where appropriate) to affect the application of the animation using correlating methods. AnimationState also has a very simple method ’addTime’ which allows you to alter the animation position incrementally, and it will automatically loop for you. addTime can take positive or negative values (so you can reverse the animation if you want).</p>
<h1><a class="anchor" id="Vertex-Animation"></a>
Vertex Animation</h1>
<p>Vertex animation is about using information about the movement of vertices directly to animate the mesh. Each track in a vertex animation targets a single VertexData instance. Vertex animation is stored inside the .mesh file since it is tightly linked to the vertex structure of the mesh.</p>
<p>There are actually two subtypes of vertex animation, for reasons which will be discussed in a moment.</p>
<dl compact="compact">
<dt><a href="#Morph-Animation">Morph Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Morph animation is a very simple technique which interpolates mesh snapshots along a keyframe timeline. Morph animation has a direct correlation to old-school character animation techniques used before skeletal animation was widely used.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Pose-Animation">Pose Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Pose animation is about blending multiple discrete poses, expressed as offsets to the base vertex data, with different weights to provide a final result. Pose animation’s most obvious use is facial animation.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Why-two-subtypes_003f"></a></p>
<h2>Why two subtypes?</h2>
<p>So, why two subtypes of vertex animation? Couldn’t both be implemented using the same system? The short answer is yes; in fact you can implement both types using pose animation. But for very good reasons we decided to allow morph animation to be specified separately since the subset of features that it uses is both easier to define and has lower requirements on hardware shaders, if animation is implemented through them. If you don’t care about the reasons why these are implemented differently, you can skip to the next part.</p>
<p>Morph animation is a simple approach where we have a whole series of snapshots of vertex data which must be interpolated, e.g. a running animation implemented as morph targets. Because this is based on simple snapshots, it’s quite fast to use when animating an entire mesh because it’s a simple linear change between keyframes. However, this simplistic approach does not support blending between multiple morph animations. If you need animation blending, you are advised to use skeletal animation for full-mesh animation, and pose animation for animation of subsets of meshes or where skeletal animation doesn’t fit - for example facial animation. For animating in a vertex shader, morph animation is quite simple and just requires the 2 vertex buffers (one the original position buffer) of absolute position data, and an interpolation factor. Each track in a morph animation references a unique set of vertex data.</p>
<p>Pose animation is more complex. Like morph animation each track references a single unique set of vertex data, but unlike morph animation, each keyframe references 1 or more ’poses’, each with an influence level. A pose is a series of offsets to the base vertex data, and may be sparse - i.e. it may not reference every vertex. Because they’re offsets, they can be blended - both within a track and between animations. This set of features is very well suited to facial animation.</p>
<p>For example, let’s say you modelled a face (one set of vertex data), and defined a set of poses which represented the various phonetic positions of the face. You could then define an animation called ’SayHello’, containing a single track which referenced the face vertex data, and which included a series of keyframes, each of which referenced one or more of the facial positions at different influence levels - the combination of which over time made the face form the shapes required to say the word ’hello’. Since the poses are only stored once, but can be referenced may times in many animations, this is a very powerful way to build up a speech system.</p>
<p>The downside of pose animation is that it can be more difficult to set up, requiring poses to be separately defined and then referenced in the keyframes. Also, since it uses more buffers (one for the base data, and one for each active pose), if you’re animating in hardware using vertex shaders you need to keep an eye on how many poses you’re blending at once. You define a maximum supported number in your vertex program definition, via the includes_pose_animation material script entry, See <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p>So, by partitioning the vertex animation approaches into 2, we keep the simple morph technique easy to use, whilst still allowing all the powerful techniques to be used. Note that morph animation cannot be blended with other types of vertex animation on the same vertex data (pose animation or other morph animation); pose animation can be blended with other pose animation though, and both types can be combined with skeletal animation. This combination limitation applies per set of vertex data though, not globally across the mesh (see below). Also note that all morph animation can be expressed (in a more complex fashion) as pose animation, but not vice versa.</p>
<p><a class="anchor" id="Subtype-applies-per-track"></a></p>
<h2>Subtype applies per track</h2>
<p>It’s important to note that the subtype in question is held at a track level, not at the animation or mesh level. Since tracks map onto VertexData instances, this means that if your mesh is split into SubMeshes, each with their own dedicated geometry, you can have one SubMesh animated using pose animation, and others animated with morph animation (or not vertex animated at all).</p>
<p>For example, a common set-up for a complex character which needs both skeletal and facial animation might be to split the head into a separate SubMesh with its own geometry, then apply skeletal animation to both submeshes, and pose animation to just the head.</p>
<p>To see how to apply vertex animation, See <a href="#Animation-State">Animation State</a>.</p>
<p><a class="anchor" id="Vertex-buffer-arrangements"></a></p>
<h2>Vertex buffer arrangements</h2>
<p>When using vertex animation in software, vertex buffers need to be arranged such that vertex positions reside in their own hardware buffer. This is to avoid having to upload all the other vertex data when updating, which would quickly saturate the GPU bus. When using the OGRE .mesh format and the tools / exporters that go with it, OGRE organises this for you automatically. But if you create buffers yourself, you need to be aware of the layout arrangements.</p>
<p>To do this, you have a set of helper functions in <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a>. See API Reference entries for <a class="el" href="class_ogre_1_1_vertex_data.html#a106603d9d4d6c2f6e0d5eb6c25eb7c32" title="Reorganises the data in the vertex buffers according to the new vertex declaration passed in...">Ogre::VertexData::reorganiseBuffers()</a> and <a class="el" href="class_ogre_1_1_vertex_declaration.html#a130fa715330d23d0e9d2feef630b460c" title="Generates a new VertexDeclaration for optimal usage based on the current vertex declaration, which can be used with VertexData::reorganiseBuffers later if you wish, or simply used as a template. ">Ogre::VertexDeclaration::getAutoOrganisedDeclaration()</a>. The latter will turn a vertex declaration into one which is recommended for the usage you’ve indicated, and the former will reorganise the contents of a set of buffers to conform to that layout.</p>
<p><a class="anchor" id="Morph-Animation"></a> <a class="anchor" id="Morph-Animation-1"></a></p>
<h2>Morph Animation</h2>
<p>Morph animation works by storing snapshots of the absolute vertex positions in each keyframe, and interpolating between them. Morph animation is mainly useful for animating objects which could not be adequately handled using skeletal animation; this is mostly objects that have to radically change structure and shape as part of the animation such that a skeletal structure isn’t appropriate.</p>
<p>Because absolute positions are used, it is not possible to blend more than one morph animation on the same vertex data; you should use skeletal animation if you want to include animation blending since it is much more efficient. If you activate more than one animation which includes morph tracks for the same vertex data, only the last one will actually take effect. This also means that the ’weight’ option on the animation state is not used for morph animation.</p>
<p>Morph animation can be combined with skeletal animation if required See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>. Morph animation can also be implemented in hardware using vertex shaders, See <a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a>.</p>
<p><a class="anchor" id="Pose-Animation"></a> <a class="anchor" id="Pose-Animation-1"></a></p>
<h2>Pose Animation</h2>
<p>Pose animation allows you to blend together potentially multiple vertex poses at different influence levels into final vertex state. A common use for this is facial animation, where each facial expression is placed in a separate animation, and influences used to either blend from one expression to another, or to combine full expressions if each pose only affects part of the face.</p>
<p>In order to do this, pose animation uses a set of reference poses defined in the mesh, expressed as offsets to the original vertex data. It does not require that every vertex has an offset - those that don’t are left alone. When blending in software these vertices are completely skipped - when blending in hardware (which requires a vertex entry for every vertex), zero offsets for vertices which are not mentioned are automatically created for you.</p>
<p>Once you’ve defined the poses, you can refer to them in animations. Each pose animation track refers to a single set of geometry (either the shared geometry of the mesh, or dedicated geometry on a submesh), and each keyframe in the track can refer to one or more poses, each with its own influence level. The weight applied to the entire animation scales these influence levels too. You can define many keyframes which cause the blend of poses to change over time. The absence of a pose reference in a keyframe when it is present in a neighbouring one causes it to be treated as an influence of 0 for interpolation.</p>
<p>You should be careful how many poses you apply at once. When performing pose animation in hardware (See [Pose Animation in Vertex Programs](@ ref Pose-Animation-in-Vertex-Programs)), every active pose requires another vertex buffer to be added to the shader, and in when animating in software it will also take longer the more active poses you have. Bear in mind that if you have 2 poses in one keyframe, and a different 2 in the next, that actually means there are 4 active keyframes when interpolating between them.</p>
<p>You can combine pose animation with skeletal animation, See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>, and you can also hardware accelerate the application of the blend with a vertex shader, See <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p><a class="anchor" id="Combining-Skeletal-and-Vertex-Animation"></a> <a class="anchor" id="Combining-Skeletal-and-Vertex-Animation-1"></a></p>
<h2>Combining Skeletal and Vertex Animation</h2>
<p>Skeletal animation and vertex animation (of either subtype) can both be enabled on the same entity at the same time (See <a href="#Animation-State">Animation State</a>). The effect of this is that vertex animation is applied first to the base mesh, then skeletal animation is applied to the result. This allows you, for example, to facially animate a character using pose vertex animation, whilst performing the main movement animation using skeletal animation.</p>
<p>Combining the two is, from a user perspective, as simple as just enabling both animations at the same time. When it comes to using this feature efficiently though, there are a few points to bear in mind:</p>
<ul>
<li><a href="#Combined-Hardware-Skinning">Combined Hardware Skinning</a></li>
<li><a href="#Submesh-Splits">Submesh Splits</a></li>
</ul>
<p><a class="anchor" id="Combined-Hardware-Skinning"></a><a class="anchor" id="Combined-Hardware-Skinning-1"></a></p>
<h2>Combined Hardware Skinning</h2>
<p>For complex characters it is a very good idea to implement hardware skinning by including a technique in your materials which has a vertex program which can perform the kinds of animation you are using in hardware. See <a class="el" href="_high-level-_programs.html#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a>, <a class="el" href="_high-level-_programs.html#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a>, <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p>When combining animation types, your vertex programs must support both types of animation that the combined mesh needs, otherwise hardware skinning will be disabled. You should implement the animation in the same way that OGRE does, i.e. perform vertex animation first, then apply skeletal animation to the result of that. Remember that the implementation of morph animation passes 2 absolute snapshot buffers of the from &amp; to keyframes, along with a single parametric, which you have to linearly interpolate, whilst pose animation passes the base vertex data plus ’n’ pose offset buffers, and ’n’ parametric weight values.</p>
<p><a class="anchor" id="Submesh-Splits"></a><a class="anchor" id="Submesh-Splits-1"></a></p>
<h2>Submesh Splits</h2>
<p>If you only need to combine vertex and skeletal animation for a small part of your mesh, e.g. the face, you could split your mesh into 2 parts, one which needs the combination and one which does not, to reduce the calculation overhead. Note that it will also reduce vertex buffer usage since vertex keyframe / pose buffers will also be smaller. Note that if you use hardware skinning you should then implement 2 separate vertex programs, one which does only skeletal animation, and the other which does skeletal and vertex animation.</p>
<h1><a class="anchor" id="SceneNode-Animation"></a>
SceneNode Animation</h1>
<p>SceneNode animation is created from the SceneManager in order to animate the movement of SceneNodes, to make any attached objects move around automatically. You can see this performing a camera swoop in Demo_CameraTrack, or controlling how the fish move around in the pond in Demo_Fresnel.</p>
<p>At it’s heart, scene node animation is mostly the same code which animates the underlying skeleton in skeletal animation. After creating the main Animation using SceneManager::createAnimation you can create a NodeAnimationTrack per SceneNode that you want to animate, and create keyframes which control its position, orientation and scale which can be interpolated linearly or via splines. You use <a href="#Animation-State">Animation State</a> in the same way as you do for skeletal/vertex animation, except you obtain the state from SceneManager instead of from an individual Entity.Animations are applied automatically every frame, or the state can be applied manually in advance using the _applySceneAnimations() method on SceneManager. See the API reference for full details of the interface for configuring scene animations.</p>
<h1><a class="anchor" id="Numeric-Value-Animation"></a>
Numeric Value Animation</h1>
<p>Apart from the specific animation types which may well comprise the most common uses of the animation framework, you can also use animations to alter any value which is exposed via the <a href="#AnimableObject">AnimableObject</a> interface.</p>
<p><a class="anchor" id="AnimableObject"></a><a class="anchor" id="AnimableObject-1"></a></p>
<h2>AnimableObject</h2>
<p>AnimableObject is an abstract interface that any class can extend in order to provide access to a number of <a href="#AnimableValue">AnimableValue</a>s. It holds a ’dictionary’ of the available animable properties which can be enumerated via the getAnimableValueNames method, and when its createAnimableValue method is called, it returns a reference to a value object which forms a bridge between the generic animation interfaces, and the underlying specific object property.</p>
<p>One example of this is the Light class. It extends AnimableObject and provides AnimableValues for properties such as "diffuseColour" and "attenuation". Animation tracks can be created for these values and thus properties of the light can be scripted to change. Other objects, including your custom objects, can extend this interface in the same way to provide animation support to their properties.</p>
<p><a class="anchor" id="AnimableValue"></a><a class="anchor" id="AnimableValue-1"></a></p>
<h2>AnimableValue</h2>
<p>When implementing custom animable properties, you have to also implement a number of methods on the AnimableValue interface - basically anything which has been marked as unimplemented. These are not pure virtual methods simply because you only have to implement the methods required for the type of value you’re animating. Again, see the examples in Light to see how this is done. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Animation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_animation.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Animation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>OGRE supports a pretty flexible animation system that allows you to script animation for several different purposes:</p>
<dl compact="compact">
<dt><a href="#Skeletal-Animation">Skeletal Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Mesh animation using a skeletal structure to determine how the mesh deforms. <br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Vertex-Animation">Vertex Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Mesh animation using snapshots of vertex data to determine how the shape of the mesh changes.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#SceneNode-Animation">SceneNode Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Animating SceneNodes automatically to create effects like camera sweeps, objects following predefined paths, etc.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Numeric-Value-Animation">Numeric Value Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Using OGRE’s extensible class structure to animate any value.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Skeletal-Animation"></a>
Skeletal Animation</h1>
<p>Skeletal animation is a process of animating a mesh by moving a set of hierarchical bones within the mesh, which in turn moves the vertices of the model according to the bone assignments stored in each vertex. An alternative term for this approach is ’skinning’. The usual way of creating these animations is with a modelling tool such as Softimage XSI, Milkshape 3D, Blender, 3D Studio or Maya among others. OGRE provides exporters to allow you to get the data out of these modellers and into the engine See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a>.</p>
<p>There are many grades of skeletal animation, and not all engines (or modellers for that matter) support all of them. OGRE supports the following features:</p>
<ul>
<li>Each mesh can be linked to a single skeleton</li>
<li>Unlimited bones per skeleton</li>
<li>Hierarchical forward-kinematics on bones</li>
<li>Multiple named animations per skeleton (e.g. ’Walk’, ’Run’, ’Jump’, ’Shoot’ etc)</li>
<li>Unlimited keyframes per animation</li>
<li>Linear or spline-based interpolation between keyframes</li>
<li>A vertex can be assigned to multiple bones and assigned weightings for smoother skinning</li>
<li>Multiple animations can be applied to a mesh at the same time, again with a blend weighting</li>
</ul>
<p><br />
</p>
<p>Skeletons and the animations which go with them are held in .skeleton files, which are produced by the OGRE exporters. These files are loaded automatically when you create an Entity based on a Mesh which is linked to the skeleton in question. You then use <a href="#Animation-State">Animation State</a> to set the use of animation on the entity in question.</p>
<p>Skeletal animation can be performed in software, or implemented in shaders (hardware skinning). Clearly the latter is preferable, since it takes some of the work away from the CPU and gives it to the graphics card, and also means that the vertex data does not need to be re-uploaded every frame. This is especially important for large, detailed models. You should try to use hardware skinning wherever possible; this basically means assigning a material which has a vertex program powered technique. See <a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a> for more details. Skeletal animation can be combined with vertex animation, See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>.</p>
<p><a class="anchor" id="Animation-State"></a> </p><h2>Animation State</h2>
<p>When an entity containing animation of any type is created, it is given an ’animation state’ object per animation to allow you to specify the animation state of that single entity (you can animate multiple entities using the same animation definitions, OGRE sorts the reuse out internally).</p>
<p>You can retrieve a pointer to the AnimationState object by calling Entity::getAnimationState. You can then call methods on this returned object to update the animation, probably in the frameStarted event. Each AnimationState needs to be enabled using the setEnabled method before the animation it refers to will take effect, and you can set both the weight and the time position (where appropriate) to affect the application of the animation using correlating methods. AnimationState also has a very simple method ’addTime’ which allows you to alter the animation position incrementally, and it will automatically loop for you. addTime can take positive or negative values (so you can reverse the animation if you want).</p>
<h1><a class="anchor" id="Vertex-Animation"></a>
Vertex Animation</h1>
<p>Vertex animation is about using information about the movement of vertices directly to animate the mesh. Each track in a vertex animation targets a single VertexData instance. Vertex animation is stored inside the .mesh file since it is tightly linked to the vertex structure of the mesh.</p>
<p>There are actually two subtypes of vertex animation, for reasons which will be discussed in a moment.</p>
<dl compact="compact">
<dt><a href="#Morph-Animation">Morph Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Morph animation is a very simple technique which interpolates mesh snapshots along a keyframe timeline. Morph animation has a direct correlation to old-school character animation techniques used before skeletal animation was widely used.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Pose-Animation">Pose Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Pose animation is about blending multiple discrete poses, expressed as offsets to the base vertex data, with different weights to provide a final result. Pose animation’s most obvious use is facial animation.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Why-two-subtypes_003f"></a></p>
<h2>Why two subtypes?</h2>
<p>So, why two subtypes of vertex animation? Couldn’t both be implemented using the same system? The short answer is yes; in fact you can implement both types using pose animation. But for very good reasons we decided to allow morph animation to be specified separately since the subset of features that it uses is both easier to define and has lower requirements on hardware shaders, if animation is implemented through them. If you don’t care about the reasons why these are implemented differently, you can skip to the next part.</p>
<p>Morph animation is a simple approach where we have a whole series of snapshots of vertex data which must be interpolated, e.g. a running animation implemented as morph targets. Because this is based on simple snapshots, it’s quite fast to use when animating an entire mesh because it’s a simple linear change between keyframes. However, this simplistic approach does not support blending between multiple morph animations. If you need animation blending, you are advised to use skeletal animation for full-mesh animation, and pose animation for animation of subsets of meshes or where skeletal animation doesn’t fit - for example facial animation. For animating in a vertex shader, morph animation is quite simple and just requires the 2 vertex buffers (one the original position buffer) of absolute position data, and an interpolation factor. Each track in a morph animation references a unique set of vertex data.</p>
<p>Pose animation is more complex. Like morph animation each track references a single unique set of vertex data, but unlike morph animation, each keyframe references 1 or more ’poses’, each with an influence level. A pose is a series of offsets to the base vertex data, and may be sparse - i.e. it may not reference every vertex. Because they’re offsets, they can be blended - both within a track and between animations. This set of features is very well suited to facial animation.</p>
<p>For example, let’s say you modelled a face (one set of vertex data), and defined a set of poses which represented the various phonetic positions of the face. You could then define an animation called ’SayHello’, containing a single track which referenced the face vertex data, and which included a series of keyframes, each of which referenced one or more of the facial positions at different influence levels - the combination of which over time made the face form the shapes required to say the word ’hello’. Since the poses are only stored once, but can be referenced may times in many animations, this is a very powerful way to build up a speech system.</p>
<p>The downside of pose animation is that it can be more difficult to set up, requiring poses to be separately defined and then referenced in the keyframes. Also, since it uses more buffers (one for the base data, and one for each active pose), if you’re animating in hardware using vertex shaders you need to keep an eye on how many poses you’re blending at once. You define a maximum supported number in your vertex program definition, via the includes_pose_animation material script entry, See <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p>So, by partitioning the vertex animation approaches into 2, we keep the simple morph technique easy to use, whilst still allowing all the powerful techniques to be used. Note that morph animation cannot be blended with other types of vertex animation on the same vertex data (pose animation or other morph animation); pose animation can be blended with other pose animation though, and both types can be combined with skeletal animation. This combination limitation applies per set of vertex data though, not globally across the mesh (see below). Also note that all morph animation can be expressed (in a more complex fashion) as pose animation, but not vice versa.</p>
<p><a class="anchor" id="Subtype-applies-per-track"></a></p>
<h2>Subtype applies per track</h2>
<p>It’s important to note that the subtype in question is held at a track level, not at the animation or mesh level. Since tracks map onto VertexData instances, this means that if your mesh is split into SubMeshes, each with their own dedicated geometry, you can have one SubMesh animated using pose animation, and others animated with morph animation (or not vertex animated at all).</p>
<p>For example, a common set-up for a complex character which needs both skeletal and facial animation might be to split the head into a separate SubMesh with its own geometry, then apply skeletal animation to both submeshes, and pose animation to just the head.</p>
<p>To see how to apply vertex animation, See <a href="#Animation-State">Animation State</a>.</p>
<p><a class="anchor" id="Vertex-buffer-arrangements"></a></p>
<h2>Vertex buffer arrangements</h2>
<p>When using vertex animation in software, vertex buffers need to be arranged such that vertex positions reside in their own hardware buffer. This is to avoid having to upload all the other vertex data when updating, which would quickly saturate the GPU bus. When using the OGRE .mesh format and the tools / exporters that go with it, OGRE organises this for you automatically. But if you create buffers yourself, you need to be aware of the layout arrangements.</p>
<p>To do this, you have a set of helper functions in <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a>. See API Reference entries for <a class="el" href="class_ogre_1_1_vertex_data.html#a106603d9d4d6c2f6e0d5eb6c25eb7c32" title="Reorganises the data in the vertex buffers according to the new vertex declaration passed in...">Ogre::VertexData::reorganiseBuffers()</a> and <a class="el" href="class_ogre_1_1_vertex_declaration.html#a130fa715330d23d0e9d2feef630b460c" title="Generates a new VertexDeclaration for optimal usage based on the current vertex declaration, which can be used with VertexData::reorganiseBuffers later if you wish, or simply used as a template. ">Ogre::VertexDeclaration::getAutoOrganisedDeclaration()</a>. The latter will turn a vertex declaration into one which is recommended for the usage you’ve indicated, and the former will reorganise the contents of a set of buffers to conform to that layout.</p>
<p><a class="anchor" id="Morph-Animation"></a> <a class="anchor" id="Morph-Animation-1"></a></p>
<h2>Morph Animation</h2>
<p>Morph animation works by storing snapshots of the absolute vertex positions in each keyframe, and interpolating between them. Morph animation is mainly useful for animating objects which could not be adequately handled using skeletal animation; this is mostly objects that have to radically change structure and shape as part of the animation such that a skeletal structure isn’t appropriate.</p>
<p>Because absolute positions are used, it is not possible to blend more than one morph animation on the same vertex data; you should use skeletal animation if you want to include animation blending since it is much more efficient. If you activate more than one animation which includes morph tracks for the same vertex data, only the last one will actually take effect. This also means that the ’weight’ option on the animation state is not used for morph animation.</p>
<p>Morph animation can be combined with skeletal animation if required See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>. Morph animation can also be implemented in hardware using vertex shaders, See <a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a>.</p>
<p><a class="anchor" id="Pose-Animation"></a> <a class="anchor" id="Pose-Animation-1"></a></p>
<h2>Pose Animation</h2>
<p>Pose animation allows you to blend together potentially multiple vertex poses at different influence levels into final vertex state. A common use for this is facial animation, where each facial expression is placed in a separate animation, and influences used to either blend from one expression to another, or to combine full expressions if each pose only affects part of the face.</p>
<p>In order to do this, pose animation uses a set of reference poses defined in the mesh, expressed as offsets to the original vertex data. It does not require that every vertex has an offset - those that don’t are left alone. When blending in software these vertices are completely skipped - when blending in hardware (which requires a vertex entry for every vertex), zero offsets for vertices which are not mentioned are automatically created for you.</p>
<p>Once you’ve defined the poses, you can refer to them in animations. Each pose animation track refers to a single set of geometry (either the shared geometry of the mesh, or dedicated geometry on a submesh), and each keyframe in the track can refer to one or more poses, each with its own influence level. The weight applied to the entire animation scales these influence levels too. You can define many keyframes which cause the blend of poses to change over time. The absence of a pose reference in a keyframe when it is present in a neighbouring one causes it to be treated as an influence of 0 for interpolation.</p>
<p>You should be careful how many poses you apply at once. When performing pose animation in hardware (See [Pose Animation in Vertex Programs](@ ref Pose-Animation-in-Vertex-Programs)), every active pose requires another vertex buffer to be added to the shader, and in when animating in software it will also take longer the more active poses you have. Bear in mind that if you have 2 poses in one keyframe, and a different 2 in the next, that actually means there are 4 active keyframes when interpolating between them.</p>
<p>You can combine pose animation with skeletal animation, See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>, and you can also hardware accelerate the application of the blend with a vertex shader, See <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p><a class="anchor" id="Combining-Skeletal-and-Vertex-Animation"></a> <a class="anchor" id="Combining-Skeletal-and-Vertex-Animation-1"></a></p>
<h2>Combining Skeletal and Vertex Animation</h2>
<p>Skeletal animation and vertex animation (of either subtype) can both be enabled on the same entity at the same time (See <a href="#Animation-State">Animation State</a>). The effect of this is that vertex animation is applied first to the base mesh, then skeletal animation is applied to the result. This allows you, for example, to facially animate a character using pose vertex animation, whilst performing the main movement animation using skeletal animation.</p>
<p>Combining the two is, from a user perspective, as simple as just enabling both animations at the same time. When it comes to using this feature efficiently though, there are a few points to bear in mind:</p>
<ul>
<li><a href="#Combined-Hardware-Skinning">Combined Hardware Skinning</a></li>
<li><a href="#Submesh-Splits">Submesh Splits</a></li>
</ul>
<p><a class="anchor" id="Combined-Hardware-Skinning"></a><a class="anchor" id="Combined-Hardware-Skinning-1"></a></p>
<h2>Combined Hardware Skinning</h2>
<p>For complex characters it is a very good idea to implement hardware skinning by including a technique in your materials which has a vertex program which can perform the kinds of animation you are using in hardware. See <a class="el" href="_high-level-_programs.html#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a>, <a class="el" href="_high-level-_programs.html#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a>, <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p>When combining animation types, your vertex programs must support both types of animation that the combined mesh needs, otherwise hardware skinning will be disabled. You should implement the animation in the same way that OGRE does, i.e. perform vertex animation first, then apply skeletal animation to the result of that. Remember that the implementation of morph animation passes 2 absolute snapshot buffers of the from &amp; to keyframes, along with a single parametric, which you have to linearly interpolate, whilst pose animation passes the base vertex data plus ’n’ pose offset buffers, and ’n’ parametric weight values.</p>
<p><a class="anchor" id="Submesh-Splits"></a><a class="anchor" id="Submesh-Splits-1"></a></p>
<h2>Submesh Splits</h2>
<p>If you only need to combine vertex and skeletal animation for a small part of your mesh, e.g. the face, you could split your mesh into 2 parts, one which needs the combination and one which does not, to reduce the calculation overhead. Note that it will also reduce vertex buffer usage since vertex keyframe / pose buffers will also be smaller. Note that if you use hardware skinning you should then implement 2 separate vertex programs, one which does only skeletal animation, and the other which does skeletal and vertex animation.</p>
<h1><a class="anchor" id="SceneNode-Animation"></a>
SceneNode Animation</h1>
<p>SceneNode animation is created from the SceneManager in order to animate the movement of SceneNodes, to make any attached objects move around automatically. You can see this performing a camera swoop in Demo_CameraTrack, or controlling how the fish move around in the pond in Demo_Fresnel.</p>
<p>At it’s heart, scene node animation is mostly the same code which animates the underlying skeleton in skeletal animation. After creating the main Animation using SceneManager::createAnimation you can create a NodeAnimationTrack per SceneNode that you want to animate, and create keyframes which control its position, orientation and scale which can be interpolated linearly or via splines. You use <a href="#Animation-State">Animation State</a> in the same way as you do for skeletal/vertex animation, except you obtain the state from SceneManager instead of from an individual Entity.Animations are applied automatically every frame, or the state can be applied manually in advance using the _applySceneAnimations() method on SceneManager. See the API reference for full details of the interface for configuring scene animations.</p>
<h1><a class="anchor" id="Numeric-Value-Animation"></a>
Numeric Value Animation</h1>
<p>Apart from the specific animation types which may well comprise the most common uses of the animation framework, you can also use animations to alter any value which is exposed via the <a href="#AnimableObject">AnimableObject</a> interface.</p>
<p><a class="anchor" id="AnimableObject"></a><a class="anchor" id="AnimableObject-1"></a></p>
<h2>AnimableObject</h2>
<p>AnimableObject is an abstract interface that any class can extend in order to provide access to a number of <a href="#AnimableValue">AnimableValue</a>s. It holds a ’dictionary’ of the available animable properties which can be enumerated via the getAnimableValueNames method, and when its createAnimableValue method is called, it returns a reference to a value object which forms a bridge between the generic animation interfaces, and the underlying specific object property.</p>
<p>One example of this is the Light class. It extends AnimableObject and provides AnimableValues for properties such as "diffuseColour" and "attenuation". Animation tracks can be created for these values and thus properties of the light can be scripted to change. Other objects, including your custom objects, can extend this interface in the same way to provide animation support to their properties.</p>
<p><a class="anchor" id="AnimableValue"></a><a class="anchor" id="AnimableValue-1"></a></p>
<h2>AnimableValue</h2>
<p>When implementing custom animable properties, you have to also implement a number of methods on the AnimableValue interface - basically anything which has been marked as unimplemented. These are not pure virtual methods simply because you only have to implement the methods required for the type of value you’re animating. Again, see the examples in Light to see how this is done. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Animation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_animation.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Animation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>OGRE supports a pretty flexible animation system that allows you to script animation for several different purposes:</p>
<dl compact="compact">
<dt><a href="#Skeletal-Animation">Skeletal Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Mesh animation using a skeletal structure to determine how the mesh deforms. <br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Vertex-Animation">Vertex Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Mesh animation using snapshots of vertex data to determine how the shape of the mesh changes.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#SceneNode-Animation">SceneNode Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Animating SceneNodes automatically to create effects like camera sweeps, objects following predefined paths, etc.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Numeric-Value-Animation">Numeric Value Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Using OGRE’s extensible class structure to animate any value.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Skeletal-Animation"></a>
Skeletal Animation</h1>
<p>Skeletal animation is a process of animating a mesh by moving a set of hierarchical bones within the mesh, which in turn moves the vertices of the model according to the bone assignments stored in each vertex. An alternative term for this approach is ’skinning’. The usual way of creating these animations is with a modelling tool such as Softimage XSI, Milkshape 3D, Blender, 3D Studio or Maya among others. OGRE provides exporters to allow you to get the data out of these modellers and into the engine See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a>.</p>
<p>There are many grades of skeletal animation, and not all engines (or modellers for that matter) support all of them. OGRE supports the following features:</p>
<ul>
<li>Each mesh can be linked to a single skeleton</li>
<li>Unlimited bones per skeleton</li>
<li>Hierarchical forward-kinematics on bones</li>
<li>Multiple named animations per skeleton (e.g. ’Walk’, ’Run’, ’Jump’, ’Shoot’ etc)</li>
<li>Unlimited keyframes per animation</li>
<li>Linear or spline-based interpolation between keyframes</li>
<li>A vertex can be assigned to multiple bones and assigned weightings for smoother skinning</li>
<li>Multiple animations can be applied to a mesh at the same time, again with a blend weighting</li>
</ul>
<p><br />
</p>
<p>Skeletons and the animations which go with them are held in .skeleton files, which are produced by the OGRE exporters. These files are loaded automatically when you create an Entity based on a Mesh which is linked to the skeleton in question. You then use <a href="#Animation-State">Animation State</a> to set the use of animation on the entity in question.</p>
<p>Skeletal animation can be performed in software, or implemented in shaders (hardware skinning). Clearly the latter is preferable, since it takes some of the work away from the CPU and gives it to the graphics card, and also means that the vertex data does not need to be re-uploaded every frame. This is especially important for large, detailed models. You should try to use hardware skinning wherever possible; this basically means assigning a material which has a vertex program powered technique. See <a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a> for more details. Skeletal animation can be combined with vertex animation, See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>.</p>
<p><a class="anchor" id="Animation-State"></a> </p><h2>Animation State</h2>
<p>When an entity containing animation of any type is created, it is given an ’animation state’ object per animation to allow you to specify the animation state of that single entity (you can animate multiple entities using the same animation definitions, OGRE sorts the reuse out internally).</p>
<p>You can retrieve a pointer to the AnimationState object by calling Entity::getAnimationState. You can then call methods on this returned object to update the animation, probably in the frameStarted event. Each AnimationState needs to be enabled using the setEnabled method before the animation it refers to will take effect, and you can set both the weight and the time position (where appropriate) to affect the application of the animation using correlating methods. AnimationState also has a very simple method ’addTime’ which allows you to alter the animation position incrementally, and it will automatically loop for you. addTime can take positive or negative values (so you can reverse the animation if you want).</p>
<h1><a class="anchor" id="Vertex-Animation"></a>
Vertex Animation</h1>
<p>Vertex animation is about using information about the movement of vertices directly to animate the mesh. Each track in a vertex animation targets a single VertexData instance. Vertex animation is stored inside the .mesh file since it is tightly linked to the vertex structure of the mesh.</p>
<p>There are actually two subtypes of vertex animation, for reasons which will be discussed in a moment.</p>
<dl compact="compact">
<dt><a href="#Morph-Animation">Morph Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Morph animation is a very simple technique which interpolates mesh snapshots along a keyframe timeline. Morph animation has a direct correlation to old-school character animation techniques used before skeletal animation was widely used.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Pose-Animation">Pose Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Pose animation is about blending multiple discrete poses, expressed as offsets to the base vertex data, with different weights to provide a final result. Pose animation’s most obvious use is facial animation.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Why-two-subtypes_003f"></a></p>
<h2>Why two subtypes?</h2>
<p>So, why two subtypes of vertex animation? Couldn’t both be implemented using the same system? The short answer is yes; in fact you can implement both types using pose animation. But for very good reasons we decided to allow morph animation to be specified separately since the subset of features that it uses is both easier to define and has lower requirements on hardware shaders, if animation is implemented through them. If you don’t care about the reasons why these are implemented differently, you can skip to the next part.</p>
<p>Morph animation is a simple approach where we have a whole series of snapshots of vertex data which must be interpolated, e.g. a running animation implemented as morph targets. Because this is based on simple snapshots, it’s quite fast to use when animating an entire mesh because it’s a simple linear change between keyframes. However, this simplistic approach does not support blending between multiple morph animations. If you need animation blending, you are advised to use skeletal animation for full-mesh animation, and pose animation for animation of subsets of meshes or where skeletal animation doesn’t fit - for example facial animation. For animating in a vertex shader, morph animation is quite simple and just requires the 2 vertex buffers (one the original position buffer) of absolute position data, and an interpolation factor. Each track in a morph animation references a unique set of vertex data.</p>
<p>Pose animation is more complex. Like morph animation each track references a single unique set of vertex data, but unlike morph animation, each keyframe references 1 or more ’poses’, each with an influence level. A pose is a series of offsets to the base vertex data, and may be sparse - i.e. it may not reference every vertex. Because they’re offsets, they can be blended - both within a track and between animations. This set of features is very well suited to facial animation.</p>
<p>For example, let’s say you modelled a face (one set of vertex data), and defined a set of poses which represented the various phonetic positions of the face. You could then define an animation called ’SayHello’, containing a single track which referenced the face vertex data, and which included a series of keyframes, each of which referenced one or more of the facial positions at different influence levels - the combination of which over time made the face form the shapes required to say the word ’hello’. Since the poses are only stored once, but can be referenced may times in many animations, this is a very powerful way to build up a speech system.</p>
<p>The downside of pose animation is that it can be more difficult to set up, requiring poses to be separately defined and then referenced in the keyframes. Also, since it uses more buffers (one for the base data, and one for each active pose), if you’re animating in hardware using vertex shaders you need to keep an eye on how many poses you’re blending at once. You define a maximum supported number in your vertex program definition, via the includes_pose_animation material script entry, See <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p>So, by partitioning the vertex animation approaches into 2, we keep the simple morph technique easy to use, whilst still allowing all the powerful techniques to be used. Note that morph animation cannot be blended with other types of vertex animation on the same vertex data (pose animation or other morph animation); pose animation can be blended with other pose animation though, and both types can be combined with skeletal animation. This combination limitation applies per set of vertex data though, not globally across the mesh (see below). Also note that all morph animation can be expressed (in a more complex fashion) as pose animation, but not vice versa.</p>
<p><a class="anchor" id="Subtype-applies-per-track"></a></p>
<h2>Subtype applies per track</h2>
<p>It’s important to note that the subtype in question is held at a track level, not at the animation or mesh level. Since tracks map onto VertexData instances, this means that if your mesh is split into SubMeshes, each with their own dedicated geometry, you can have one SubMesh animated using pose animation, and others animated with morph animation (or not vertex animated at all).</p>
<p>For example, a common set-up for a complex character which needs both skeletal and facial animation might be to split the head into a separate SubMesh with its own geometry, then apply skeletal animation to both submeshes, and pose animation to just the head.</p>
<p>To see how to apply vertex animation, See <a href="#Animation-State">Animation State</a>.</p>
<p><a class="anchor" id="Vertex-buffer-arrangements"></a></p>
<h2>Vertex buffer arrangements</h2>
<p>When using vertex animation in software, vertex buffers need to be arranged such that vertex positions reside in their own hardware buffer. This is to avoid having to upload all the other vertex data when updating, which would quickly saturate the GPU bus. When using the OGRE .mesh format and the tools / exporters that go with it, OGRE organises this for you automatically. But if you create buffers yourself, you need to be aware of the layout arrangements.</p>
<p>To do this, you have a set of helper functions in <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a>. See API Reference entries for <a class="el" href="class_ogre_1_1_vertex_data.html#a106603d9d4d6c2f6e0d5eb6c25eb7c32" title="Reorganises the data in the vertex buffers according to the new vertex declaration passed in...">Ogre::VertexData::reorganiseBuffers()</a> and <a class="el" href="class_ogre_1_1_vertex_declaration.html#a130fa715330d23d0e9d2feef630b460c" title="Generates a new VertexDeclaration for optimal usage based on the current vertex declaration, which can be used with VertexData::reorganiseBuffers later if you wish, or simply used as a template. ">Ogre::VertexDeclaration::getAutoOrganisedDeclaration()</a>. The latter will turn a vertex declaration into one which is recommended for the usage you’ve indicated, and the former will reorganise the contents of a set of buffers to conform to that layout.</p>
<p><a class="anchor" id="Morph-Animation"></a> <a class="anchor" id="Morph-Animation-1"></a></p>
<h2>Morph Animation</h2>
<p>Morph animation works by storing snapshots of the absolute vertex positions in each keyframe, and interpolating between them. Morph animation is mainly useful for animating objects which could not be adequately handled using skeletal animation; this is mostly objects that have to radically change structure and shape as part of the animation such that a skeletal structure isn’t appropriate.</p>
<p>Because absolute positions are used, it is not possible to blend more than one morph animation on the same vertex data; you should use skeletal animation if you want to include animation blending since it is much more efficient. If you activate more than one animation which includes morph tracks for the same vertex data, only the last one will actually take effect. This also means that the ’weight’ option on the animation state is not used for morph animation.</p>
<p>Morph animation can be combined with skeletal animation if required See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>. Morph animation can also be implemented in hardware using vertex shaders, See <a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a>.</p>
<p><a class="anchor" id="Pose-Animation"></a> <a class="anchor" id="Pose-Animation-1"></a></p>
<h2>Pose Animation</h2>
<p>Pose animation allows you to blend together potentially multiple vertex poses at different influence levels into final vertex state. A common use for this is facial animation, where each facial expression is placed in a separate animation, and influences used to either blend from one expression to another, or to combine full expressions if each pose only affects part of the face.</p>
<p>In order to do this, pose animation uses a set of reference poses defined in the mesh, expressed as offsets to the original vertex data. It does not require that every vertex has an offset - those that don’t are left alone. When blending in software these vertices are completely skipped - when blending in hardware (which requires a vertex entry for every vertex), zero offsets for vertices which are not mentioned are automatically created for you.</p>
<p>Once you’ve defined the poses, you can refer to them in animations. Each pose animation track refers to a single set of geometry (either the shared geometry of the mesh, or dedicated geometry on a submesh), and each keyframe in the track can refer to one or more poses, each with its own influence level. The weight applied to the entire animation scales these influence levels too. You can define many keyframes which cause the blend of poses to change over time. The absence of a pose reference in a keyframe when it is present in a neighbouring one causes it to be treated as an influence of 0 for interpolation.</p>
<p>You should be careful how many poses you apply at once. When performing pose animation in hardware (See [Pose Animation in Vertex Programs](@ ref Pose-Animation-in-Vertex-Programs)), every active pose requires another vertex buffer to be added to the shader, and in when animating in software it will also take longer the more active poses you have. Bear in mind that if you have 2 poses in one keyframe, and a different 2 in the next, that actually means there are 4 active keyframes when interpolating between them.</p>
<p>You can combine pose animation with skeletal animation, See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>, and you can also hardware accelerate the application of the blend with a vertex shader, See <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p><a class="anchor" id="Combining-Skeletal-and-Vertex-Animation"></a> <a class="anchor" id="Combining-Skeletal-and-Vertex-Animation-1"></a></p>
<h2>Combining Skeletal and Vertex Animation</h2>
<p>Skeletal animation and vertex animation (of either subtype) can both be enabled on the same entity at the same time (See <a href="#Animation-State">Animation State</a>). The effect of this is that vertex animation is applied first to the base mesh, then skeletal animation is applied to the result. This allows you, for example, to facially animate a character using pose vertex animation, whilst performing the main movement animation using skeletal animation.</p>
<p>Combining the two is, from a user perspective, as simple as just enabling both animations at the same time. When it comes to using this feature efficiently though, there are a few points to bear in mind:</p>
<ul>
<li><a href="#Combined-Hardware-Skinning">Combined Hardware Skinning</a></li>
<li><a href="#Submesh-Splits">Submesh Splits</a></li>
</ul>
<p><a class="anchor" id="Combined-Hardware-Skinning"></a><a class="anchor" id="Combined-Hardware-Skinning-1"></a></p>
<h2>Combined Hardware Skinning</h2>
<p>For complex characters it is a very good idea to implement hardware skinning by including a technique in your materials which has a vertex program which can perform the kinds of animation you are using in hardware. See <a class="el" href="_high-level-_programs.html#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a>, <a class="el" href="_high-level-_programs.html#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a>, <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p>When combining animation types, your vertex programs must support both types of animation that the combined mesh needs, otherwise hardware skinning will be disabled. You should implement the animation in the same way that OGRE does, i.e. perform vertex animation first, then apply skeletal animation to the result of that. Remember that the implementation of morph animation passes 2 absolute snapshot buffers of the from &amp; to keyframes, along with a single parametric, which you have to linearly interpolate, whilst pose animation passes the base vertex data plus ’n’ pose offset buffers, and ’n’ parametric weight values.</p>
<p><a class="anchor" id="Submesh-Splits"></a><a class="anchor" id="Submesh-Splits-1"></a></p>
<h2>Submesh Splits</h2>
<p>If you only need to combine vertex and skeletal animation for a small part of your mesh, e.g. the face, you could split your mesh into 2 parts, one which needs the combination and one which does not, to reduce the calculation overhead. Note that it will also reduce vertex buffer usage since vertex keyframe / pose buffers will also be smaller. Note that if you use hardware skinning you should then implement 2 separate vertex programs, one which does only skeletal animation, and the other which does skeletal and vertex animation.</p>
<h1><a class="anchor" id="SceneNode-Animation"></a>
SceneNode Animation</h1>
<p>SceneNode animation is created from the SceneManager in order to animate the movement of SceneNodes, to make any attached objects move around automatically. You can see this performing a camera swoop in Demo_CameraTrack, or controlling how the fish move around in the pond in Demo_Fresnel.</p>
<p>At it’s heart, scene node animation is mostly the same code which animates the underlying skeleton in skeletal animation. After creating the main Animation using SceneManager::createAnimation you can create a NodeAnimationTrack per SceneNode that you want to animate, and create keyframes which control its position, orientation and scale which can be interpolated linearly or via splines. You use <a href="#Animation-State">Animation State</a> in the same way as you do for skeletal/vertex animation, except you obtain the state from SceneManager instead of from an individual Entity.Animations are applied automatically every frame, or the state can be applied manually in advance using the _applySceneAnimations() method on SceneManager. See the API reference for full details of the interface for configuring scene animations.</p>
<h1><a class="anchor" id="Numeric-Value-Animation"></a>
Numeric Value Animation</h1>
<p>Apart from the specific animation types which may well comprise the most common uses of the animation framework, you can also use animations to alter any value which is exposed via the <a href="#AnimableObject">AnimableObject</a> interface.</p>
<p><a class="anchor" id="AnimableObject"></a><a class="anchor" id="AnimableObject-1"></a></p>
<h2>AnimableObject</h2>
<p>AnimableObject is an abstract interface that any class can extend in order to provide access to a number of <a href="#AnimableValue">AnimableValue</a>s. It holds a ’dictionary’ of the available animable properties which can be enumerated via the getAnimableValueNames method, and when its createAnimableValue method is called, it returns a reference to a value object which forms a bridge between the generic animation interfaces, and the underlying specific object property.</p>
<p>One example of this is the Light class. It extends AnimableObject and provides AnimableValues for properties such as "diffuseColour" and "attenuation". Animation tracks can be created for these values and thus properties of the light can be scripted to change. Other objects, including your custom objects, can extend this interface in the same way to provide animation support to their properties.</p>
<p><a class="anchor" id="AnimableValue"></a><a class="anchor" id="AnimableValue-1"></a></p>
<h2>AnimableValue</h2>
<p>When implementing custom animable properties, you have to also implement a number of methods on the AnimableValue interface - basically anything which has been marked as unimplemented. These are not pure virtual methods simply because you only have to implement the methods required for the type of value you’re animating. Again, see the examples in Light to see how this is done. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Animation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_animation.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Animation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>OGRE supports a pretty flexible animation system that allows you to script animation for several different purposes:</p>
<dl compact="compact">
<dt><a href="#Skeletal-Animation">Skeletal Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Mesh animation using a skeletal structure to determine how the mesh deforms. <br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Vertex-Animation">Vertex Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Mesh animation using snapshots of vertex data to determine how the shape of the mesh changes.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#SceneNode-Animation">SceneNode Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Animating SceneNodes automatically to create effects like camera sweeps, objects following predefined paths, etc.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Numeric-Value-Animation">Numeric Value Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Using OGRE’s extensible class structure to animate any value.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Skeletal-Animation"></a>
Skeletal Animation</h1>
<p>Skeletal animation is a process of animating a mesh by moving a set of hierarchical bones within the mesh, which in turn moves the vertices of the model according to the bone assignments stored in each vertex. An alternative term for this approach is ’skinning’. The usual way of creating these animations is with a modelling tool such as Softimage XSI, Milkshape 3D, Blender, 3D Studio or Maya among others. OGRE provides exporters to allow you to get the data out of these modellers and into the engine See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a>.</p>
<p>There are many grades of skeletal animation, and not all engines (or modellers for that matter) support all of them. OGRE supports the following features:</p>
<ul>
<li>Each mesh can be linked to a single skeleton</li>
<li>Unlimited bones per skeleton</li>
<li>Hierarchical forward-kinematics on bones</li>
<li>Multiple named animations per skeleton (e.g. ’Walk’, ’Run’, ’Jump’, ’Shoot’ etc)</li>
<li>Unlimited keyframes per animation</li>
<li>Linear or spline-based interpolation between keyframes</li>
<li>A vertex can be assigned to multiple bones and assigned weightings for smoother skinning</li>
<li>Multiple animations can be applied to a mesh at the same time, again with a blend weighting</li>
</ul>
<p><br />
</p>
<p>Skeletons and the animations which go with them are held in .skeleton files, which are produced by the OGRE exporters. These files are loaded automatically when you create an Entity based on a Mesh which is linked to the skeleton in question. You then use <a href="#Animation-State">Animation State</a> to set the use of animation on the entity in question.</p>
<p>Skeletal animation can be performed in software, or implemented in shaders (hardware skinning). Clearly the latter is preferable, since it takes some of the work away from the CPU and gives it to the graphics card, and also means that the vertex data does not need to be re-uploaded every frame. This is especially important for large, detailed models. You should try to use hardware skinning wherever possible; this basically means assigning a material which has a vertex program powered technique. See <a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a> for more details. Skeletal animation can be combined with vertex animation, See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>.</p>
<p><a class="anchor" id="Animation-State"></a> </p><h2>Animation State</h2>
<p>When an entity containing animation of any type is created, it is given an ’animation state’ object per animation to allow you to specify the animation state of that single entity (you can animate multiple entities using the same animation definitions, OGRE sorts the reuse out internally).</p>
<p>You can retrieve a pointer to the AnimationState object by calling Entity::getAnimationState. You can then call methods on this returned object to update the animation, probably in the frameStarted event. Each AnimationState needs to be enabled using the setEnabled method before the animation it refers to will take effect, and you can set both the weight and the time position (where appropriate) to affect the application of the animation using correlating methods. AnimationState also has a very simple method ’addTime’ which allows you to alter the animation position incrementally, and it will automatically loop for you. addTime can take positive or negative values (so you can reverse the animation if you want).</p>
<h1><a class="anchor" id="Vertex-Animation"></a>
Vertex Animation</h1>
<p>Vertex animation is about using information about the movement of vertices directly to animate the mesh. Each track in a vertex animation targets a single VertexData instance. Vertex animation is stored inside the .mesh file since it is tightly linked to the vertex structure of the mesh.</p>
<p>There are actually two subtypes of vertex animation, for reasons which will be discussed in a moment.</p>
<dl compact="compact">
<dt><a href="#Morph-Animation">Morph Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Morph animation is a very simple technique which interpolates mesh snapshots along a keyframe timeline. Morph animation has a direct correlation to old-school character animation techniques used before skeletal animation was widely used.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Pose-Animation">Pose Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Pose animation is about blending multiple discrete poses, expressed as offsets to the base vertex data, with different weights to provide a final result. Pose animation’s most obvious use is facial animation.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Why-two-subtypes_003f"></a></p>
<h2>Why two subtypes?</h2>
<p>So, why two subtypes of vertex animation? Couldn’t both be implemented using the same system? The short answer is yes; in fact you can implement both types using pose animation. But for very good reasons we decided to allow morph animation to be specified separately since the subset of features that it uses is both easier to define and has lower requirements on hardware shaders, if animation is implemented through them. If you don’t care about the reasons why these are implemented differently, you can skip to the next part.</p>
<p>Morph animation is a simple approach where we have a whole series of snapshots of vertex data which must be interpolated, e.g. a running animation implemented as morph targets. Because this is based on simple snapshots, it’s quite fast to use when animating an entire mesh because it’s a simple linear change between keyframes. However, this simplistic approach does not support blending between multiple morph animations. If you need animation blending, you are advised to use skeletal animation for full-mesh animation, and pose animation for animation of subsets of meshes or where skeletal animation doesn’t fit - for example facial animation. For animating in a vertex shader, morph animation is quite simple and just requires the 2 vertex buffers (one the original position buffer) of absolute position data, and an interpolation factor. Each track in a morph animation references a unique set of vertex data.</p>
<p>Pose animation is more complex. Like morph animation each track references a single unique set of vertex data, but unlike morph animation, each keyframe references 1 or more ’poses’, each with an influence level. A pose is a series of offsets to the base vertex data, and may be sparse - i.e. it may not reference every vertex. Because they’re offsets, they can be blended - both within a track and between animations. This set of features is very well suited to facial animation.</p>
<p>For example, let’s say you modelled a face (one set of vertex data), and defined a set of poses which represented the various phonetic positions of the face. You could then define an animation called ’SayHello’, containing a single track which referenced the face vertex data, and which included a series of keyframes, each of which referenced one or more of the facial positions at different influence levels - the combination of which over time made the face form the shapes required to say the word ’hello’. Since the poses are only stored once, but can be referenced may times in many animations, this is a very powerful way to build up a speech system.</p>
<p>The downside of pose animation is that it can be more difficult to set up, requiring poses to be separately defined and then referenced in the keyframes. Also, since it uses more buffers (one for the base data, and one for each active pose), if you’re animating in hardware using vertex shaders you need to keep an eye on how many poses you’re blending at once. You define a maximum supported number in your vertex program definition, via the includes_pose_animation material script entry, See <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p>So, by partitioning the vertex animation approaches into 2, we keep the simple morph technique easy to use, whilst still allowing all the powerful techniques to be used. Note that morph animation cannot be blended with other types of vertex animation on the same vertex data (pose animation or other morph animation); pose animation can be blended with other pose animation though, and both types can be combined with skeletal animation. This combination limitation applies per set of vertex data though, not globally across the mesh (see below). Also note that all morph animation can be expressed (in a more complex fashion) as pose animation, but not vice versa.</p>
<p><a class="anchor" id="Subtype-applies-per-track"></a></p>
<h2>Subtype applies per track</h2>
<p>It’s important to note that the subtype in question is held at a track level, not at the animation or mesh level. Since tracks map onto VertexData instances, this means that if your mesh is split into SubMeshes, each with their own dedicated geometry, you can have one SubMesh animated using pose animation, and others animated with morph animation (or not vertex animated at all).</p>
<p>For example, a common set-up for a complex character which needs both skeletal and facial animation might be to split the head into a separate SubMesh with its own geometry, then apply skeletal animation to both submeshes, and pose animation to just the head.</p>
<p>To see how to apply vertex animation, See <a href="#Animation-State">Animation State</a>.</p>
<p><a class="anchor" id="Vertex-buffer-arrangements"></a></p>
<h2>Vertex buffer arrangements</h2>
<p>When using vertex animation in software, vertex buffers need to be arranged such that vertex positions reside in their own hardware buffer. This is to avoid having to upload all the other vertex data when updating, which would quickly saturate the GPU bus. When using the OGRE .mesh format and the tools / exporters that go with it, OGRE organises this for you automatically. But if you create buffers yourself, you need to be aware of the layout arrangements.</p>
<p>To do this, you have a set of helper functions in <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a>. See API Reference entries for <a class="el" href="class_ogre_1_1_vertex_data.html#a106603d9d4d6c2f6e0d5eb6c25eb7c32" title="Reorganises the data in the vertex buffers according to the new vertex declaration passed in...">Ogre::VertexData::reorganiseBuffers()</a> and <a class="el" href="class_ogre_1_1_vertex_declaration.html#a130fa715330d23d0e9d2feef630b460c" title="Generates a new VertexDeclaration for optimal usage based on the current vertex declaration, which can be used with VertexData::reorganiseBuffers later if you wish, or simply used as a template. ">Ogre::VertexDeclaration::getAutoOrganisedDeclaration()</a>. The latter will turn a vertex declaration into one which is recommended for the usage you’ve indicated, and the former will reorganise the contents of a set of buffers to conform to that layout.</p>
<p><a class="anchor" id="Morph-Animation"></a> <a class="anchor" id="Morph-Animation-1"></a></p>
<h2>Morph Animation</h2>
<p>Morph animation works by storing snapshots of the absolute vertex positions in each keyframe, and interpolating between them. Morph animation is mainly useful for animating objects which could not be adequately handled using skeletal animation; this is mostly objects that have to radically change structure and shape as part of the animation such that a skeletal structure isn’t appropriate.</p>
<p>Because absolute positions are used, it is not possible to blend more than one morph animation on the same vertex data; you should use skeletal animation if you want to include animation blending since it is much more efficient. If you activate more than one animation which includes morph tracks for the same vertex data, only the last one will actually take effect. This also means that the ’weight’ option on the animation state is not used for morph animation.</p>
<p>Morph animation can be combined with skeletal animation if required See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>. Morph animation can also be implemented in hardware using vertex shaders, See <a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a>.</p>
<p><a class="anchor" id="Pose-Animation"></a> <a class="anchor" id="Pose-Animation-1"></a></p>
<h2>Pose Animation</h2>
<p>Pose animation allows you to blend together potentially multiple vertex poses at different influence levels into final vertex state. A common use for this is facial animation, where each facial expression is placed in a separate animation, and influences used to either blend from one expression to another, or to combine full expressions if each pose only affects part of the face.</p>
<p>In order to do this, pose animation uses a set of reference poses defined in the mesh, expressed as offsets to the original vertex data. It does not require that every vertex has an offset - those that don’t are left alone. When blending in software these vertices are completely skipped - when blending in hardware (which requires a vertex entry for every vertex), zero offsets for vertices which are not mentioned are automatically created for you.</p>
<p>Once you’ve defined the poses, you can refer to them in animations. Each pose animation track refers to a single set of geometry (either the shared geometry of the mesh, or dedicated geometry on a submesh), and each keyframe in the track can refer to one or more poses, each with its own influence level. The weight applied to the entire animation scales these influence levels too. You can define many keyframes which cause the blend of poses to change over time. The absence of a pose reference in a keyframe when it is present in a neighbouring one causes it to be treated as an influence of 0 for interpolation.</p>
<p>You should be careful how many poses you apply at once. When performing pose animation in hardware (See [Pose Animation in Vertex Programs](@ ref Pose-Animation-in-Vertex-Programs)), every active pose requires another vertex buffer to be added to the shader, and in when animating in software it will also take longer the more active poses you have. Bear in mind that if you have 2 poses in one keyframe, and a different 2 in the next, that actually means there are 4 active keyframes when interpolating between them.</p>
<p>You can combine pose animation with skeletal animation, See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>, and you can also hardware accelerate the application of the blend with a vertex shader, See <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p><a class="anchor" id="Combining-Skeletal-and-Vertex-Animation"></a> <a class="anchor" id="Combining-Skeletal-and-Vertex-Animation-1"></a></p>
<h2>Combining Skeletal and Vertex Animation</h2>
<p>Skeletal animation and vertex animation (of either subtype) can both be enabled on the same entity at the same time (See <a href="#Animation-State">Animation State</a>). The effect of this is that vertex animation is applied first to the base mesh, then skeletal animation is applied to the result. This allows you, for example, to facially animate a character using pose vertex animation, whilst performing the main movement animation using skeletal animation.</p>
<p>Combining the two is, from a user perspective, as simple as just enabling both animations at the same time. When it comes to using this feature efficiently though, there are a few points to bear in mind:</p>
<ul>
<li><a href="#Combined-Hardware-Skinning">Combined Hardware Skinning</a></li>
<li><a href="#Submesh-Splits">Submesh Splits</a></li>
</ul>
<p><a class="anchor" id="Combined-Hardware-Skinning"></a><a class="anchor" id="Combined-Hardware-Skinning-1"></a></p>
<h2>Combined Hardware Skinning</h2>
<p>For complex characters it is a very good idea to implement hardware skinning by including a technique in your materials which has a vertex program which can perform the kinds of animation you are using in hardware. See <a class="el" href="_high-level-_programs.html#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a>, <a class="el" href="_high-level-_programs.html#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a>, <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p>When combining animation types, your vertex programs must support both types of animation that the combined mesh needs, otherwise hardware skinning will be disabled. You should implement the animation in the same way that OGRE does, i.e. perform vertex animation first, then apply skeletal animation to the result of that. Remember that the implementation of morph animation passes 2 absolute snapshot buffers of the from &amp; to keyframes, along with a single parametric, which you have to linearly interpolate, whilst pose animation passes the base vertex data plus ’n’ pose offset buffers, and ’n’ parametric weight values.</p>
<p><a class="anchor" id="Submesh-Splits"></a><a class="anchor" id="Submesh-Splits-1"></a></p>
<h2>Submesh Splits</h2>
<p>If you only need to combine vertex and skeletal animation for a small part of your mesh, e.g. the face, you could split your mesh into 2 parts, one which needs the combination and one which does not, to reduce the calculation overhead. Note that it will also reduce vertex buffer usage since vertex keyframe / pose buffers will also be smaller. Note that if you use hardware skinning you should then implement 2 separate vertex programs, one which does only skeletal animation, and the other which does skeletal and vertex animation.</p>
<h1><a class="anchor" id="SceneNode-Animation"></a>
SceneNode Animation</h1>
<p>SceneNode animation is created from the SceneManager in order to animate the movement of SceneNodes, to make any attached objects move around automatically. You can see this performing a camera swoop in Demo_CameraTrack, or controlling how the fish move around in the pond in Demo_Fresnel.</p>
<p>At it’s heart, scene node animation is mostly the same code which animates the underlying skeleton in skeletal animation. After creating the main Animation using SceneManager::createAnimation you can create a NodeAnimationTrack per SceneNode that you want to animate, and create keyframes which control its position, orientation and scale which can be interpolated linearly or via splines. You use <a href="#Animation-State">Animation State</a> in the same way as you do for skeletal/vertex animation, except you obtain the state from SceneManager instead of from an individual Entity.Animations are applied automatically every frame, or the state can be applied manually in advance using the _applySceneAnimations() method on SceneManager. See the API reference for full details of the interface for configuring scene animations.</p>
<h1><a class="anchor" id="Numeric-Value-Animation"></a>
Numeric Value Animation</h1>
<p>Apart from the specific animation types which may well comprise the most common uses of the animation framework, you can also use animations to alter any value which is exposed via the <a href="#AnimableObject">AnimableObject</a> interface.</p>
<p><a class="anchor" id="AnimableObject"></a><a class="anchor" id="AnimableObject-1"></a></p>
<h2>AnimableObject</h2>
<p>AnimableObject is an abstract interface that any class can extend in order to provide access to a number of <a href="#AnimableValue">AnimableValue</a>s. It holds a ’dictionary’ of the available animable properties which can be enumerated via the getAnimableValueNames method, and when its createAnimableValue method is called, it returns a reference to a value object which forms a bridge between the generic animation interfaces, and the underlying specific object property.</p>
<p>One example of this is the Light class. It extends AnimableObject and provides AnimableValues for properties such as "diffuseColour" and "attenuation". Animation tracks can be created for these values and thus properties of the light can be scripted to change. Other objects, including your custom objects, can extend this interface in the same way to provide animation support to their properties.</p>
<p><a class="anchor" id="AnimableValue"></a><a class="anchor" id="AnimableValue-1"></a></p>
<h2>AnimableValue</h2>
<p>When implementing custom animable properties, you have to also implement a number of methods on the AnimableValue interface - basically anything which has been marked as unimplemented. These are not pure virtual methods simply because you only have to implement the methods required for the type of value you’re animating. Again, see the examples in Light to see how this is done. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OGRE: Animation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ogre_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ogre-logo-wetfloor.gif"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OGRE
   &#160;<span id="projectnumber">1.10.12</span>
   </div>
   <div id="projectbrief">Object-Oriented Graphics Rendering Engine</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('_animation.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Animation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>OGRE supports a pretty flexible animation system that allows you to script animation for several different purposes:</p>
<dl compact="compact">
<dt><a href="#Skeletal-Animation">Skeletal Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Mesh animation using a skeletal structure to determine how the mesh deforms. <br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Vertex-Animation">Vertex Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Mesh animation using snapshots of vertex data to determine how the shape of the mesh changes.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#SceneNode-Animation">SceneNode Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Animating SceneNodes automatically to create effects like camera sweeps, objects following predefined paths, etc.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Numeric-Value-Animation">Numeric Value Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Using OGRE’s extensible class structure to animate any value.</p>
<p class="enddd"></p>
</dd>
</dl>
<h1><a class="anchor" id="Skeletal-Animation"></a>
Skeletal Animation</h1>
<p>Skeletal animation is a process of animating a mesh by moving a set of hierarchical bones within the mesh, which in turn moves the vertices of the model according to the bone assignments stored in each vertex. An alternative term for this approach is ’skinning’. The usual way of creating these animations is with a modelling tool such as Softimage XSI, Milkshape 3D, Blender, 3D Studio or Maya among others. OGRE provides exporters to allow you to get the data out of these modellers and into the engine See <a class="el" href="_mesh-_tools.html#Exporters">Exporters</a>.</p>
<p>There are many grades of skeletal animation, and not all engines (or modellers for that matter) support all of them. OGRE supports the following features:</p>
<ul>
<li>Each mesh can be linked to a single skeleton</li>
<li>Unlimited bones per skeleton</li>
<li>Hierarchical forward-kinematics on bones</li>
<li>Multiple named animations per skeleton (e.g. ’Walk’, ’Run’, ’Jump’, ’Shoot’ etc)</li>
<li>Unlimited keyframes per animation</li>
<li>Linear or spline-based interpolation between keyframes</li>
<li>A vertex can be assigned to multiple bones and assigned weightings for smoother skinning</li>
<li>Multiple animations can be applied to a mesh at the same time, again with a blend weighting</li>
</ul>
<p><br />
</p>
<p>Skeletons and the animations which go with them are held in .skeleton files, which are produced by the OGRE exporters. These files are loaded automatically when you create an Entity based on a Mesh which is linked to the skeleton in question. You then use <a href="#Animation-State">Animation State</a> to set the use of animation on the entity in question.</p>
<p>Skeletal animation can be performed in software, or implemented in shaders (hardware skinning). Clearly the latter is preferable, since it takes some of the work away from the CPU and gives it to the graphics card, and also means that the vertex data does not need to be re-uploaded every frame. This is especially important for large, detailed models. You should try to use hardware skinning wherever possible; this basically means assigning a material which has a vertex program powered technique. See <a href="#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a> for more details. Skeletal animation can be combined with vertex animation, See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>.</p>
<p><a class="anchor" id="Animation-State"></a> </p><h2>Animation State</h2>
<p>When an entity containing animation of any type is created, it is given an ’animation state’ object per animation to allow you to specify the animation state of that single entity (you can animate multiple entities using the same animation definitions, OGRE sorts the reuse out internally).</p>
<p>You can retrieve a pointer to the AnimationState object by calling Entity::getAnimationState. You can then call methods on this returned object to update the animation, probably in the frameStarted event. Each AnimationState needs to be enabled using the setEnabled method before the animation it refers to will take effect, and you can set both the weight and the time position (where appropriate) to affect the application of the animation using correlating methods. AnimationState also has a very simple method ’addTime’ which allows you to alter the animation position incrementally, and it will automatically loop for you. addTime can take positive or negative values (so you can reverse the animation if you want).</p>
<h1><a class="anchor" id="Vertex-Animation"></a>
Vertex Animation</h1>
<p>Vertex animation is about using information about the movement of vertices directly to animate the mesh. Each track in a vertex animation targets a single VertexData instance. Vertex animation is stored inside the .mesh file since it is tightly linked to the vertex structure of the mesh.</p>
<p>There are actually two subtypes of vertex animation, for reasons which will be discussed in a moment.</p>
<dl compact="compact">
<dt><a href="#Morph-Animation">Morph Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Morph animation is a very simple technique which interpolates mesh snapshots along a keyframe timeline. Morph animation has a direct correlation to old-school character animation techniques used before skeletal animation was widely used.<br />
</p>
<p class="enddd"></p>
</dd>
<dt><a href="#Pose-Animation">Pose Animation</a> </dt>
<dd><p class="startdd"></p>
<p>Pose animation is about blending multiple discrete poses, expressed as offsets to the base vertex data, with different weights to provide a final result. Pose animation’s most obvious use is facial animation.</p>
<p class="enddd"></p>
</dd>
</dl>
<p><a class="anchor" id="Why-two-subtypes_003f"></a></p>
<h2>Why two subtypes?</h2>
<p>So, why two subtypes of vertex animation? Couldn’t both be implemented using the same system? The short answer is yes; in fact you can implement both types using pose animation. But for very good reasons we decided to allow morph animation to be specified separately since the subset of features that it uses is both easier to define and has lower requirements on hardware shaders, if animation is implemented through them. If you don’t care about the reasons why these are implemented differently, you can skip to the next part.</p>
<p>Morph animation is a simple approach where we have a whole series of snapshots of vertex data which must be interpolated, e.g. a running animation implemented as morph targets. Because this is based on simple snapshots, it’s quite fast to use when animating an entire mesh because it’s a simple linear change between keyframes. However, this simplistic approach does not support blending between multiple morph animations. If you need animation blending, you are advised to use skeletal animation for full-mesh animation, and pose animation for animation of subsets of meshes or where skeletal animation doesn’t fit - for example facial animation. For animating in a vertex shader, morph animation is quite simple and just requires the 2 vertex buffers (one the original position buffer) of absolute position data, and an interpolation factor. Each track in a morph animation references a unique set of vertex data.</p>
<p>Pose animation is more complex. Like morph animation each track references a single unique set of vertex data, but unlike morph animation, each keyframe references 1 or more ’poses’, each with an influence level. A pose is a series of offsets to the base vertex data, and may be sparse - i.e. it may not reference every vertex. Because they’re offsets, they can be blended - both within a track and between animations. This set of features is very well suited to facial animation.</p>
<p>For example, let’s say you modelled a face (one set of vertex data), and defined a set of poses which represented the various phonetic positions of the face. You could then define an animation called ’SayHello’, containing a single track which referenced the face vertex data, and which included a series of keyframes, each of which referenced one or more of the facial positions at different influence levels - the combination of which over time made the face form the shapes required to say the word ’hello’. Since the poses are only stored once, but can be referenced may times in many animations, this is a very powerful way to build up a speech system.</p>
<p>The downside of pose animation is that it can be more difficult to set up, requiring poses to be separately defined and then referenced in the keyframes. Also, since it uses more buffers (one for the base data, and one for each active pose), if you’re animating in hardware using vertex shaders you need to keep an eye on how many poses you’re blending at once. You define a maximum supported number in your vertex program definition, via the includes_pose_animation material script entry, See <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p>So, by partitioning the vertex animation approaches into 2, we keep the simple morph technique easy to use, whilst still allowing all the powerful techniques to be used. Note that morph animation cannot be blended with other types of vertex animation on the same vertex data (pose animation or other morph animation); pose animation can be blended with other pose animation though, and both types can be combined with skeletal animation. This combination limitation applies per set of vertex data though, not globally across the mesh (see below). Also note that all morph animation can be expressed (in a more complex fashion) as pose animation, but not vice versa.</p>
<p><a class="anchor" id="Subtype-applies-per-track"></a></p>
<h2>Subtype applies per track</h2>
<p>It’s important to note that the subtype in question is held at a track level, not at the animation or mesh level. Since tracks map onto VertexData instances, this means that if your mesh is split into SubMeshes, each with their own dedicated geometry, you can have one SubMesh animated using pose animation, and others animated with morph animation (or not vertex animated at all).</p>
<p>For example, a common set-up for a complex character which needs both skeletal and facial animation might be to split the head into a separate SubMesh with its own geometry, then apply skeletal animation to both submeshes, and pose animation to just the head.</p>
<p>To see how to apply vertex animation, See <a href="#Animation-State">Animation State</a>.</p>
<p><a class="anchor" id="Vertex-buffer-arrangements"></a></p>
<h2>Vertex buffer arrangements</h2>
<p>When using vertex animation in software, vertex buffers need to be arranged such that vertex positions reside in their own hardware buffer. This is to avoid having to upload all the other vertex data when updating, which would quickly saturate the GPU bus. When using the OGRE .mesh format and the tools / exporters that go with it, OGRE organises this for you automatically. But if you create buffers yourself, you need to be aware of the layout arrangements.</p>
<p>To do this, you have a set of helper functions in <a class="el" href="class_ogre_1_1_mesh.html" title="Resource holding data about 3D mesh. ">Ogre::Mesh</a>. See API Reference entries for <a class="el" href="class_ogre_1_1_vertex_data.html#a106603d9d4d6c2f6e0d5eb6c25eb7c32" title="Reorganises the data in the vertex buffers according to the new vertex declaration passed in...">Ogre::VertexData::reorganiseBuffers()</a> and <a class="el" href="class_ogre_1_1_vertex_declaration.html#a130fa715330d23d0e9d2feef630b460c" title="Generates a new VertexDeclaration for optimal usage based on the current vertex declaration, which can be used with VertexData::reorganiseBuffers later if you wish, or simply used as a template. ">Ogre::VertexDeclaration::getAutoOrganisedDeclaration()</a>. The latter will turn a vertex declaration into one which is recommended for the usage you’ve indicated, and the former will reorganise the contents of a set of buffers to conform to that layout.</p>
<p><a class="anchor" id="Morph-Animation"></a> <a class="anchor" id="Morph-Animation-1"></a></p>
<h2>Morph Animation</h2>
<p>Morph animation works by storing snapshots of the absolute vertex positions in each keyframe, and interpolating between them. Morph animation is mainly useful for animating objects which could not be adequately handled using skeletal animation; this is mostly objects that have to radically change structure and shape as part of the animation such that a skeletal structure isn’t appropriate.</p>
<p>Because absolute positions are used, it is not possible to blend more than one morph animation on the same vertex data; you should use skeletal animation if you want to include animation blending since it is much more efficient. If you activate more than one animation which includes morph tracks for the same vertex data, only the last one will actually take effect. This also means that the ’weight’ option on the animation state is not used for morph animation.</p>
<p>Morph animation can be combined with skeletal animation if required See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>. Morph animation can also be implemented in hardware using vertex shaders, See <a href="#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a>.</p>
<p><a class="anchor" id="Pose-Animation"></a> <a class="anchor" id="Pose-Animation-1"></a></p>
<h2>Pose Animation</h2>
<p>Pose animation allows you to blend together potentially multiple vertex poses at different influence levels into final vertex state. A common use for this is facial animation, where each facial expression is placed in a separate animation, and influences used to either blend from one expression to another, or to combine full expressions if each pose only affects part of the face.</p>
<p>In order to do this, pose animation uses a set of reference poses defined in the mesh, expressed as offsets to the original vertex data. It does not require that every vertex has an offset - those that don’t are left alone. When blending in software these vertices are completely skipped - when blending in hardware (which requires a vertex entry for every vertex), zero offsets for vertices which are not mentioned are automatically created for you.</p>
<p>Once you’ve defined the poses, you can refer to them in animations. Each pose animation track refers to a single set of geometry (either the shared geometry of the mesh, or dedicated geometry on a submesh), and each keyframe in the track can refer to one or more poses, each with its own influence level. The weight applied to the entire animation scales these influence levels too. You can define many keyframes which cause the blend of poses to change over time. The absence of a pose reference in a keyframe when it is present in a neighbouring one causes it to be treated as an influence of 0 for interpolation.</p>
<p>You should be careful how many poses you apply at once. When performing pose animation in hardware (See [Pose Animation in Vertex Programs](@ ref Pose-Animation-in-Vertex-Programs)), every active pose requires another vertex buffer to be added to the shader, and in when animating in software it will also take longer the more active poses you have. Bear in mind that if you have 2 poses in one keyframe, and a different 2 in the next, that actually means there are 4 active keyframes when interpolating between them.</p>
<p>You can combine pose animation with skeletal animation, See <a href="#Combining-Skeletal-and-Vertex-Animation">Combining Skeletal and Vertex Animation</a>, and you can also hardware accelerate the application of the blend with a vertex shader, See <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p><a class="anchor" id="Combining-Skeletal-and-Vertex-Animation"></a> <a class="anchor" id="Combining-Skeletal-and-Vertex-Animation-1"></a></p>
<h2>Combining Skeletal and Vertex Animation</h2>
<p>Skeletal animation and vertex animation (of either subtype) can both be enabled on the same entity at the same time (See <a href="#Animation-State">Animation State</a>). The effect of this is that vertex animation is applied first to the base mesh, then skeletal animation is applied to the result. This allows you, for example, to facially animate a character using pose vertex animation, whilst performing the main movement animation using skeletal animation.</p>
<p>Combining the two is, from a user perspective, as simple as just enabling both animations at the same time. When it comes to using this feature efficiently though, there are a few points to bear in mind:</p>
<ul>
<li><a href="#Combined-Hardware-Skinning">Combined Hardware Skinning</a></li>
<li><a href="#Submesh-Splits">Submesh Splits</a></li>
</ul>
<p><a class="anchor" id="Combined-Hardware-Skinning"></a><a class="anchor" id="Combined-Hardware-Skinning-1"></a></p>
<h2>Combined Hardware Skinning</h2>
<p>For complex characters it is a very good idea to implement hardware skinning by including a technique in your materials which has a vertex program which can perform the kinds of animation you are using in hardware. See <a class="el" href="_high-level-_programs.html#Skeletal-Animation-in-Vertex-Programs">Skeletal Animation in Vertex Programs</a>, <a class="el" href="_high-level-_programs.html#Morph-Animation-in-Vertex-Programs">Morph Animation in Vertex Programs</a>, <a class="el" href="_high-level-_programs.html#Pose-Animation-in-Vertex-Programs">Pose Animation in Vertex Programs</a>.</p>
<p>When combining animation types, your vertex programs must support both types of animation that the combined mesh needs, otherwise hardware skinning will be disabled. You should implement the animation in the same way that OGRE does, i.e. perform vertex animation first, then apply skeletal animation to the result of that. Remember that the implementation of morph animation passes 2 absolute snapshot buffers of the from &amp; to keyframes, along with a single parametric, which you have to linearly interpolate, whilst pose animation passes the base vertex data plus ’n’ pose offset buffers, and ’n’ parametric weight values.</p>
<p><a class="anchor" id="Submesh-Splits"></a><a class="anchor" id="Submesh-Splits-1"></a></p>
<h2>Submesh Splits</h2>
<p>If you only need to combine vertex and skeletal animation for a small part of your mesh, e.g. the face, you could split your mesh into 2 parts, one which needs the combination and one which does not, to reduce the calculation overhead. Note that it will also reduce vertex buffer usage since vertex keyframe / pose buffers will also be smaller. Note that if you use hardware skinning you should then implement 2 separate vertex programs, one which does only skeletal animation, and the other which does skeletal and vertex animation.</p>
<h1><a class="anchor" id="SceneNode-Animation"></a>
SceneNode Animation</h1>
<p>SceneNode animation is created from the SceneManager in order to animate the movement of SceneNodes, to make any attached objects move around automatically. You can see this performing a camera swoop in Demo_CameraTrack, or controlling how the fish move around in the pond in Demo_Fresnel.</p>
<p>At it’s heart, scene node animation is mostly the same code which animates the underlying skeleton in skeletal animation. After creating the main Animation using SceneManager::createAnimation you can create a NodeAnimationTrack per SceneNode that you want to animate, and create keyframes which control its position, orientation and scale which can be interpolated linearly or via splines. You use <a href="#Animation-State">Animation State</a> in the same way as you do for skeletal/vertex animation, except you obtain the state from SceneManager instead of from an individual Entity.Animations are applied automatically every frame, or the state can be applied manually in advance using the _applySceneAnimations() method on SceneManager. See the API reference for full details of the interface for configuring scene animations.</p>
<h1><a class="anchor" id="Numeric-Value-Animation"></a>
Numeric Value Animation</h1>
<p>Apart from the specific animation types which may well comprise the most common uses of the animation framework, you can also use animations to alter any value which is exposed via the <a href="#AnimableObject">AnimableObject</a> interface.</p>
<p><a class="anchor" id="AnimableObject"></a><a class="anchor" id="AnimableObject-1"></a></p>
<h2>AnimableObject</h2>
<p>AnimableObject is an abstract interface that any class can extend in order to provide access to a number of <a href="#AnimableValue">AnimableValue</a>s. It holds a ’dictionary’ of the available animable properties which can be enumerated via the getAnimableValueNames method, and when its createAnimableValue method is called, it returns a reference to a value object which forms a bridge between the generic animation interfaces, and the underlying specific object property.</p>
<p>One example of this is the Light class. It extends AnimableObject and provides AnimableValues for properties such as "diffuseColour" and "attenuation". Animation tracks can be created for these values and thus properties of the light can be scripted to change. Other objects, including your custom objects, can extend this interface in the same way to provide animation support to their properties.</p>
<p><a class="anchor" id="AnimableValue"></a><a class="anchor" id="AnimableValue-1"></a></p>
<h2>AnimableValue</h2>
<p>When implementing custom animable properties, you have to also implement a number of methods on the AnimableValue interface - basically anything which has been marked as unimplemented. These are not pure virtual methods simply because you only have to implement the methods required for the type of value you’re animating. Again, see the examples in Light to see how this is done. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="manual.html">Manual</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
